[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R4PDE",
    "section": "",
    "text": "R for Plant Disease Epidemiology (R4PDE) is a book project in the early stage of development. It is based on the teaching notes of my graduate course, FIP 602 - Plant Disease Epidemiology, offered every year for students of the Graduate Program in Plant Pathology of the Universidade Federal de Viçosa.\nThis book is for those interested in studying and modelling plant disease epidemics using R. Here, I provide context and showcase several methods for describing, visualizing and analyzing epidemic data collected over time and/or space. Users should have have a minimum knowledge of R to run the examples.\nThis book is not a resource to learn data science using R as there are excellent books such as the R for data science. If you read Portuguese (as many of the students of my course), I strongly recommend Análises Ecológicas no R and Software R para avaliação de dados experimentais as excellent resources for learning R (and also statistics with R).\nThroughout the book, I use several general and a few specific R packages for conducting the most common analysis of plant disease epidemiology data, in particular epifitter and epiphy. In most part, I use data and reproduce some of the analyses shown in the book The study of Plant Disease Epidemics (Madden, Hughes, and Bosch 2017).\nThis is a work in progress, meaning that it is under frequent technical and copy editing. This website is free to use, and is licensed under a Creative Commons licence. The codes for reproducing all analyses shown here are available on GitHub. There are no immediate plans to publish a physical copy of the book, but I may consider it in the future as the book takes shape. This website is hosted by https://www.netlify.com/.\n\n\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch. 2017. “The Study of Plant Disease Epidemics,” August. https://doi.org/10.1094/9780890545058."
  },
  {
    "objectID": "data-terminology.html",
    "href": "data-terminology.html",
    "title": "1  Disease variables",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "data-terminology.html#introduction",
    "href": "data-terminology.html#introduction",
    "title": "1  Disease variables",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nStudies on the progress of epidemics in time or their spread in space cannot be conducted without data collected in the field - or in same cases simulated. The study of plant disease quantification is known as Phytopathometry, a branch of plant pathology tasked with the science of disease measurement, but which has strong roots in epidemiology (Bock et al. 2021). Historically, disease quantification has been performed visually, but advances in both imaging and remote sensing technologies have directly impacted the field during the last several decades. Therefore, the quantity of disease can be obtained via estimation (visually by human eye) or measurement (sensor or digital technologies). This means that several variables can be used to express disease occurrence and quantity.\nWhile measuring disease is a a more objective task, visual assessment is largely subjective and, as such, known to vary among human raters. This occurs because raters vary in their inherent abilities, training, or are more or less affected by the chosen method (e.g. scales). Disease is estimated or measured on a specimen in a population, or on a sample of specimens drawn from a population. The specimen can be a plant organ, an individual plant, a group of plants, a field or a farm, and these also dictate how terms are defined to refer to disease quantity.\nFinally, when developing new or improving existing disease assessment methods, it is important to assess how close the estimations or measurements are from reference (gold standard) values. There are several methods that can be used to assess reliability, precision and accuracy of estimates or measures. The choice depends on the objective of the work but largely on the type or nature of the data, as it will be discussed further."
  },
  {
    "objectID": "data-terminology.html#terminology",
    "href": "data-terminology.html#terminology",
    "title": "1  Disease variables",
    "section": "1.2 Terminology",
    "text": "1.2 Terminology\nA general term used to refer to the quantity of disease expressed by any means is disease intensity. This term has little or no practical value as it only suggest that the disease is more or less “intense”, but there is no scale for intensity. We need more specific terms to refer to disease quantity. A primary task in disease assessment is to classify each specimen, usually in a sample or in the population, as diseased or not diseased. This binary (yes/no) assessment may be sufficient to express disease intensity if the goal is to assess, for example, the number or proportion of diseased specimens in a sample or a population of specimens.\nThe above leads to two terms: disease incidence and prevalence. While incidence is commonly used to refer to the proportion or number (count) of plants (or their organs) as the observational units at the field scale or below, prevalence is used when referring to the proportion or number of fields or farms with diseased plants in a large production area or region (Nutter, Esker, and Netto 2006). Hence, prevalence is equivalent to incidence, only differing in the spatial scale of the sampling unit.\nIn many cases we need to ascertain the degree to which a specimen is diseased, which is a definition of disease severity. Elsewhere, severity is defined restrictively as the proportion of the unit that is symptomatic (Nutter, Esker, and Netto 2006). However, a broader view of severity encompasses other metrics including lesion counts and scores based on ordinal scales. These latter scales can be further divided into classes defined based on either the percentage scale or descriptions of symptoms (Bock et al. 2021). In some cases disease is expressed in terms of (average) lesion size or area, which can be considered a measure of severity. It is commonly used to determine host resistance, pathogen aggressiveness or environmental influence.\n\n\n\n\nBock, Clive H., Sarah J. Pethybridge, Jayme G. A. Barbedo, Paul D. Esker, Anne-Katrin Mahlein, and Emerson M. Del Ponte. 2021. “A Phytopathometry Glossary for the Twenty-First Century: Towards Consistency and Precision in Intra- and Inter-Disciplinary Dialogues.” Tropical Plant Pathology 47 (1): 14–24. https://doi.org/10.1007/s40858-021-00454-0.\n\n\nNutter, Forrest W., Paul D. Esker, and Rosalee A. Coelho Netto. 2006. “Disease Assessment Concepts and the Advancements Made in Improving the Accuracy and Precision of Plant Disease Data.” European Journal of Plant Pathology 115 (1): 95–103. https://doi.org/10.1007/s10658-005-1230-z."
  },
  {
    "objectID": "data-type-distributions.html",
    "href": "data-type-distributions.html",
    "title": "2  Data types and distributions",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "data-type-distributions.html#data-types",
    "href": "data-type-distributions.html#data-types",
    "title": "2  Data types and distributions",
    "section": "2.1 Data types",
    "text": "2.1 Data types\nThe data used to express disease as incidence or any kind of severity measures vary in their nature as they can be discrete or continuous.\nDiscrete variables are countable (involve integers) at a finite amount of time. That is, only a limited number of values (nominal or ordinal) is possible and these cannot be subdivided into parts. For example, a plant or plant part can be either disease or not diseased (nominal data). We cannot count 1.5 diseased plants. Also, a plant classified as diseased may exhibit a certain number of lesions (count data) or be classified into a specific class of severity (ordinal data, common in ordinal scales, e.g. 1-9). Disease data in the form of counts usually relate to the number of infections per sampling units. Most commonly, counts refer to the pathogen population that is assessed, such as number of airborne or soilborne propagules.\nA continuous variable, different from discrete, can be measured on a scale and can have any numeric value between two numbers. For example, the size of a lesion on a plant can be measured at a very precise scale (cm or mm). An estimate of severity in percent scale (% diseased area) can take any value between non zero and 100%. Although discrete at the individual level, incidence at the sample level can be treated as continuous, as it can take any value in proportion or percentage.\nThe disease variables can also be described by a statistical distribution, which are models that give the probability that a particular value (or a range of values) will be drawn from a specific distribution. Knowledge about statistical or mathematical distributions constitute an important step to improve our understanding of data-collection methods, designs of experiments and data analysis such as data summarization or hypothesis testing."
  },
  {
    "objectID": "data-type-distributions.html#statistical-distributions",
    "href": "data-type-distributions.html#statistical-distributions",
    "title": "2  Data types and distributions",
    "section": "2.2 Statistical distributions",
    "text": "2.2 Statistical distributions\n\n2.2.1 Binomial distribution\nFor incidence (and prevalence), the data is binary at the individual level, as there are only two possible outcomes in a trial: the plant or plant part is disease or not diseased. The statistical distribution that best describe the incidence data at the individual level is the binomial distribution.\nLet’s simulate the binomial outcomes for a range of probabilities in a sample of 100 units, using the rbinom function in R. For a single trial (e.g. status of plants in a single plant row), the size argument is set to 1.\n\nlibrary(tidyverse)\ntheme_set(theme_bw(base_size = 14)) # set global theme\n\nset.seed(123) # for reproducibility\nP.1 <- rbinom(100, size = 1, prob = 0.1)\nP.3 <- rbinom(100, size = 1, prob = 0.3)\nP.7 <- rbinom(100, size = 1, prob = 0.7)\nP.9 <- rbinom(100, size = 1, prob = 0.9)\nbinomial_data <- data.frame(P.1, P.3, P.7, P.9)\n\nWe can then visualize the plots.\n\nbinomial_data |>\n  pivot_longer(1:4, names_to = \"P\", \n               values_to = \"value\") |> \n  ggplot(aes(value))+\n  geom_histogram(fill = \"MediumSeaGreen\", \n                 bins =10)+\n  facet_wrap(~P)+\n  labs(title = \"Binomial distribution to describe binary data\")\n\n\n\n\n\n\n2.2.2 Beta distribution\nDisease incidence (or prevalence) at the sample or population level can be expressed as proportion of diseased individuals. The same applies to disease severity when expressed as proportion of the organ area affected (a ratio variable). For such cases, the beta distribution, which is bounded between 0 and 1, provides a good description. Let’s simulate some data using the rbeta function.\n\nbeta1.5 <- rbeta(n = 1000, shape1 = 1, shape2 = 5)\nbeta5.5 <- rbeta(n = 1000, shape1 = 5, shape2 = 5)\nbeta_data <- data.frame(beta1.5, beta5.5)\n\nNotice that there are two shape parameters in the beta distribution: shape1 and shape2 to be defined. This makes the distribution very flexible and with different potential shapes as we can see below.\n\nlibrary(tidyverse)\ntheme_set(theme_bw(base_size = 14)) # set global theme\n\nbeta_data |>\n  pivot_longer(1:2, names_to = \"P\", \n               values_to = \"value\") |> \n  ggplot(aes(value))+\n  geom_histogram(fill = \"MediumSeaGreen\", \n                 color = \"white\", \n                 bins = 15)+\n  scale_x_continuous(limits = c(0,1))+\n  facet_wrap(~P)+\n  labs(title = \"Beta distribution to describe proportion data\")\n\n\n\n\n\n\n2.2.3 Poisson distribution\nThe number of diseased plants, plant parts or individual symptoms (lesions) are discrete variables (integers) which cannot take negative values. These can be described by a Poisson distribution, a discrete distribution that counts the number of events in a Poisson process. In R, we can used the rpois function to obtain 100 random observations following a Poisson distribution. For such, we need to inform the number of observation (n = 100) and lambda, the vector of means.\n\npoisson5 <- rpois(100, lambda = 10)\npoisson35 <- rpois(100, lambda = 35)\npoisson_data <- data.frame(poisson5, poisson35)\n\n\npoisson_data |>\n  pivot_longer(1:2, names_to = \"P\", \n               values_to = \"value\") |> \n  ggplot(aes(value))+\n  geom_histogram(fill = \"MediumSeaGreen\", \n                 color = \"white\", \n                 bins = 15)+\n  facet_wrap(~P)+\n  labs(title = \"Poisson distribution to describe count data\")\n\n\n\n\n\n\n2.2.4 Gamma distribution\nWhen working with a continuous variables, such as lesion size, these random variables are usually described by the normal distribution. However, the problem is that the normal (Gaussian) distribution includes negative values, a non realistic situation. Therefore, we can use the gamma distribution, which cannot take negative values, to simulate continuous plant disease data. We can use the rgamma function that requires the number of samples (n = 100 in our case) and the shape, or the mean value.\n\ngamma10 <- rgamma(n = 100, shape = 10, scale = 1)\ngamma35 <- rgamma(n = 100, shape = 35, scale = 1)\ngamma_data <- data.frame(gamma10, gamma35)\n\n\ngamma_data |>\n  pivot_longer(1:2, names_to = \"P\", \n               values_to = \"value\") |> \n  ggplot(aes(value))+\n  geom_histogram(fill = \"MediumSeaGreen\", \n                 color = \"white\", \n                 bins = 15)+\n  ylim(0, max(gamma_data$gamma35) )+\n  facet_wrap(~P)+\n  labs(title = \"Gamma distribution to describe continous data\")"
  },
  {
    "objectID": "data-accuracy.html",
    "href": "data-accuracy.html",
    "title": "3  Reliability and accuracy",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "data-accuracy.html#terminology",
    "href": "data-accuracy.html#terminology",
    "title": "3  Reliability and accuracy",
    "section": "3.1 Terminology",
    "text": "3.1 Terminology\nDisease severity, mainly when expressed in percent area diseased assessed visually, is acknowledged as a more difficult and less time- and cost-effective plant disease variable to obtain. However, errors may occur even when assessing a more objective measure such as incidence. This is the case when an incorrect assignment or confusion of symptoms occur. In either case, the quality of the assessment of any disease variable is very important and should be gauged in the studies. Several terms can be used when evaluating the quality of disease assessments, including reliability, precision, accuracy or agreement.\n\nReliability: The extent to which the same estimates or measurements of diseased specimens obtained under different conditions yield similar results. There are two types. The inter-rater reliability (or reproducibility) is a measure of consistency of disease assessment across the same specimens between raters or devices. The intra-rater reliability (or repeatability) measures consistency by the same rater or instrument on the same specimens (e.g. two assessments in time by the same rater).\nPrecision: A statistical term to express the measure of variability of the estimates or measurements of disease on the same specimens obtained by different raters (or instruments). However, reliable or precise estimates (or measurements) are not necessarily close to an actual value, but precision is a component of accuracy or agreement.\nAccuracy or agreement: These two terms can be treated as synonymous in plant pathological research. They refer to the closeness (or concordance) of an estimate or measurement to the actual severity value for a specimen on the same scale. Actual values may be obtained using various methods, against which estimates or measurements using an experimental assessment method are compared."
  },
  {
    "objectID": "data-accuracy.html#statistical-summaries",
    "href": "data-accuracy.html#statistical-summaries",
    "title": "3  Reliability and accuracy",
    "section": "3.2 Statistical summaries",
    "text": "3.2 Statistical summaries\nA formal assessment of the quality of estimates or measures is made using statistical summaries of the data expressed as indices that represent reliability, precision and accuracy. These indices can further be used to test hypothesis such as if one or another method is superior than the other. The indices or the tests vary according to the nature of the variable, whether continuous, binary or categorical.\n\n3.2.1 Inter-rater reliability\nTo calculate measures of inter-rater reliability (or reproducibility) we will work with a fraction of a larger dataset used in a published study. There, the authors tested the effect of standard area diagrams (SADs) on the reliability and accuracy of visual estimates of severity of soybean rust.\nThe selected dataset consists of five columns with 20 rows. The first is the leaf number and the others correspond to assessments of percent soybean rust severity by four raters (R1 to R4). Each row correspond to one symptomatic leaf. Let’s assign the tibble to a dataframe called sbr (an acronym for soybean rust). Note that the variable is continuous.\n\nlibrary(tidyverse)\nsbr <- tribble(\n         ~leaf, ~R1, ~R2,  ~R3, ~R4,\n            1L, 0.6, 0.6,  0.7, 0.6,\n            2L,   2, 0.7,    5,   1,\n            3L,   5,   5,    8,   5,\n            4L,   2,   4,    6,   2,\n            5L,   6,  14,   10,   7,\n            6L,   5,   6,   10,   5,\n            7L,  10,  18, 12.5,  12,\n            8L,  15,  30,   22,  10,\n            9L,   7,   2,   12,   8,\n           10L,   6,   9, 11.5,   8,\n           11L,   7,   7,   20,   9,\n           12L,   6,  23,   22,  14,\n           13L,  10,  35, 18.5,  20,\n           14L,  19,  10,    9,  10,\n           15L,  15,  20,   19,  20,\n           16L,  17,  30,   18,  13,\n           17L,  19,  53,   33,  38,\n           18L,  17, 6.8,   15,   9,\n           19L,  15,  20,   18,  16,\n           20L,  18,  22,   24,  15\n         )\n\nLet’s explore the data using various approaches. First, we can visualize how the individual estimates by the raters differ for a same leaf.\n\n# set the global theme\ntheme_set(theme_light())\n\n# transform from wide to long format\nsbr2 <- sbr |> \n  pivot_longer(2:5, names_to = \"rater\",\n               values_to = \"estimate\") \n\n# create the plot\nsbr2 |> \n  ggplot(aes(leaf, estimate, color = rater,\n             group = leaf))+\n  geom_line(color = \"black\")+\n  geom_point(size = 2)+\n  labs(y = \"Severity estimate (%)\",\n       x = \"Leaf number\")\n\n\n\n\nAlternatively, we can visualize the distribution of the estimates by rater using boxplots.\n\nsbr2 |> \n  ggplot(aes(rater, estimate))+\n  geom_boxplot(outlier.colour = NA, width =0.5)+\n  geom_jitter(width = 0.1,shape = 1, size =2)+\n  labs(y = \"Severity estimate (%)\",\n       x = \"Rater\")\n\n\n\n\nAnother interesting visualization is the correlation matrix of the estimates between all possible pair of raters. The ggpairs function of the GGally package is handy for this task.\n\nlibrary(GGally)\ntheme_set(theme_light())\n\n# create a new dataframe with only raters\nraters <- sbr |> \n  select(2:5)\n\nggpairs(raters)\n\n\n\n\n\n3.2.1.1 Coefficient of determination\nWe noticed earlier that the correlation coefficients varied across all pairs of rater. Sometimes, the means of squared Pearson’s R values (R2), or the coefficient of determination is used as a measure of inter-rater reliability. We can further examine the pair-wise correlations in more details using the correlation function of the performance package.\n\nlibrary(correlation)\nraters_cor <- correlation(raters)\nraters_cor\n\n# Correlation Matrix (pearson-method)\n\nParameter1 | Parameter2 |    r |       95% CI | t(18) |         p\n-----------------------------------------------------------------\nR1         |         R2 | 0.63 | [0.26, 0.84] |  3.46 | 0.003**  \nR1         |         R3 | 0.68 | [0.34, 0.86] |  3.96 | 0.003**  \nR1         |         R4 | 0.68 | [0.33, 0.86] |  3.89 | 0.003**  \nR2         |         R3 | 0.84 | [0.64, 0.94] |  6.60 | < .001***\nR2         |         R4 | 0.89 | [0.74, 0.96] |  8.38 | < .001***\nR3         |         R4 | 0.86 | [0.68, 0.94] |  7.20 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 20\n\n\nThe means of coefficient of determination can be easily obtained as follows.\n\n# All pairwise R2\nraters_cor$r^2\n\n[1] 0.4000610 0.4659340 0.4565685 0.7078417 0.7960531 0.7422633\n\n# means of R2\nmean(raters_cor$r^2) \n\n[1] 0.5947869\n\n\n\n\n3.2.1.2 Intraclass Correlation Coefficient\nA common statistic to report in reliability studies is the Intraclass Correlation Coefficient (ICC). There are several formulations for the ICC whose choice depend on the particular experimental design. Following the convention of the seminal work by Shrout and Fleiss (1979), there are three main ICCs:\n\nOne-way random effects model, ICC(1,1): in our context, each leaf is rated by different raters who are considered as sampled from a larger pool of raters (random effects)\nTwo-way random effects model, ICC(2,1): both raters and leaves are viewed as random effects\nTwo-way mixed model, ICC(3,1): raters are considered as fixed effects and leaves are considered as random.\n\nAdditionally, the ICC may depend on whether the ratings are an average or not of several ratings. When an average is considered, these are called ICC(1,k), ICC(2,k) and ICC(3,k).\nThe ICC can be computed using the ICC() or the icc() functions of the psych or irr packages, respectively. They both provide the coefficient, F value, and the upper and lower bounds of the 95% confidence interval.\n\nlibrary(psych)\nICC(raters)\n\nCall: ICC(x = raters)\n\nIntraclass correlation coefficients \n                         type  ICC    F df1 df2       p lower bound upper bound\nSingle_raters_absolute   ICC1 0.64  8.1  19  60 1.8e-10        0.47        0.79\nSingle_random_raters     ICC2 0.65 10.0  19  57 6.1e-12        0.47        0.80\nSingle_fixed_raters      ICC3 0.69 10.0  19  57 6.1e-12        0.54        0.82\nAverage_raters_absolute ICC1k 0.88  8.1  19  60 1.8e-10        0.78        0.94\nAverage_random_raters   ICC2k 0.88 10.0  19  57 6.1e-12        0.78        0.94\nAverage_fixed_raters    ICC3k 0.90 10.0  19  57 6.1e-12        0.82        0.95\n\n Number of subjects = 20     Number of Judges =  4\nSee the help file for a discussion of the other 4 McGraw and Wong estimates,\n\n\nThe output of interest is a dataframe with the results of all distinct ICCs. We note that the ICC1 and ICC2 gave very close results. Now, let’s obtain the various ICCs using the irr package. Differently from the the ICC() function, this one requires further specification of the model to use.\n\nlibrary(irr)\nicc(raters, \"oneway\")\n\n Single Score Intraclass Correlation\n\n   Model: oneway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n     ICC(1) = 0.641\n\n F-Test, H0: r0 = 0 ; H1: r0 > 0 \n   F(19,60) = 8.13 , p = 1.8e-10 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.44 < ICC < 0.813\n\n# The one used in the SBR paper\nicc(raters, \"twoway\")\n\n Single Score Intraclass Correlation\n\n   Model: twoway \n   Type : consistency \n\n   Subjects = 20 \n     Raters = 4 \n   ICC(C,1) = 0.692\n\n F-Test, H0: r0 = 0 ; H1: r0 > 0 \n   F(19,57) = 9.98 , p = 6.08e-12 \n\n 95%-Confidence Interval for ICC Population Values:\n  0.503 < ICC < 0.845\n\n\n\n\n3.2.1.3 Overall Concordance Correlation Coefficient\nAnother useful index is the Overall Concordance Correlation Coefficient (OCCC) for evaluating agreement among multiple observers. It was proposed by Barnhart, Haber, and Song (2002) based on the original index proposed by Lin (1989), earlier defined in the context of two fixed observers. In the paper, the authors introduced the OCCC in terms of the interobserver variability for assessing agreement among multiple fixed observers. As outcome, and similar to the original CCC, the approach addresses the precision and accuracy indices as components of the OCCC. The epi.occc function of the epiR packge does the job but it does compute a confidence interval.\n\nlibrary(epiR)\nepi.occc(raters, na.rm = FALSE, pairs = TRUE)\n\n\nOverall CCC           0.6372\nOverall precision     0.7843\nOverall accuracy      0.8125\n\n\n\n\n\n3.2.2 Intrarater reliability\nAs defined, the intrarater reliability is also known as repeatability, because it measures consistency by the same rater at repeated assessments (e.g. different times) on the same sample. In some studies, we may be interested in testing whether a new method increases repeatability of assessments by a single rater compared with another one. The same indices used for assessing reproducibility (interrater) can be used to assess repeatability, and these are reported at the rater level.\n\n\n3.2.3 Precision\nWhen assessing precision, one measures the variability of the estimates (or measurements) of disease on the same sampling units obtained by different raters (or instruments). A very high precision does not mean that the estimates are closer to the actual value (which is given by measures of bias). However, precision is a component of overall accuracy, or agreement. It is given by the Pearson’s correlation coefficient.\nDifferent from reliability, that requires only the estimates or measures by the raters, now we need a reference (gold standard) value to compare the estimates to. These can be an accurate rater or measures by an instrument. Let’s get back to the soybean rust severity estimation dataset and add a column for the (assumed) actual values of severity on each leaf. In that work, the actual severity values were obtained using image analysis.\n\nsbr <- tibble::tribble(\n         ~leaf, ~actual, ~R1, ~R2,  ~R3, ~R4,\n            1L,    0.25, 0.6, 0.6,  0.7, 0.6,\n            2L,     2.5,   2, 0.7,    5,   1,\n            3L,    7.24,   5,   5,    8,   5,\n            4L,    7.31,   2,   4,    6,   2,\n            5L,    9.07,   6,  14,   10,   7,\n            6L,    11.6,   5,   6,   10,   5,\n            7L,   12.46,  10,  18, 12.5,  12,\n            8L,    13.1,  15,  30,   22,  10,\n            9L,   14.61,   7,   2,   12,   8,\n           10L,   16.06,   6,   9, 11.5,   8,\n           11L,    16.7,   7,   7,   20,   9,\n           12L,    19.5,   6,  23,   22,  14,\n           13L,   20.75,  10,  35, 18.5,  20,\n           14L,   23.56,  19,  10,    9,  10,\n           15L,   23.77,  15,  20,   19,  20,\n           16L,   24.45,  17,  30,   18,  13,\n           17L,   25.78,  19,  53,   33,  38,\n           18L,   26.03,  17, 6.8,   15,   9,\n           19L,   26.42,  15,  20,   18,  16,\n           20L,   28.89,  18,  22,   24,  15\n         )\n\nWe can explore visually via scatter plots for the relationships between the actual value and the estimates by each rater. To facilitate, we need the data in the long format.\n\nsbr2 <- sbr |> \n  pivot_longer(3:6, names_to = \"rater\",\n               values_to = \"estimate\") \n\nsbr2 |> \n  ggplot(aes(actual, estimate))+\n  geom_point(size = 3, alpha = 0.7)+\n  facet_wrap(~rater)+\n  ylim(0,45)+\n  xlim(0,45)+\n  geom_abline(intercept = 0, slope =1)+\n  labs(x = \"Actual severity (%)\",\n       y = \"Estimate severity (%)\")\n\n\n\n\nThe Pearson’s r for the relationship, or the precision of the estimates by each rater, can be obtained using the correlation function of the correlation package.\n\nprecision <- sbr2 |> \n  select(-leaf) |> \n  group_by(rater) |> \n  correlation() \n\nprecision\n\n# Correlation Matrix (pearson-method)\n\nGroup | Parameter1 | Parameter2 |    r |       95% CI | t(18) |         p\n-------------------------------------------------------------------------\nR1    |     actual |   estimate | 0.87 | [0.70, 0.95] |  7.58 | < .001***\nR2    |     actual |   estimate | 0.58 | [0.19, 0.82] |  3.06 | 0.007**  \nR3    |     actual |   estimate | 0.75 | [0.47, 0.90] |  4.86 | < .001***\nR4    |     actual |   estimate | 0.71 | [0.39, 0.88] |  4.29 | < .001***\n\np-value adjustment method: Holm (1979)\nObservations: 20\n\nmean(precision$r)\n\n[1] 0.7302795\n\nmean(precision$CI_low)\n\n[1] 0.4373825\n\nmean(precision$CI_high)\n\n[1] 0.8847261\n\n\n\n\n3.2.4 Accuracy\n\n3.2.4.1 Absolute errors\n\nsbr2 |> \n  ggplot(aes(actual, estimate-actual))+\n  geom_point(size = 3, alpha = 0.7)+\n  facet_wrap(~rater)+\n  geom_hline(yintercept = 0)+\n  labs(x = \"Actual severity (%)\",\n       y = \"Error (Estimate - Actual)\")\n\n\n\n\n\n\n3.2.4.2 Concordance correlation coefficient\nLin’s (1989, 2000) proposed the concordance correlation coefficient (CCC) for agreement on a continuous measure obtained by two methods. The CCC combines measures of both precision and accuracy to determine how far the observed data deviate from the line of perfect concordance. Lin’s CCC increases in value as a function of the nearness of the data’s reduced major axis to the line of perfect concordance (the accuracy of the data) and of the tightness of the data about its reduced major axis (the precision of the data).\nThe epi.ccc function of the epiR package allows to obtain the Lin’s CCC statistics. Let’s filter only rater 2 and calculate the CCC statistics for this rater.\n\nlibrary(epiR)\n\n# Only rater 2\nsbr3 <- sbr2 |> filter(rater == \"R2\")\n\n\nccc <- epi.ccc(sbr3$actual, sbr3$estimate)\n\n# Concordance coefficient\nrho <- ccc$rho.c[,1]\nrho\n\n[1] 0.5230656\n\n# Bias coefficient\nCb <- ccc$C.b\nCb\n\n[1] 0.8948494\n\n# Precision\nr <- ccc$C.b*ccc$rho.c[,1]\nr\n\n[1] 0.4680649\n\n# Scale-shift\nss <- ccc$s.shift\nss\n\n[1] 1.609118\n\n# Location-shift\nls <- ccc$l.shift\nls\n\n[1] -0.06660692\n\nMetrics <- c(\"Agreement\", \"Bias coefficient\", \"Precision\", \"scale-shift\", \"location-shift\")\nValue <- c(rho, Cb, r, ss, ls)\nres <- data.frame(Metrics, Value)\nres\n\n           Metrics       Value\n1        Agreement  0.52306557\n2 Bias coefficient  0.89484944\n3        Precision  0.46806493\n4      scale-shift  1.60911784\n5   location-shift -0.06660692\n\n\n\n\n\n\nBarnhart, Huiman X., Michael Haber, and Jingli Song. 2002. “Overall Concordance Correlation Coefficient for Evaluating Agreement Among Multiple Observers.” Biometrics 58 (4): 1020–27. https://doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nLin, Lawrence I-Kuei. 1989. “A Concordance Correlation Coefficient to Evaluate Reproducibility.” Biometrics 45 (1): 255. https://doi.org/10.2307/2532051.\n\n\nShrout, Patrick E., and Joseph L. Fleiss. 1979. “Intraclass Correlations: Uses in Assessing Rater Reliability.” Psychological Bulletin 86 (2): 420–28. https://doi.org/10.1037/0033-2909.86.2.420."
  },
  {
    "objectID": "data-actual-severity.html",
    "href": "data-actual-severity.html",
    "title": "4  Measuring severity",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "data-actual-severity.html#the-actual-severity",
    "href": "data-actual-severity.html#the-actual-severity",
    "title": "4  Measuring severity",
    "section": "4.1 The actual severity",
    "text": "4.1 The actual severity\nAmong the different ways to express plant disease severity, the percent area affected (symptomatic) by the disease is one of the most common especially when dealing with diseases that affect leaves. To evaluate whether the visual estimates of plant disease severity are sufficiently accurate (as seen in the previous chapter), one needs the actual severity values. These are also needed when preparing standard area diagrams (SADs) which are diagrammatic representations of severity values used as an aid prior or during the visual assessment to standardize and produce more accurate results across different raters (Del Ponte et al. 2017).\nThe actual severity values are usually approximated using image analysis where each pixel of the image is labeled according to three different classes:\n\nDiseased (or symptomatic)\nNon-diseased (or healthy)\nBackground (the non-plant portion of the image)\n\nThe ratio between the non diseased and diseased area of the unit (e.g. a fraction of the whole leaf image) gives the proportion of diseased area or the percent area affected (when multiplied by 100). Several different proprietary or open-source software has been used by researchers to determine the actual severity according a review on standard area diagrams (Del Ponte et al. 2017).\nHere, we will use the measure_disease function of the pliman (Plant IMage ANanalysis) (Olivoto 2022) R package to determine the actual severity values. The package was compared with other software for determining plant disease severity on five different plant diseases and showed to produce concordant results for most of the cases (Olivoto et al. 2022). The workflow is based on two main steps: 1) preparing the image palettes that define each class of the image; and 2) run the function that will classify each pixel of the images according to the colors of the palettes."
  },
  {
    "objectID": "data-actual-severity.html#image-palettes",
    "href": "data-actual-severity.html#image-palettes",
    "title": "4  Measuring severity",
    "section": "4.2 Image palettes",
    "text": "4.2 Image palettes\nThe most critical is the initial step, when the user needs to correctly define the color palettes. In pliman the palettes are separate images representing each of three classes named background (b), symptomatic (s) and healthy (h).\nThe reference image palettes can be made simply by manually sampling small areas of the image and producing a composite image. Of course, the results may vary depending on how these areas are chosen, and are subjective in nature due to how researchers prepare the palettes. A work on the validation of the pliman to determine disease severity showed the effect of different palettes prepared independently by three researchers (Olivoto et al. 2022)\nThe observation of the processed masks becomes important to create image palettes that are most representative of the respective class.\nHere, I cut and pasted several sections of images representative of each class from a few leaves into a Google slide. Once the image palette was ready, I exported each one as a separate image PNG file (JPG also works). These were named: sbr_b.png, sbr_h.png and sbr_s.png.\n\n\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image analysis. Methods in Ecology and Evolution. 13:789–798 Available at: http://dx.doi.org/10.1111/2041-210X.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring plant disease severity in R: introducing and evaluating the pliman package. Tropical Plant Pathology. 47:95–104 Available at: http://dx.doi.org/10.1007/s40858-021-00487-5."
  },
  {
    "objectID": "temporal-dpc.html",
    "href": "temporal-dpc.html",
    "title": "5  Disease progress curves",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "temporal-dpc.html#disease-progress-curves",
    "href": "temporal-dpc.html#disease-progress-curves",
    "title": "5  Disease progress curves",
    "section": "5.1 Disease progress curves",
    "text": "5.1 Disease progress curves\nA key understanding of the epidemics relates to the knowledge of rates and patterns. Epidemics can be viewed as dynamic systems that change their state as time goes. The first and simplest way to characterize such changes in time is to produce a graphical plot called disease progress curve (DPC). This curve can be obtained as long as the intensity of the disease (y) in the host population is assessed sequentially in time (t).\nA DPC summarizes the interaction of the three main components of the disease triangle occurring during the epidemic. The curves can vary greatly in shape according to variations in each of the components, in particular due to management practices that alter the course of the epidemics and for which the goal is to stop disease increase. We can create a dataframe in R for a single DPC and make a plot using ggplot. By convention we use t for time and y for disease intensity, expressed in percentage (0 to 100%).\nFirstly, let’s load the essential R packages and set up the environment.\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot) # for themes \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\nThere are several ways to create a dataframe in R. I like to use the tribble function as below. The entered data will be assigned to a dataframe called dpc.\n\ndpc <- \n  tribble(\n   ~t,  ~y, \n   0,  8, \n   7,  13, \n  14,  78, \n  21,  92, \n  28,  99, \n  35, 99.5, \n  42, 99.9, \n  )\n\nNow the plot\n\ndpc |>\n  ggplot(aes(t, y)) +\n  geom_line(size = 1)+\n  geom_point(size = 4, shape = 16)+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")"
  },
  {
    "objectID": "temporal-dpc.html#curve-descriptors-audpc",
    "href": "temporal-dpc.html#curve-descriptors-audpc",
    "title": "5  Disease progress curves",
    "section": "5.2 Curve descriptors: AUDPC",
    "text": "5.2 Curve descriptors: AUDPC\nThe depiction and analysis of disease progress curves can provide useful information for gaining understanding of the underlying epidemic process. The curves are extensively used to evaluate how disease control measures affect epidemics. When characterizing DPCs, a researcher may be interested in describing and comparing epidemics that result from different treatments, or simply in their variations as affected by changes in environment, host or pathogen.\nThe precision and complexity of the analysis of progress curve data depends on the objective of the study. In general, the goal is to synthesize similarities and different among epidemics based on common descriptors of the disease progress curves. For example, the simple appraisal of the disease intensity at any time during the course of the epidemic should be sufficient for certain situations. Furthermore, a few descriptors can be extracted including the epidemic duration, the initial and maximum disease, and the area under the disease progress curve (AUDPC).\nThe AUDPC summarizes the “total measure of disease stress” and is largely used to compare epidemics (Jeger and Viljanen-Rollinson 2001). The most common approach to calculate AUDPC is the trapezoidal method, which splits the disease progress curves into a series of rectangles, calculating the area of each of them and then summing the areas. Let’s extend the plot code to show those rectangles using the annotate function.\n\nlibrary(ggthemes)\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\ndpc1 <- dpc |>\n  ggplot(aes(t, y)) +\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n    geom_area(alpha = 0.5, fill = \"MediumSeaGreen\")+\n    geom_line(size = 1)+\n  geom_point(size = 3, shape = 16)+\n  theme_few()+\n  \n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42))\n\ndpc1\n\n\n\n\n\ndpc2 <- dpc |>\n  ggplot(aes(t, y)) +\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity (%)\")+\n  annotate(\"rect\", xmin = dpc$t[1], xmax = dpc$t[2], \n           ymin = 0, ymax = (dpc$y[1]+ dpc$y[2])/2, \n           color = \"darkgreen\", fill = \"MediumSeaGreen\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[2], xmax = dpc$t[3], \n            ymin = 0, ymax = (dpc$y[2]+ dpc$y[3])/2, \n            color = \"darkgreen\", fill = \"MediumSeaGreen\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[3], xmax = dpc$t[4], \n            ymin = 0, ymax = (dpc$y[3]+ dpc$y[4])/2,\n            color = \"darkgreen\",, fill = \"MediumSeaGreen\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[4], xmax = dpc$t[5], \n            ymin = 0, ymax = (dpc$y[4]+ dpc$y[5])/2, \n            color = \"darkgreen\", fill = \"MediumSeaGreen\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[5], xmax = dpc$t[6], \n            ymin = 0, ymax = (dpc$y[5]+ dpc$y[6])/2, \n            color = \"darkgreen\",fill = \"MediumSeaGreen\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[6], xmax = dpc$t[7], \n            ymin = 0, ymax = (dpc$y[6]+ dpc$y[7])/2, \n            color = \"darkgreen\", fill = \"MediumSeaGreen\", alpha = 0.5)+\n  geom_line(size = 1)+\n  geom_point(size = 3, shape = 16)+\n  annotate(geom = \"text\", x = 26.5, y = 50,\n           label = \"AUDPC = 3048.5\", size = 6)+\n  theme_few()+\n  scale_x_continuous(breaks = c(0, 7, 14, 21, 28, 35, 42))\ndpc2\n\n\n\n\nIn R, we can obtain the AUDPC for the DPC we created earlier using the AUDPC function offered by the epifitter package. Because we are using the percent data, we need to set the argument y_proportion = FALSE. The function returns the absolute AUDPC. If one is interested in relative AUDPC, the argument type should be set to \"relative\". There is also the alternative to AUDPC, the area under the disease progress stairs (AUDPS) (Simko and Piepho 2012).\n\nlibrary(epifitter)\nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 3048.15\n\n# The relative AUDPC \nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE, \n      type = \"relative\")\n\n[1] 0.72575\n\n# To calculate AUDPS, the alternative to AUDPC\nAUDPS(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 3425.8\n\n\n\n\n\n\nJeger, M. J., and S. L. H. Viljanen-Rollinson. 2001. “The Use of the Area Under the Disease-Progress Curve (AUDPC) to Assess Quantitative Disease Resistance in Crop Cultivars.” Theoretical and Applied Genetics 102 (1): 32–40. https://doi.org/10.1007/s001220051615.\n\n\nSimko, Ivan, and Hans-Peter Piepho. 2012. “The Area Under the Disease Progress Stairs: Calculation, Advantage, and Application.” Phytopathology® 102 (4): 381–89. https://doi.org/10.1094/phyto-07-11-0216."
  },
  {
    "objectID": "temporal-models.html",
    "href": "temporal-models.html",
    "title": "6  Population models",
    "section": "",
    "text": "Mathematical models can be fitted to the DPC data to express epidemic progress in terms of rates and absolute/relative quantities. The latter can be accomplished using population dynamics (or growth-curve) models for which the estimated parameters are usually meaningful biologically and appropriately describe epidemics that do not decrease in disease intensity. By fitting an appropriate model to the progress curve data, another set of parameters is available to the researcher when attempting to represent, understand or compare epidemics.\nThe family of models that describe the growth of epidemics, hence population dynamics model, are known as deterministic models of continuous time (Madden, Hughes, and van den Bosch 2017). These models are usually fitted to DPC data to obtain two or more biologically meaningful parameters. Here, these models and their formulations are shown using R scripts to simulate the theoretical curves for each model.\n\n6.0.1 Non-flexible models\nThese population dynamics models require at least two parameters, hence they are known as non-flexible, as opposed to the flexible ones for which there are at least one additional (third) parameter.\nFollowing the convention proposed by (Madden, Hughes, and van den Bosch 2017) in their book “The study of plant disease epidemics”:\n\ntime is represented by \\(t\\)\ndisease intensity by \\(y\\)\nthe rate of change in \\(y\\) between two time units is represented by \\(\\frac{dy}{dt}\\)\n\nNow we can proceed and learn which non-flexible models exist and for which situation they are more appropriate.\n\n6.0.1.1 Exponential\nThe differential equation for the exponential model is given by\n\\(\\frac{dy}{dt} = r_E.y\\),\nwhere \\(r_E\\) is the apparent infection rate (subscript E for this model) (sensu Vanderplank) and \\(y\\) is the disease intensity. Biologically, this formulation suggests that diseased plants, or \\(y\\), and \\(r_E\\) at each time contribute to disease increase. The value of \\(\\frac{dy}{dt}\\) is minimal when \\(y = 0\\) and increases exponentially with the increase in \\(y\\).\nThe integral for the exponential model is given by\n\\(y = y_0 e^{r_Et}\\),\nwhere \\(y0\\) is and \\(r\\) are obtained via estimation. Let’s simulate two curves by varying \\(r\\) while fixing \\(y0\\) and varying the latter while fixing \\(r_E\\). We produce the two plots in ggplot and add the predicted curve using the `stat_function`. But first, we need to define values for the two model parameters. Further modifications to these values will be handled directly in the simulation (e.g. doubling infection rate, reducing initial inoculum by half, etc.).\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot) # for themes \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\n\ny0 <- 0.001 \nr <- 0.06 \ntmax <- 60 # maximum duration t of the epidemics\ndat <- data.frame(t = seq(1:tmax), y = seq(0:1)) # define the axes\n\nIn the plot below, note that the infection rate in one curve was doubled (\\(r\\) = 0.12)\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  labs(\n    title = \"Exponential model\",\n    subtitle = \"2 times r (dashed) same y0\",\n    x = \"Time\"\n  )\n\nWarning: Removed 5 row(s) containing missing values (geom_path).\n\n\n\n\n\nNow the inoculum was increased five times while using the same doubled rate.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * 5 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  labs(title = \"Exponential model\", x = \"Time\",\n       subtitle = \"5 times y0 (dashed) same r\")\n\nWarning: Removed 5 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 27 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n6.0.1.2 Monomolecular\nThe differential of the monomolecular model is given by\n\\(\\frac{dy}{dt} = r_M (1-y)\\)\nwhere now the \\(r_M\\) is the rate parameter of the monomolecular model and \\((1-y)\\) is the proportion of non-infected (healthy) individuals or host tissue. Note that \\(\\frac{dy}{dt}\\) is maximum when \\(y = 0\\) and decreases when \\(y\\) approaches 1. Its decline is due to decrease in the proportion of individuals or healthy sites with the increase in \\(y\\). Any inoculum capable of infecting the host will more likely land on infected individuals or sites.\nThe integral of the monomolecular model is given by\n\\(\\frac{dy}{dt} = 1 - (1-y)e^{-r_Mt}\\)\nThis model commonly describes the temporal patterns of the monocyclic epidemics. In those, the inoculum produced during the course of the epidemics do not contribute new infections. Therefore, different from the exponential model, disease intensity \\(y\\) does not affect the epidemics and so the absolute rate is proportional to \\((1-y)\\).\nLet’s simulate two monomolecular curve with different rate parameters where one is one third of the other.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r * t))) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-(r / 3) * t))) +\n  labs(title = \"Monomolecular model\",\n         subtitle = \"Fixed y0 = 0.001\", x = \"Time\"\n       ) +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"r = 0.06\") +\n  annotate(geom = \"text\", x = 50, y = 0.55, label = \"r = 0.02\")\n\n\n\n\nNow inoculum was increased 100 times with the reduced rate.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r / 2 * t))) +\n  stat_function(fun = function(t) 1 - ((1 - (y0 * 100)) * exp(-r / 2 * t))) +\n  labs(title = \"Monomolecular model\", \n       subtitle = \"Fixed r = 0.06\", x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.01\") +\n  annotate(geom = \"text\", x = 45, y = 0.65, label = \"y0 = 0.001\")\n\n\n\n\n\n\n6.0.1.3 Logistic\nThe logistic model is a more elaborated version of the two previous models as it incorporates the features of them both. Its differential is given by\n\\(\\frac{dy}{dt} = r_L. y . (1 - y)\\),\nwhere \\(r_L\\) is the infection rate of the logistic model, \\(y\\) is the proportion of diseased individuals or host tissue and \\((1-y)\\) is the proportion of non-affected individuals or host area.\nBiologically, \\(y\\) in its differential equation implies that \\(\\frac{dy}{dt}\\) increases with the increase in \\(y\\) (as in the exponential) because more disease means more inoculum. However, \\((1-y)\\) leads to a decrease in \\(\\frac{dy}{dt}\\) when \\(y\\) approaches the maximum \\(y=1\\), because the proportion of healthy individuals or host area decreases (as in the monomolecular). Therefore, \\(\\frac{dy}{dt}\\) is minimal at the onset of the epidemics, reaches a maximum when \\(y/2\\) and declines until \\(y=1\\).\nThe integral is given by\n\\(y = \\frac{1}{1 + (1-y_0).e^{-r.t}}\\),\nwhere \\(r_L\\) is the apparent infection rate of the logistic model and \\(y0\\) is the disease intensity at \\(t=0\\). This model provides a good fit to polycyclic epidemics.\nLet’s check two curves where in one the infection rate is double while keeping the same initial inoculum.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 2 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 4 * t))) +\n  labs(title = \"Logistic model\", subtitle = \"Fixed y0 = 0.001\", x = \"Time\") +\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.18\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.024\")\n\n\n\n\nNow the inoculum is reduced 10 times for a same infection rate.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - (y0 / 10)) / (y0 / 10)) * exp(-r * 3 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 3 * t))) +\n  labs(title = \"Logistic model\", subtitle = \"Fixed r = 0.24\", x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"y0 = 0.0001\")\n\n\n\n\n\n\n6.0.1.4 Gompertz\nThe Gompertz model is similar to the logistic and also provides a very good fit to several polycyclic diseases. The differential equation is given by\n\\(\\frac{dy}{dt} = r_G.[ln(1) - ln(y)]\\)\nDifferently from the logistic, the variable representing the non-infected individuals or host area is \\(-ln(y)\\). The integral equation is given by\n\\(y = e^{(ln(y0)).{e^{-r_G.t)}}}\\),\nwhere \\(r_G\\) is the apparent infection rate for the Gompertz models and \\(y_0\\) is the disease intensity at \\(t = 0\\).\nLet’s check curves for two rates.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r/2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0) * exp(-r*2 * t))) +\n  labs(title = \"Gompertz model\", subtitle = \"Fixed y0 = 0.001\", x = \"Time\") +\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.12\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.03\")\n\n\n\n\nAnd those when inoculum was reduced thousand times.\n\ndat |>\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r*2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0/1000) * exp(-r*2 * t))) +\n  labs(title = \"Gompertz model\", subtitle = \"Fixed r = 0.12\", x = \"Time\") +\n  annotate(geom = \"text\", x = 15, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 25, y = 0.10, label = \"y0 = 0.00001\")\n\n\n\n\n\n\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch, eds. 2017. “CHAPTER 4: Temporal Analysis i: Quantifying and Comparing Epidemics.” In, 63–116. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.004."
  },
  {
    "objectID": "temporal-fitting.html",
    "href": "temporal-fitting.html",
    "title": "7  Model fitting",
    "section": "",
    "text": "In this tutorial, you will learn how to fit models to multiple actual disease progress curves (DPCs) data obtained from the literature. I will demonstrate how to fit and select the models using the epifitter package. A few user friendly functions will help us decide which model to choose to obtain the parameters of interest and further compare the epidemics.\nTo illustrate, I will use two datasets available from Chapter 3 from the book, Study of Plant Disease Epidemics (Madden, Hughes, and van den Bosch 2017). In the book, SAS codes are presented to perform a few analysis. We then provide an alternative code for performing similar analysis, although not perfectly reproducing the results from the book.\n\n7.0.1 Non-replicated epidemics\nWe will compare three DPCs of the incidence of tobacco etch, a virus disease, in peppers. Evaluations of incidence were evaluated at a 7-day interval, up to 49 days.The data are available in chapter 4 (page 93). Let’s input the data manually and create a data frame. First column is the assessment time and the other columns correspond to the treatments, called groups in the book, from 1 to 3.\n\n\n7.0.2 Entering data\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot) # for themes \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\n\npepper <- \n  tribble(\n   ~t,  ~`1`,  ~`2`,  ~`3`,\n   0,  0.08, 0.001, 0.001,\n   7,  0.13,  0.01, 0.001,\n  14,  0.78,  0.09,  0.01,\n  21,  0.92,  0.25,  0.05,\n  28,  0.99,   0.8,  0.18,\n  35, 0.995,  0.98,  0.34,\n  42, 0.999,  0.99,  0.48,\n  49, 0.999, 0.999,  0.74\n  ) \n\n\n\n7.0.3 Visualize the DPCs\nBefore proceeding with model selection and fitting, let’s visualize the three epidemics. The code below reproduces quite exactly the top plot of Fig. 4.15 ((Madden, Hughes, and van den Bosch 2017) page 94). The appraisal of the curves might give us a hint on which models are the best candidates.\nBecause the data was entered in the wide format (each DPCs in a different columns) we need to reshape it to the long format. The pivot_longer function will do the job of reshaping from wide to long format so we can finally use the ggplot function to produce the plot.\n\npepper |> \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\") |> \n  ggplot (aes(t, inc, \n              linetype = treat, \n              shape = treat, \n              group = treat))+\n  geom_line(size = 1)+\n  geom_point(size =3, shape = 16)+\n  annotate(geom = \"text\", x = 15, y = 0.84, label = \"1\")+\n  annotate(geom = \"text\", x = 23, y = 0.6, label = \"2\")+\n  annotate(geom = \"text\", x = 32, y = 0.33, label = \"3\")+\n  labs(y = \"Disease incidence (y)\",\n       x = \"Time (days)\")+\n  theme(legend.position = \"none\")\n\n\n\n\nMost of the three curves show a sigmoid shape with the exception of group 3 that resembles an exponential growth, not reaching the maximum value, and thus suggesting an incomplete epidemic. We can easily eliminate the monomolecular and exponential models and decide on the other two non-flexible models: logistic or Gompertz. To do that, let’s proceed to model fitting and evaluate the statistics for supporting a final decision. There are two modeling approaches for model fitting in epifitter: the linear or nonlinear parameter-estimation methods.\n\n\n7.0.4 Fitting: single epidemics\nAmong the several options offered by epifitter we start with the simplest one, which is fit a model to a single epidemics using the linear regression approach. For such, the fit_lin() requires two arguments: time (time) and disease intensity (y) each one as a vector stored or not in a dataframe.\nSince we have three epidemics, fit_lin() will be use three times. The function produces a list object with six elements. Let’s first look at the Stats dataframe of each of the three lists named epi1 to epi3.\n\nlibrary(epifitter)\nepi1 <- fit_lin(time = pepper$t,  \n                y = pepper$`1` )\nepi1$Stats\n\n                 CCC r_squared    RSE\nGompertz      0.9848    0.9700 0.5911\nMonomolecular 0.9838    0.9681 0.5432\nLogistic      0.9782    0.9572 0.8236\nExponential   0.7839    0.6447 0.6705\n\n\n\nepi2 <- fit_lin(time = pepper$t,  \n  y = pepper$`2` )\nepi2$Stats\n\n                 CCC r_squared    RSE\nLogistic      0.9962    0.9924 0.4524\nGompertz      0.9707    0.9431 0.8408\nMonomolecular 0.9248    0.8601 1.0684\nExponential   0.8971    0.8134 1.2016\n\n\n\nepi3 <- fit_lin(time = pepper$t,  \n  y = pepper$`3` )\nepi3$Stats\n\n                 CCC r_squared    RSE\nLogistic      0.9829    0.9665 0.6045\nGompertz      0.9825    0.9656 0.2263\nExponential   0.9636    0.9297 0.7706\nMonomolecular 0.8592    0.7531 0.2534\n\n\nThe statistics of the model fit confirms our initial guess that the predictions by the logistic or the Gompertz are closer to the observations than predictions by the other models. There is no much difference between them based on these statistics. However, to pick one of the models, it is important to inspect the curves with the observed and predicted values to check which model is best for all curves.\n\n\n7.0.5 Fitting: multiple epidemics\nBefore looking at the prediction, let’s use another handy function that allows us to simultaneously fit the models to multiple DPC data. Different from fit_lin(), fit_multi() requires the data to be structured in the long format where there is a column specifying each of the epidemics.\nLet’s then create a new data set called pepper2 using the data transposing functions of the tidyr package.\n\npepper2 <- pepper |> \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\")\n\nNow we fit the models to all DPCs. Note that the name of the variable indicating the DPC code needs to be informed in strata_cols argument.\n\nepi_all <- fit_multi(\n  time_col = \"t\",\n  intensity_col = \"inc\",\n  data = pepper2,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nNow let’s select the statistics of model fitting. Again, Epifitter ranks the models based on the CCC (the higher the better) but it is important to check the RSE as well - the lower the better. In fact, the RSE is more important when the goal is prediction.\n\nepi_all$Parameters |> \n  select(treat, model, best_model, RSE, CCC)\n\n   treat         model best_model       RSE       CCC\n1      1      Gompertz          1 0.5911056 0.9847857\n2      1 Monomolecular          2 0.5431977 0.9838044\n3      1      Logistic          3 0.8235798 0.9781534\n4      1   Exponential          4 0.6705085 0.7839381\n5      2      Logistic          1 0.4523616 0.9961683\n6      2      Gompertz          2 0.8407922 0.9707204\n7      2 Monomolecular          3 1.0683633 0.9247793\n8      2   Exponential          4 1.2015809 0.8971003\n9      3      Logistic          1 0.6045243 0.9829434\n10     3      Gompertz          2 0.2262550 0.9824935\n11     3   Exponential          3 0.7705736 0.9635747\n12     3 Monomolecular          4 0.2533763 0.8591837\n\n\nTo be more certain about our decision, let’s advance to the final step which is to produce the plots with the observed and predicted values for each assessment time by calling the Data dataframe of the `epi_all list.\n\nepi_all$Data |>\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |> \n  ggplot(aes(time, predicted, shape = treat)) +\n  geom_point(aes(time, y)) +\n  geom_line() +\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(0, 1)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nOverall, the logistic model seems a better fit for all the curves. Let’s produce a plot with the prediction error versus time.\n\nepi_all$Data |>\n filter(model %in% c(\"Gompertz\", \"Logistic\")) |> \n  ggplot(aes(time, predicted -y, shape = treat)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype =2)+\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(-0.4, 0.4)) + # set the max to 0.6\n  labs(\n    y = \"Prediction error\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nThe plots above confirms the logistic model as good fit overall because the errors for all epidemics combined are more scattered around the non-error line.\n\n  epi_all$Parameters |>\n    filter(model == \"Logistic\") |>\n    select(treat, y0, y0_ci_lwr, y0_ci_upr, r, r_ci_lwr, r_ci_upr \n)\n\n  treat           y0    y0_ci_lwr   y0_ci_upr         r  r_ci_lwr  r_ci_upr\n1     1 0.0935037690 0.0273207272 0.274728744 0.2104047 0.1659824 0.2548270\n2     2 0.0013727579 0.0006723537 0.002800742 0.2784814 0.2540818 0.3028809\n3     3 0.0008132926 0.0003131745 0.002110379 0.1752146 0.1426077 0.2078215\n\n\nWe can produce a plot for visual inference on the differences in the parameters.\n\np1 <- epi_all$Parameters |>\n  filter(model == \"Logistic\") |>\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np2 <- epi_all$Parameters |>\n  filter(model == \"Logistic\") |>\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:cowplot':\n\n    align_plots\n\np1 | p2\n\n\n\n\n\n\n7.0.6 Designed experiments\nIn this next section, we will work with disease data collected over time in the same plot unit (also called repeated measures) from a designed experiment for evaluating and comparing treatment effects.\nAgain, we will use a dataset of progress curves shown in page 98 (Madden, Hughes, and van den Bosch 2017). The curves represent the incidence of soybean plants symptomatic for bud blight caused by tobacco streak virus. Four treatments (different planting dates) were evaluated in randomized complete block design with four replicates. There are four assessment in time for each curve. The data was stored as a csv file and will be loaded using read_csv() function and stored as dataframe called budblight.\n\n7.0.6.1 Loading data\n\nbudblight <- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/bud-blight-soybean.csv\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  treat = col_character(),\n  time = col_double(),\n  block = col_double(),\n  y = col_double()\n)\n\n\nLet’s have a look at the first six rows of the dataset and check the data type for each column. There is an additional column representing the replicates, called block.\n\nhead(budblight)\n\n# A tibble: 6 × 4\n  treat  time block     y\n  <chr> <dbl> <dbl> <dbl>\n1 PD1      30     1  0.1 \n2 PD1      30     2  0.3 \n3 PD1      30     3  0.1 \n4 PD1      30     4  0.1 \n5 PD1      40     1  0.3 \n6 PD1      40     2  0.38\n\n\n\n\n7.0.6.2 Visualizing the DPCs\nLet’s have a look at the curves and produce a combo plot figure similar to Fig. 4.17 of the book, but without the line of the predicted values.\n\np3 <- budblight |>\n  ggplot(aes(\n    time, y,\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 1.5) +\n  ylim(0, 0.6) +\n  theme(legend.position = \"none\")+\n  facet_wrap(~treat, ncol =1)+\n  labs(y = \"Disease incidence\",\n       x = \"Time (days after emergence)\")\n\n\np4 <- budblight |>\n  ggplot(aes(\n    time, log(1 / (1 - y)),\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 2) +\n  facet_wrap(~treat, ncol = 1) +\n  theme(legend.position = \"none\")+\n  labs(y = \"Transformed incidence\", x = \"Time (days after emergence)\")\n\np3 | p4\n\n\n\n\n\n\n7.0.6.3 Model fitting\nRemember that the first step in model selection is the visual appraisal of the curve data linearized with the model transformation. In the case the curves represent complete epidemics (close to 100%) appraisal of the absolute rate (difference in y between two times) over time is also helpful.\nFor the treatments above, it looks like the curves are typical of a monocyclic disease (the case of soybean bud blight), for which the monomolecular is usually a good fit, but other models are also possible as well. For this exercise, we will use both the linear and the nonlinear estimation method.\n\n7.0.6.3.1 Linear regression\nFor convenience, we use the fit_multi() to handle multiple epidemics. The function returns a list object where a series of statistics are provided to aid in model selection and parameter estimation. We need to provide the names of columns (arguments): assessment time (time_col), disease incidence (intensity_col), and treatment (strata_cols).\n\nlin1 <- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nLet’s look at how well the four models fitted the data. Epifitter suggests the best fitted model (1 to 4, where 1 is best) for each treatment. Let’s have a look at the statistics of model fitting.\n\n  lin1$Parameters |> \n    select(treat, best_model, model, CCC, RSE)\n\n   treat best_model         model       CCC        RSE\n1    PD1          1 Monomolecular 0.9348429 0.09805661\n2    PD1          2      Gompertz 0.9040182 0.22226189\n3    PD1          3      Logistic 0.8711178 0.44751963\n4    PD1          4   Exponential 0.8278055 0.36124036\n5    PD2          1 Monomolecular 0.9547434 0.07003116\n6    PD2          2      Gompertz 0.9307192 0.17938711\n7    PD2          3      Logistic 0.9062012 0.38773023\n8    PD2          4   Exponential 0.8796705 0.32676216\n9    PD3          1 Monomolecular 0.9393356 0.06832499\n10   PD3          2      Gompertz 0.9288436 0.17156394\n11   PD3          3      Logistic 0.9085414 0.39051075\n12   PD3          4   Exponential 0.8896173 0.33884790\n13   PD4          1      Gompertz 0.9234736 0.17474422\n14   PD4          2 Monomolecular 0.8945962 0.06486949\n15   PD4          3      Logistic 0.8911344 0.52412586\n16   PD4          4   Exponential 0.8739618 0.49769642\n\n\nAnd now we extract values for each parameter estimated from the fit of the monomolecular model.\n\n  lin1$Parameters |>\n  filter(model == \"Monomolecular\") |>\n  select(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.5727700 0.02197351\n2   PD2 -0.5220593 0.01902952\n3   PD3 -0.4491365 0.01590586\n4   PD4 -0.3619898 0.01118047\n\n\nNow we visualize the fit of the monomolecular model (using filter function - see below) to the data together with the observed data and then reproduce the right plots in Fig. 4.17 from the book.\n\nlin1$Data |>\n  filter(model == \"Monomolecular\") |>\n  ggplot(aes(time, predicted)) +\n  geom_point(aes(time, y)) +\n  geom_line(size = 0.5) +\n  facet_wrap(~treat) +\n  coord_cartesian(ylim = c(0, 0.6)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nNow we can plot the means and respective 95% confidence interval of the apparent infection rate (\\(r\\)) and initial inoculum (\\(y_0\\)) for visual inference.\n\np5 <- lin1$Parameters |>\n  filter(model == \"Monomolecular\") |>\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np6 <- lin1$Parameters |>\n  filter(model == \"Monomolecular\") |>\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\np5 | p2\n\n\n\n\n\n\n7.0.6.3.2 Non-linear regression\nTo estimate the parameters using the non-linear approach, we repeat the same arguments in the fit_multi function, but include an additional argument nlin set to TRUE.\n\nnlin1 <- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = TRUE\n)\n\nWarning in log(y0/1): NaNs produced\n\nWarning in log(y0/1): NaNs produced\n\nWarning in log(y0/1): NaNs produced\n\n\nLet’s check statistics of model fit.\n\nnlin1$Parameters |>\n  select(treat, model, CCC, RSE, best_model)\n\n   treat         model       CCC        RSE best_model\n1    PD1 Monomolecular 0.9382991 0.06133704          1\n2    PD1      Gompertz 0.9172407 0.06986307          2\n3    PD1      Logistic 0.8957351 0.07700720          3\n4    PD1   Exponential 0.8544194 0.08799512          4\n5    PD2 Monomolecular 0.9667886 0.04209339          1\n6    PD2      Gompertz 0.9348370 0.05726761          2\n7    PD2      Logistic 0.9077857 0.06657793          3\n8    PD2   Exponential 0.8702365 0.07667322          4\n9    PD3 Monomolecular 0.9570853 0.04269129          1\n10   PD3      Gompertz 0.9261609 0.05443852          2\n11   PD3      Logistic 0.8997106 0.06203037          3\n12   PD3   Exponential 0.8703443 0.06891021          4\n13   PD4 Monomolecular 0.9178226 0.04595409          1\n14   PD4      Gompertz 0.9085579 0.04791331          2\n15   PD4      Logistic 0.8940731 0.05083336          3\n16   PD4   Exponential 0.8842437 0.05267415          4\n\n\nAnd now we obtain the two parameters of interest. Note that the values are not the sames as those estimated using linear regression, but they are similar and highly correlated.\n\n  nlin1$Parameters |>\n    filter(model == \"Monomolecular\") |>\n    select(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.7072562 0.02381573\n2   PD2 -0.6335713 0.02064629\n3   PD3 -0.5048763 0.01674209\n4   PD4 -0.3501234 0.01094368\n\n\n\np7 <- nlin1$Parameters |>\n  filter(model == \"Monomolecular\") |>\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np8 <- nlin1$Parameters |>\n  filter(model == \"Monomolecular\") |>\n  ggplot(aes(treat, y0)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\np7 | p8\n\n\n\n\n\n\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch, eds. 2017. “CHAPTER 4: Temporal Analysis i: Quantifying and Comparing Epidemics.” In, 63–116. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.004."
  },
  {
    "objectID": "temporal-simulation.html",
    "href": "temporal-simulation.html",
    "title": "8  Simulation",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "temporal-simulation.html#hlir-function",
    "href": "temporal-simulation.html#hlir-function",
    "title": "8  Simulation",
    "section": "8.1 HLIR function",
    "text": "8.1 HLIR function\n\nHLIR_fun <- function(t, y, par) {\n  # Variables\n  H <- y[1]\n  L <- y[2]\n  I <- y[3]\n  R <- y[4]\n  beta <- par$beta\n  gama <- par$gama\n  mu <- par$mu\n\n  # Right hand side of the model\n  dH <- -beta * H * I\n  dL <- beta * H * I - gama * L\n  dI <- gama * L - mu * I\n  dR <- mu * I\n  return(list(c(dH, dL, dI, dR)))\n}"
  },
  {
    "objectID": "temporal-simulation.html#setting-up",
    "href": "temporal-simulation.html#setting-up",
    "title": "8  Simulation",
    "section": "8.2 Setting up",
    "text": "8.2 Setting up\n\n# Set up parameters\nbeta <- 0.002 # Per capita rate of infection of susceptible hosts\ngama <- 1 / 5 #  Rate at which exposed (i.e., latently infected) hosts become infectious\ndelta <- 15 # infectious period\nmu <- 1 / delta\nInitCond <- c(1000, 1, 0, 0)"
  },
  {
    "objectID": "temporal-simulation.html#solving-equation",
    "href": "temporal-simulation.html#solving-equation",
    "title": "8  Simulation",
    "section": "8.3 Solving equation",
    "text": "8.3 Solving equation\n\nsteps <- seq(1, 60, by = 1)\nparms <- list(beta = beta, gama = gama, mu = mu)\nHLIR <- ode(InitCond, steps, HLIR_fun, parms)\nepidemics <- data.frame(time = HLIR[, 1], H = HLIR[, 2], L = HLIR[, 3], I = HLIR[, 4], R = HLIR[, 5])"
  },
  {
    "objectID": "temporal-simulation.html#visualizing-hlir-output",
    "href": "temporal-simulation.html#visualizing-hlir-output",
    "title": "8  Simulation",
    "section": "8.4 Visualizing HLIR output",
    "text": "8.4 Visualizing HLIR output\n\nlibrary(ggthemes)\n\n\nAttaching package: 'ggthemes'\n\n\nThe following object is masked from 'package:cowplot':\n\n    theme_map\n\np1 <- epidemics %>%\n  ggplot() +\n  geom_line(aes(time, H, color = \"H\"), size = 2) +\n  geom_line(aes(time, L, color = \"L\"), size = 2) +\n  geom_line(aes(time, I, color = \"I\"), size = 2) +\n  geom_line(aes(time, R, color = \"R\"), size = 2) +\n  geom_line(aes(time, I+R, color = \"DIS\"), size =2, linetype = 2)+\n  labs(y = \"Population size (count)\", x = \"Time (days)\", color = \"Sites\") \n p1"
  },
  {
    "objectID": "spatial-gradients.html",
    "href": "spatial-gradients.html",
    "title": "9  Spatial gradients",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "spatial-gradients.html#introduction",
    "href": "spatial-gradients.html#introduction",
    "title": "9  Spatial gradients",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\nThe assessment of disease in space, in terms of changes in the intensity as it spreads over distance, is called disease gradient. In reality, it is the dispersal (migration) of the pathogen by various means (e.g. wind, vectors, rain, movement of infected material or human mediation) that promotes the spread of plant diseases within a field or across continents and generates the disease gradients. There are two kinds of gradients, the inoculum gradient where host availability is not necessarily required and the disease gradient where the three elements of the disease triangle are required.\nIn disease gradients, assuming that there is only a single source of inoculum, the intensity of the disease decreases more steeply within short distances of the source, and less steeply at greater distances until they reach zero or a low background level of occasional diseased plants. The shapes of the gradients are defined by mechanisms associated with the dispersal of the inoculum which depends on the biology of the pathogen but strongly to environmental factors that affect pathogen dispersal.\nWhen studying disease gradients, researchers need to make sure that there is a well defined single source of inoculum. In gradients, this is called a focus, from where the inoculum originates. The resulting gradients can be classified in two types: primary or secondary. The primary gradient originates only from the initial focus, while the secondary gradient originates from the movement of inoculum produced at previously infected (due to primary gradients) plants to other plants at increasing distances from the source. It is expected that a mix of both kinds of gradients exists as the disease increases over time.\nSimilar to disease progress curves, models can be fitted empirically to observed disease gradient curves and provide insights into the mechanisms of inoculum dispersal and deposition, the source of inoculum, and the physical processes underlying dispersal."
  },
  {
    "objectID": "spatial-gradients.html#models",
    "href": "spatial-gradients.html#models",
    "title": "9  Spatial gradients",
    "section": "9.2 Models",
    "text": "9.2 Models\nWhen modeling disease gradients, the distance is represented by \\(x\\), a continuous variable which can be expressed by various units (cm, m, km, etc). The gradient models, similar to the population dynamics models (disease progress) are of the deterministic type. The difference is that, for disease progress curves, disease intensity tends to increase with increasing time, while in disease gradients the disease intensity tends to decrease with increasing distance from the source of inoculum. Two models are most commonly fitted to data on disease gradients. More details about these models can be obtained it this tutorial.\n\n9.2.1 Exponential model\nThe exponential model is also known as Kiyosawa & Shiyomi model. The differential of the exponential model is given by\n\\(\\frac{dy}{dx}\\) = \\(-b_{E}.y\\) ,\nwhere \\(b_{E}\\) is the exponential form of the rate of decline and \\(y\\) is the disease intensity. This model suggests that \\(y\\) (any disease intensity) is greater close to the source of inoculum, or at the distance zero. The integral form of the model is given by\n\\(y = a . e^{-b.x}\\) ,\nwhere \\(a\\) is the disease intensity at the distance zero and \\(b\\) is the rate of decline, in this case negative because disease intensity decreases with the increase of the distance from inoculum source. Let’s make a plot for two disease gradients of varying parameters for this model.\nFirst we need to load essential packages for programming, customizing the outputs and defining a global ggplot theme.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(cowplot) # for themes \ntheme_set(theme_bw(base_size = 16)) # set global theme\n\nSet the parameters for the exponential model with two rates and same inoculum at the source:\n\na1 <- 0.2 # y at distance zero for gradient 1\na2 <- 0.2 # y at distance zero for gradient 2\nb1 <- 0.1 # decline rate for gradient 1\nb2 <- 0.05 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\nThe following code allows to visualize the model predictions.\n\ndat |>\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) a1 * exp(-b1 * x), linetype = 1) +\n  stat_function(fun = function(x) a2 * exp(-b2 * x), linetype = 2) +\n  ylim(0, a1) +\n  annotate(\"text\", x = 20, y = 0.04, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.10, label = \"b = 0.05\") +\n  labs(\n    title = \"Exponential model\",\n    subtitle = \"\",\n    x = \"Distance (m)\",\n    y = \"Disease incidence (proportion)\"\n  )\n\n\n\n\n\n\n9.2.2 Power law model\nAlso known as the modified Gregory’s model (Gregory was a pioneer in the use this model to describe plant disease gradients). In the power law model, \\(Y\\) is proportional to the power of the distance, and is given by:\n\\(Y = a_{P}.x - b_{P}\\)\nwhere \\(a_{P}\\) and \\(b_{P}\\) are the two parameters of the power law model. They differ from the exponential because as closer to \\(x\\) is to zero, \\(Y\\) is indefinitely large (not meaningful biologically). However, the model can still be useful because it produces realistic values at any distance \\(x\\) away from the source. The values of the \\(a_{P}\\) parameter should be interpreted in accord to the scale of \\(x\\), whether in centimeters or meters. If the distance between the source and the first measure away from the source is 0.5m, it is so more appropriate to record the distance in cm than in m or km.\nOnce \\(y\\) at the distance zero from the source is undefined when using the power law model, this is usually modified by the addition of a positive constant \\(C\\) in \\(x\\):\n\\(Y = a_{P}.(x + C) - b_{P}\\)\nFor this reason, the model is named as the modified power law. Here, the constant \\(C\\) is of the same unit of \\(x\\). At the distance zero, the positive constant is a term that express the size of the inoculum source. In other words, the \\(a\\) parameter is a theoretical value of \\(Y\\) at the distance \\(1-C\\) from the center of the inoculum source.\nLet’s plot two gradients with two rate parameters for the modified power law model:\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\n\ndat2 |>\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) a1 * ((x + C)^-b1), linetype = 1) +\n  stat_function(fun = function(x) a2 * ((x + C)^-b2), linetype = 2) +\n  ylim(0, a1 - 0.02) +\n  annotate(\"text\", x = 20, y = 0.03, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.06, label = \"b = 0.05\") +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"Distance (m)\",\n    y = \"Disease incidence\"\n  )\n\n\n\n\nThe differential equation of the power law model is given by:\n\\(\\frac{dy}{dx}\\) = \\(\\frac{-b_{P}.Y}{x - C}\\)\nSimilar to the exponential model, \\(\\frac{dy}{dx}\\) is proportional to \\(Y\\), meaning that the gradient is steeper (more negative) at the highest disease intensity value, usually closer to the source."
  },
  {
    "objectID": "spatial-gradients.html#linearization-of-the-models",
    "href": "spatial-gradients.html#linearization-of-the-models",
    "title": "9  Spatial gradients",
    "section": "9.3 Linearization of the models",
    "text": "9.3 Linearization of the models\n\n9.3.1 Transformations of y\nThe gradient models, again similar to the temporal disease models, are non linear in their parameters. The model is intrinsically linear if transformations are applied (according to the model) in both sides of the equations. The linear model in its generic state is given by\n\\(y* = a* + bx\\) ,\nwhere the asterisk in \\(a\\) indicated that one of the transformations was applied in \\(y\\) that produced the linear model. Note that \\(a*\\) is the transformed version of the initial disease intensity, which needs to be returned to the original scale according to the respective back-transformation. Follows the linearized form of the two most common gradient models.\n\\(ln(y) = ln(a_{E}) - b_{E}. x\\)\n\\(ln(y) = ln(a_{P}) - b_{E}. ln(x+C)\\)\n\n\n9.3.2 Plot for the linearized form of models\nLet’s visualize the linearization of the exponential model with two different slopes (gradient 1 and 2). Note that the transformation used was \\(ln(y)\\).\nFollows the linearization of the modified power law model.\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |>\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) log(a1) - (b1 * x), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * x), linetype = 2) +\n  labs(\n    title = \"Exponential\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )\n\n\n\n\nFollows the linearization of the modified power law model. Note that the transformation used was \\(ln(y)\\) and \\(ln(x+C)\\) .\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- log(80) # maximum distance for gradient 1\nmax2 <- log(80) # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 |>\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) log(a1) - (b1 * log(x + C)), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * log(x + C)), linetype = 2) +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )"
  },
  {
    "objectID": "spatial-fitting.html",
    "href": "spatial-fitting.html",
    "title": "10  Model fitting",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "spatial-fitting.html#dataset",
    "href": "spatial-fitting.html#dataset",
    "title": "10  Model fitting",
    "section": "10.1 Dataset",
    "text": "10.1 Dataset\nThe hypothetical data below shows a gradient for the number of lesions counted at varying distances in meters from the source. Let’s create two vectors, one for the distances \\(x\\) and the other for the lesion count \\(Y\\), and then a dataframe by combining the two vectors.\n\n# create the two vectors\nx <- c(0.8, 1.6, 2.4, 3.2, 4, 7.2, 12, 15.2, 21.6, 28.8)\nY <- c(184.9, 113.3, 113.3, 64.1, 25, 8, 4.3, 2.5, 1, 0.8)\ngrad1 <- data.frame(x, Y) # create the dataframe\ngrad1 # show the gradient\n\n      x     Y\n1   0.8 184.9\n2   1.6 113.3\n3   2.4 113.3\n4   3.2  64.1\n5   4.0  25.0\n6   7.2   8.0\n7  12.0   4.3\n8  15.2   2.5\n9  21.6   1.0\n10 28.8   0.8"
  },
  {
    "objectID": "spatial-fitting.html#visualize-the-gradient",
    "href": "spatial-fitting.html#visualize-the-gradient",
    "title": "10  Model fitting",
    "section": "10.2 Visualize the gradient",
    "text": "10.2 Visualize the gradient\n\ngrad1 |> \n  ggplot(aes(x, Y))+\n  geom_point()+\n  geom_line()+\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")"
  },
  {
    "objectID": "spatial-patterns.html",
    "href": "spatial-patterns.html",
    "title": "11  Spatial patterns",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing"
  },
  {
    "objectID": "spatial-patterns.html#definitions",
    "href": "spatial-patterns.html#definitions",
    "title": "11  Spatial patterns",
    "section": "11.1 Definitions",
    "text": "11.1 Definitions\nA spatial disease pattern can be defined as the arrangement of diseased entities relative to each other and to the architecture of the host crop (Madden, Hughes, and van den Bosch 2017). Such arrangement is the realization of the underlying dispersal of the pathogen, from one or several sources within and/or outside the area of interest, under the influence of physical, biological and environmental factors.\nThe study of spatial patterns is conducted at a specific time or multiple times during the epidemic. When assessed multiple times, both spatial and temporal processes can be characterized. Because epidemics change over time, it is expected that spatial patterns are not constant but change over time as well. Usually, plant pathologists are interested in determining spatial patterns at one or various spatial scales, depending on the objective of the study. The scale of interest may be a leaf or root, plant, field, municipality, state, country or even intercontinental area. The diseased units observed may vary from lesions on a single leaf to diseased fields in a large production region.\nThe patterns can be classified into two main types that occur naturally: random or aggregated. The random pattern originates because the chances for the units (leaf, plant, crop) to be infected are equal and low, and are largely independent from each other. In aggregated spatial patterns, such chances are unequal and there is dependency among the units. For example, a healthy unit close to a diseased unit is at higher risk than more distant units.\nLet’s simulate in R two vectors (x,y) for the positions of diseased unitss that follow a random or an aggregated pattern. For the random pattern, we use runif, a function which generates random deviates from the uniform distribution.\n\nset.seed(123)          # for reproducibility\nx <- runif(50, 0, 30)  # x vector\ny <- runif(50, 0, 30)  # y vector\ndat <- data.frame(x,y) # dataframe for plotting\n\nNow, the plot to visualize the random pattern.\n\nlibrary(tidyverse) \nlibrary(ggthemes)\ntheme_set(theme_few())\n\npr <- dat |> # R base pipe operator\n  ggplot(aes(x, y))+\n  geom_point(size =3, \n             color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  theme_few()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Random\")\npr\n\n\n\n\nNow, we can generate new x and y vectors using rnbinom function which allows generating values for the negative binomial distribution (which should give rise to aggregated patterns) with parameters size and prob. Let’s simulate 50 values with mean 12 and size 20 as dispersal parameter.\n\nx <- rnbinom(n = 50, mu = 12, size = 20)\ny <- rnbinom(n = 50, mu = 5, size = 20)\ndat2 <- data.frame(x, y)\n\nThis should give us an aggregated pattern.\n\npag <- dat2 |>\n  ggplot(aes(x, y))+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\n  theme_few()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Aggregated\")\npag\n\n\n\n\nA rare pattern found in nature is the regular pattern, but it may be generated artificially by the man when conducting experimentation. Follows a code to produce the regular pattern.\n\nx <- rep(c(0,5,10,15,20, 25, 30, 35, 40, 45), 5) \ny <- rep(c(0, 5, 10, 15, 20, 25, 30, 35, 40, 45), each = 10)\ndat3 <- data.frame(x, y)\n\npreg <- dat3 |>\n  ggplot(aes(x, y))+\n  geom_point(size = 3, color = \"darkred\")+\n  ylim(0,30)+\n  xlim(0,30)+\n  coord_fixed()+\ntheme_few()+\n  labs(x = \"Distance x\", y = \"Distance y\", \n       title = \"Regular\")\npreg\n\nWarning: Removed 51 rows containing missing values (geom_point).\n\n\n\n\n\n\nlibrary(patchwork)\n\npreg + pr + pag\n\n\n\nggsave(\"imgs/spatial.png\", width = 10, height = 4)\n\nWe can create an animation showing the three patterns using gganimate package\n\nrand <- dat |> \n  mutate(i = \"random\")\n\nagg <- dat2 |> \n  mutate(i = \"aggregated\")\n\nreg <- dat3 |> \n  mutate(i = \"regular\")\n\npatterns <- rbind(agg, rand, reg)\n\nlibrary(gganimate)\n\npatterns |> \nggplot(aes(x, y, color = factor(i)))+\n  geom_point(size = 4)+\n  theme_light()+\n  coord_fixed()+\n  ylim(0,25)+\n  xlim(0,25)+\n  transition_states(i)+\n  theme(legend.position = \"none\")+\n  ggtitle('Pattern: {closest_state}')\nanim_save(\"imgs/patterns.gif\")"
  },
  {
    "objectID": "spatial-patterns.html#spatiotemporal",
    "href": "spatial-patterns.html#spatiotemporal",
    "title": "11  Spatial patterns",
    "section": "11.2 Spatiotemporal",
    "text": "11.2 Spatiotemporal\nThe location of diseased plants can be assessed over time and so we can appraise both the progress and pattern of the epidemics. Let’s visualize spatial data collected from actual epidemics monitored (plant is diseased or not diseased) during six times during the epidemics. The data is available in the epiphy R package. Let’s use only one variety and one irrigation type.\n\nlibrary(epiphy)\ntswv_1928 <- tomato_tswv$field_1928\n\ntswv_1928 |>\n  filter(variety == \"Burwood-Prize\"&\n         irrigation == \"trenches\") |> \n  ggplot(aes(x, y, color = factor(i)))+\n  geom_point(aes(group = seq_along(factor(t))), size =2)+\n  coord_fixed()+\n  scale_color_manual(values = c(\"MediumSeaGreen\", \"darkred\"))+\n  labs(fill = \"Status\", title = \"\")+\n  theme_void()+\n  facet_wrap(~ t, nrow =1)\n\n\n\n\nWe can use the gganimate package to visualize the spatiotemporal dynamics of the epidemics.\n\nlibrary(gganimate)\ntswv_1928 |>\n  filter(variety == \"Burwood-Prize\"&\n         irrigation == \"trenches\") |> \n  ggplot(aes(y, x, color = factor(i)))+\n  geom_point(aes(group = seq_along(factor(t))), size =5)+\n  scale_color_manual(values = c(\"MediumSeaGreen\", \"darkred\"))+\n  labs(fill = \"Status\", title = \"\")+\n  theme_void()+\n  coord_fixed()+\n  transition_states(t,\n                  transition_length = 1,\n                    state_length = 1)+\n   ggtitle('Assessment time: {closest_state}')\nanim_save(\"imgs/spatiotemporal.gif\")\n\n\n\n\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch, eds. 2017. “CHAPTER 9: Spatial Aspects of EpidemicsIII: Patterns of Plant Disease.” In, 235–78. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.009."
  },
  {
    "objectID": "spatial-tests.html",
    "href": "spatial-tests.html",
    "title": "12  Statistical tests",
    "section": "",
    "text": "This is a work in progress that is currently undergoing heavy technical editing and copy-editing\nA range of techniques, most based on statistical tests, can be used to detect deviations from randomness in space and the choice of the methods depends on the scale of observation. Usually, more than one test is applied for the same or different scales of interest depending on how the data are collected. The several statistical tests can be classified based on the spatial scale and type of data (binary, count, etc) collected, but mainly if the spatial location of the unit is known (mapped) or not known (sampled). Following Madden et al. (2007), two major groups can be formed. The sparsely sampled (incidence or count data) data or intensively mapped (binary or grouped data) data."
  },
  {
    "objectID": "spatial-tests.html#intensively-mapped",
    "href": "spatial-tests.html#intensively-mapped",
    "title": "12  Statistical tests",
    "section": "12.1 Intensively mapped",
    "text": "12.1 Intensively mapped\n\n12.1.1 Binary data\nIn this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted is a binay variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.\n\n12.1.1.1 Runs test\nA run is defined as a succession of one or more diseased or healthy plants, which are followed and preceded by a plant of the other disease status or no plant at all. There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.\nLet’s create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 20 plants and assign it to y. For plotting purposes, we make a dataframe for more complete information.\n\nlibrary(tidyverse) \nlibrary(gt)\ntheme_set(theme_bw(base_size = 16))\n\n\ny1 <- c(1,1,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,1)\nx1 <- c(1:20) # position of each plant\nz1 <- 1\nrow1 <- data.frame(x1, y1, z1) # create a dataframe\n\nWe can then visualize the series using ggplot and count the number of runs as 7, aided by the color used to identify a run.\n\nrow1 |> \n  ggplot(aes(x1, z1, label = x1, color = factor(y1)))+\n  geom_point(shape =15, size =6)+\n  theme_void()+\n  scale_x_continuous(breaks = max(z1))+\n  scale_color_manual(values = c(\"darkred\", \"MediumSeaGreen\"))+\n  geom_text(vjust = 0, nudge_y = 0.5)+\ncoord_fixed()+\n  ylim(-0.5,2.5)+\n  theme(legend.position = \"right\")+\n  labs(color = \"Status\", title = \"Sequence of diseased (1) or non-diseased (0) units (plants)\", \n       subtitle = \"The numbers represent the position of the unit\")\n\n\n\n\nWe can write a code in R and create a function named oruns.test for the ordinary runs test.\n\noruns.test <- function(x) {\n# identify the sequence\nS <- x \n# Compute the number or runs\nU = max(cumsum(c(1, diff(S)!=0)))\n# Compute the number of diseased plants\nm = sum(S)\n# Count the total number of plants\nN = length(S)\n# Calculate the number of expected runs\nEU = 1 + (2 * m*(N - m)/N)\n# Calculate the standard deviation in the sample\nsU = sqrt(2 * m * (N - m) * (2 * m *(N-m)-N)/ (N^2 *(N-1)))\n# Calculate the z-value\nZ = (U - EU)/sU\n# Obtain the p-value for the Z\npvalue <- (2*pnorm(abs(Z), lower.tail=FALSE))\n# test if Z is lower than 1.64\nresult <- ifelse(Z < 1.64, \nc(\"clustering\"), \nc(\"randomness\"))\n# Print the results\nprint(paste(\"There are\", U,\"runs. The number of expected runs is\", round(EU,1), \"P-value:\",round(pvalue,6), \". Alternative hypothesis: non-randomness\"))\n}\n\nWe can now run the test for the example series above.\n\noruns.test(row1$y1)\n\n[1] \"There are 7 runs. The number of expected runs is 10.6 P-value: 0.084166 . Alternative hypothesis: non-randomness\"\n\n\nThere are built-in functions in R packages that allow for running the ordinary runs test. Let’s load the packages and runt the test. Note that the results of the runs.test is the same as the one produced by our custom function.\n\n#library(randtests)\n#runs.test(row1$y1, threshold = 0.5)\n\nlibrary(DescTools)\nr <- RunsTest(row1$y1)\nr\n\n\n    Runs Test for Randomness\n\ndata:  row1$y1\nruns = 7, m = 12, n = 8, p-value = 0.09595\nalternative hypothesis: true number of runs is not equal the expected number\n\n\n\n\n12.1.1.2 Doublets\nDoublet analysis is used to compare the observed number or adjacent diseased plants, a doublet (DD or 11), to the number expected if the disease were randomly distributed in the yard. If the observed number is greater than the expected number, contagion within the field is suspected.\nLet’s manually produce a code to execute the doublets test. To facilitate, we can create a function and name it doublets.test. The only argument needed is the vector of binary data.\n\ndoublets.test <- function(x) {\n# Identify the sequence\nS <- x\n# Compute the number of doublets Db\nmatrix <- cbind(S[-length(S)], S[-1])\npairs <- table(data.frame(matrix))\nDb <- pairs[2,2]\n# Count the number of diseased plants\nN <- length(S) \n# Count the number of total plants\nm = sum(S) \n# Expected number of doublets\nEDb = m *((m -1)/N)\n# Standard deviation \nSDb = sqrt ( EDb * (1 - (2 / N)))\n# Calculate the Z-value \nZDb = (Db - EDb)/ SDb \n# two-sided P-value calculation\npvalue <- (2*pnorm(abs(ZDb), lower.tail = FALSE))\n# Result of the test\nresult <- ifelse(abs(ZDb) >= 1.64, \nc(\"aggregation or clustering\"), \nc(\"randomness\")) \n# Print the results\nprint(paste(\"There are\",Db,\"doublets. The number of expected doublets is\",EDb,\".\",\"P-value:\", round(pvalue,4), \". Alternative hypothesis: non-randomness\"))\n}\n\n\n# Run the function calling the vector\ndoublets.test(row1$y1)\n\n[1] \"There are 4 doublets. The number of expected doublets is 2.8 . P-value: 0.4497 . Alternative hypothesis: non-randomness\"\n\n\n\n\n12.1.1.3 Join count\nIn this analysis, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The orientation(s) of interest (along rows, across rows, diagonally, or a a combination o these) should be specified. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation.\nIn R, we can use the join.count function of the spdep package to perform a joint count test. First we need to create the series of binary data from top to bottom and left to right. The data are shown in Fig. 9.13 in page 260 of the book chapter on spatial analysis (Madden, Hughes, and van den Bosch 2017). In the example, there are 5 rows and 5 columns. This will be informed later to run the test.\n\n# Enter the data\nS2 <- c(1,0,1,1,0,\n       1,1,0,0,0,\n       1,0,1,0,0,\n       1,0,0,1,0,\n       0,1,0,1,1)\n\nVisualize the two-dimensional array:\n\n# Convert to raster \nmapS2 <- raster::raster(matrix(S2, 5 ,5))\n\n# Convert to data frame\nmapS3 <- raster::as.data.frame(mapS2,xy=TRUE)\n\n# Map using ggplot\nmapS3 |> \n  ggplot(aes(x, y, label = layer, fill = factor(layer)))+\n  geom_tile(color = \"black\", size =0.5)+\n  theme_void()+\n  labs(fill = \"Status\")+\n  scale_fill_manual(values = c(\"MediumSeaGreen\", \"darkred\"))\n\n\n\n\nLoad the library\n\nlibrary(spdep)\n\nFirst, we need to generate a list of neighbors (nb) for a grid of cells. This is performed with the cell2nb function by informing the number of rows and columns. The argument “rook” means shared edge, but it could be the “queen”, for shared edge or vertex. We can use the default.\n\nnb <- cell2nb(nrow = 5, \n              ncol = 5, \n              type=\"rook\")\n\nThe joincount.test function runs the BB join count test for spatial autocorrelation. From the function description, the method uses a spatial weights matrix in weights list form for testing whether same-status joins occur more frequently than would be expected if the zones were labelled in a spatially random way. We need to inform the sequence as factor and the nb object we created previously.\n\njoincount.test(factor(S2), \n                nb2listw(nb))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = -0.58266, p-value = 0.7199\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.9583333             3.2500000             0.2505797 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = -0.66841, p-value = 0.7481\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.4166667             2.7500000             0.2486957 \n\n\nThe function returns a list with a class for each of the status (in this case 0 and 1) with several components. We should look at the P-value. The alternative hypothesis (greater) is that the same status joins occur more frequently than expected if they were labelled in a spatial random way. In this case, we do not reject the null hypothesis of randomness.\nWe can run the ordinary runs and doublets tests, which only considers the adjacent neighbor, for the same series and compare the results.\n\noruns.test(S2)\n\n[1] \"There are 17 runs. The number of expected runs is 13.5 P-value: 0.149673 . Alternative hypothesis: non-randomness\"\n\ndoublets.test(S2)\n\n[1] \"There are 3 doublets. The number of expected doublets is 5.28 . P-value: 0.3009 . Alternative hypothesis: non-randomness\"\n\n\nLet’s repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence to reject the null hypothesis, indicating aggregation of plants.\n\nS3 <- c(1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       0,0,0,0,0)\n\njoincount.test(factor(S3), \n                nb2listw(nb))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = 4.2451, p-value = 1.093e-05\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.3750000             3.2500000             0.2505797 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = 4.5953, p-value = 2.16e-06\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.0416667             2.7500000             0.2486957 \n\noruns.test(S3)\n\n[1] \"There are 8 runs. The number of expected runs is 13.5 P-value: 0.024904 . Alternative hypothesis: non-randomness\"\n\n\nWe can apply these tests for a real example epidemic data provided by the epiphy R package. Let’s work with part of the intensively mapped data on the incidence of tomato spotted wilt virus (TSWV) disease in field trials reported by Cochran (1936) and Bald (1937). First, we need to load the library and then assign one dataframe (the dataset has two dataframes) of the dataset tomato_tswv to a new dataframe called tswv_1929.\n\nlibrary(epiphy)\nlibrary(DT)\nlibrary(cowplot) # theming the ggplot\ntswv_1929 <- tomato_tswv$field_1929\n\ntswv_1929 |> \n  head(6) |> \n  gt()\n\n\n\n\n\n  \n  \n    \n      x\n      y\n      t\n      i\n      n\n    \n  \n  \n    1\n1\n1\n0\n1\n    1\n2\n1\n1\n1\n    1\n3\n1\n0\n1\n    1\n4\n1\n1\n1\n    1\n5\n1\n0\n1\n    1\n6\n1\n0\n1\n  \n  \n  \n\n\n\n\nThe inspection of the first 10 rows of the dataframe shows five variables where x and y are spatial grid coordinates, t is assessment time, i is the status of the plant (0 = healthy, 1 = diseased) and n is the sampling unit size (here all one). Let’s visualize these data for each sampling time.\n\ntswv_1929 |> \n  ggplot(aes(x, y, fill = factor(i)))+\n  geom_tile()+\n  coord_fixed()+\n  scale_fill_manual(values = c(\"MediumSeaGreen\", \"darkred\"))+\n  facet_wrap(~ t)+\n  labs(fill = \"Status\")\n\n\n\n\nCheck the number of rows (y) and columns (x) for further preparing the neighbor object for the join count statistics.\n\ntswv_1929 |> \n  dplyr::select(x, y) |> \n  summary()\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 6.75   1st Qu.:15.75  \n Median :12.50   Median :30.50  \n Mean   :12.50   Mean   :30.50  \n 3rd Qu.:18.25   3rd Qu.:45.25  \n Max.   :24.00   Max.   :60.00  \n\n\nThere are 60 rows and 24 columns.\n\n# Neighbor grid\nnb1 <- cell2nb(nrow = 60, \n              ncol = 24, \n              type=\"rook\")\n\n# Pull the binary sequence of time 1\nS1 <- tswv_1929 |> \n  filter(t == \"1\") |> \n  pull(i)\n\njoincount.test(factor(S1), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = -0.28351, p-value = 0.6116\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           482.000000            482.578874              4.169132 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = -0.059497, p-value = 0.5237\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            23.458333             23.578874              4.104614 \n\n\nWe can apply the join count test for time 2 and time 3. Results show that the pattern changes from random to aggregate over time.\n\n# Pull the binary sequence of time 1\nS2 <- tswv_1929 |> \n  filter(t == \"2\") |> \n  pull(i)\n\njoincount.test(factor(S2), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 0.35872, p-value = 0.3599\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           317.000000            315.900625              9.392312 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 0.34604, p-value = 0.3647\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            82.958333             81.900625              9.342754 \n\n# Pull the binary sequence of time 1\nS3 <- tswv_1929 |> \n  filter(t == \"3\") |> \n  pull(i)\n\njoincount.test(factor(S3), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 1.8541, p-value = 0.03186\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            136.12500             129.92773              11.17243 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 1.7275, p-value = 0.04204\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            243.70833             237.92773              11.19743 \n\n\n\n\n\n12.1.2 Grouped data\nIf the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are autocorrelation (known as Moran’s I), semivariance and SADIE (an alternative approach to autocorrelation.)\n\n12.1.2.1 Autocorrelation\nSpatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity (Madden, Hughes, and van den Bosch 2017).\nWe will illustrate the method by reproducing the example provided in page 264 of the chapter on spatial analysis (Madden, Hughes, and van den Bosch 2017), which was extracted from table 11.3 of Campbell and Madden (1990). The data represent a single transect with the number of Macrophomia phaseolina propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.\n\nmp <- data.frame(\n  i = c(1:16),\n  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)\n)\nmp\n\n    i  y\n1   1 41\n2   2 60\n3   3 81\n4   4 22\n5   5  8\n6   6 20\n7   7 28\n8   8  2\n9   9  0\n10 10  2\n11 11  2\n12 12  8\n13 13  0\n14 14 43\n15 15 61\n16 16 50\n\n\nWe can produce a plot to visualize the number of propagules across the transect.\n\nmp |> \n  ggplot(aes(i, y))+\n  geom_col(fill = \"MediumSeaGreen\")+\n  labs(x = \"Relative position within a transect\", \n       y = \"Number of propagules\",\n       title = \"Macrophomina phaseolina in the soil\",\n       caption = \"Source: Campbell and Madden (1990)\")\n\n\n\n\nTo calculate the autocorrelation coefficient in R, we can use the ac function of the tseries package.\n\nlibrary(tseries)\nac_mp <- acf(mp$y, lag = 5, pl = FALSE)\nac_mp\n\n\nAutocorrelations of series 'mp$y', by lag\n\n     0      1      2      3      4      5 \n 1.000  0.586  0.126 -0.033 -0.017 -0.181 \n\n\nLet’s store the results in a dataframe to facilitate visualization using ggplot.\n\nac_mp_dat <- data.frame(index = ac_mp$lag, ac_mp$acf)\nac_mp_dat\n\n  index   ac_mp.acf\n1     0  1.00000000\n2     1  0.58579374\n3     2  0.12636306\n4     3 -0.03307249\n5     4 -0.01701392\n6     5 -0.18092810\n\n\nAnd now the plot known as autocorrelogram.\n\nac_mp_dat |> \n  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf,3)))+\n  geom_col(fill = \"MediumSeaGreen\")+\n  geom_text(vjust = 0, nudge_y = 0.05)+\n  scale_x_continuous(n.breaks = 6)+\n  geom_hline(yintercept = 0)+\n  labs(x = \"Distance lag\", y = \"Autocorrelation coefficient\")\n\n\n\n\nThe values we obtained here are not the same but quite close to the values reported in Madden et al. (2007). For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.\nThe method above is usually referred to Moran’s I (Moran, 1950). Let’s use another example dataset from the book to calculate the Moran’s I in R. The data is shown in page 269 of the book. The data represent the number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats. It was based on an epidemic generated using the stochastic simulator of Xu and Madden (2004). The data is stored in a csv file.\n\nepi <- read_csv(\"data/xu-madden-simulated.csv\")\n# Transform from wide to long format \n# Pull the n variable to store as a vector\nepi1 <- epi |> \n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |> \n  pull(n)\n\nUsing moran function of the spdep R package.\n\nset.seed(100)\nlibrary(spdep)\n\nThe cell2nb function creates the neighbor list with 12 rows and 12 columns, which is how the 144 quadrats are arranged.\n\nnb <- cell2nb(12, 12, type=\"queen\", torus = FALSE)\n\nThe nb2listw function supplements a neighbors list with spatial weights for the chosen coding scheme. We use the default W, which is the row standardized (sums over all links to n). We then create the col.W neighbor list.\n\ncol.W <- nb2listw(nb, style=\"W\")\n\nThe Moran’s I statistic is given by the moran function\n\nmoran(x = epi1, # numeric vector\n      listw = col.W, # the nb list\n      n = 12, # number of zones\n      S0 = Szero(col.W)) # global sum of weights\n\n$I\n[1] 0.05818595\n\n$K\n[1] 2.878088\n\n\nThe Moran’s test for spatial autocorrelation uses spatial weights matrix in weights list form.\n\nmoran.test(x = epi1, \n           listw = col.W)\n\n\n    Moran I test under randomisation\n\ndata:  epi1  \nweights: col.W    \n\nMoran I statistic standard deviate = 15.919, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.698231416      -0.006993007       0.001962596 \n\n\n\ncorrel_I <- sp.correlogram(nb, epi1, \n                           order = 10,\n                           method = \"I\",  \n                           zero.policy = TRUE)\n\nWe can generate a correlogram using the output of the sp.correlogram function. Note that the figure below is very similar to the one shown in Figure 91.5 in page 269 of the book chapter (Madden, Hughes, and van den Bosch 2017). Let’s store the results in a dataframe.\n\ndf_correl <- data.frame(correl_I$res) |> \n  mutate(lag = c(1:10))\n\n# Show the spatial autocorrelation for 10 distance lags\nround(df_correl$X1,3)\n\n [1]  0.698  0.340  0.086 -0.002 -0.009 -0.024 -0.090 -0.180 -0.217 -0.124\n\n\nThen, we can generate the plot using ggplot.\n\ndf_correl |> \n  ggplot(aes(lag, X1))+\n  geom_col(fill = \"MediumSeaGreen\")+\n  scale_x_continuous(n.breaks = 10)+\n  labs(x = \"Distance lag\", y = \"Spatial autocorrelation\")\n\n\n\n\n\n\n12.1.2.2 Semivariance\nSemi-variance is a key quantity in geostatistics. This differs from spatial autocorrelation because distances are usually measured in discrete spatial lags. The semi-variance can be defined as half the variance of the differences between all possible points spaced a constant distance apart.\nThe semi-variance at a distance d = 0 will be zero, because there are no differences between points that are compared to themselves. However, as points are compared to increasingly distant points, the semi-variance increases. At some distance, called the Range, the semi-variance will become approximately equal to the variance of the whole surface itself. This is the greatest distance over which the value at a point on the surface is related to the value at another point. In fact, when the distance between two sampling units is small, the sampling units are close together and, usually, variability is low. As the distance increases, so (usually) does the variability.\nResults of semi-variance analysis are normally presented as a graphical plot of semi-variance against distance, which is referred to as a semi-variogram. The main characteristics of the semi-variogram of interest are the nugget, the range and the sill, and their estimations are usually based on an appropriate (non-linear) model fitted to the data points representing the semi-variogram.\nFor the semi-variance, we will use the variog function of the geoR package. We need the data in the long format (x, y and z). Let’s reshape the data to the long format and store it in epi2 dataframe.\n\nepi2 <-epi |> \n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |> \n  mutate(y = as.numeric(y))\n\nhead(epi2)\n\n# A tibble: 6 × 3\n      x     y     n\n  <dbl> <dbl> <dbl>\n1     1     1     2\n2     1     2     2\n3     1     3     3\n4     1     4    33\n5     1     5     4\n6     1     6     0\n\n\n\nlibrary(geoR)\n# the coordinates are x and y and the data is the n\nv1 <- variog(coords = epi2[,1:2], data = epi2[,3])\n\nvariog: computing omnidirectional variogram\n\n\n\n# Model fitting\nv2 <- variofit(v1, ini.cov.pars = c(1200, 12), \n               cov.model = \"exponential\", \n               fix.nugget = F)\n\nvariofit: covariance model used is exponential \nvariofit: weights used: npairs \nvariofit: minimisation function used: optim \n\n# Plotting \nplot(v1, xlim = c(0,15))\nlines(v2, lty = 1, lwd = 2)\n\n\n\n\n\n\n12.1.2.3 SADIE\nSADIE (spatial analysis by distance indices) is an alternative to autocorrelation and semi-variance methods described previously, which has found use in plant pathology (Madden, Hughes, and van den Bosch 2017; Xu and Madden 2004; Li, Madden, and Xu 2011). Similar to those methods, the spatial coordinates for the disease intensity (count of diseased individuals) or pathogen propagules values should be provided.\nSADIE quantifies spatial pattern by calculating the minimum total distance to regaularity. That is, the distance that individuals must be moved from the starting point defined by the observed counts to the end point at which there is the same number of individuals in each sampling unit. Therefore, if the data are highly aggregated, the distance to regularity will be large, but if the data are close to regular to start with, the distance to regularity will be smaller.\nThe null hypothesis to test is that the observed pattern is random. SADIE calculates an index of aggregation (Ia). When this is equal to 1, the pattern is random. If this is greater than 1, the pattern is aggregated. Hypothesis testing is based on the randomization procedure. The null hypothesis of randomness, with an alternative hypothesis of aggregation.\nAn extension was made to quantify the contribution of each sampling unit count to the observed pattern. Regions with large counts are defined as patches and regions with small counts are defined as gaps. For each sampling unit, a clustering index is calculated and can be mapped.\nIn R, we can use the sadie function of the epiphy package. The function computes the different indices and probabilities based on the distance to regularity for the observed spatial pattern and a specified number of random permutations of this pattern. To run the analysis, the dataframe should have only three columns: the first two must be the x and y coordinates and the third one the observations. Let’s continue working with the simulated epidemic dataset named epi2. We can map the original data as follows:\n\nepi2 |> \n  ggplot(aes(x, y, label = n, fill = n))+\n  geom_tile()+\n  geom_text(size = 5)+\n  theme_void()+\n  coord_fixed()+\n  scale_fill_gradient(low = \"MediumSeaGreen\", high = \"darkred\")\n\n\n\n\n\nlibrary(epiphy)\nsadie_epi2 <- sadie(epi2)\n\nComputation of Perry's indices:\n\nsadie_epi2\n\nSpatial Analysis by Distance IndicEs (sadie)\n\nCall:\nsadie.data.frame(data = epi2)\n\nIa: 2.4622 (Pa = < 2.22e-16)\n\n\nThe simple output shows the Ia value and associated P-value. As suggested by the low value of the P-value, the pattern is highly aggregated. The summary function provides a more complete information such as the overall inflow and outflow measures. A dataframe with the clustering index for each sampling unit is also provided using the summary function.\n\nsummary(sadie_epi2)\n\n\nCall:\nsadie.data.frame(data = epi2)\n\nFirst 6 rows of clustering indices:\n  x y  i cost_flows      idx_P idx_LMX prob\n1 1 1  2 -11.382725 -7.2242617      NA   NA\n2 1 2  2  -9.461212 -6.2258877      NA   NA\n3 1 3  3  -7.299482 -5.3390880      NA   NA\n4 1 4 33   1.000000  0.8708407      NA   NA\n5 1 5  4  -5.830952 -3.6534511      NA   NA\n6 1 6  0  -5.301329 -2.9627172      NA   NA\n\nSummary indices:\n                      overall    inflow  outflow\nPerry's index        2.495346 -2.811023 2.393399\nLi-Madden-Xu's index       NA        NA       NA\n\nMain outputs:\nIa: 2.4622 (Pa = < 2.22e-16)\n\n'Total cost': 201.6062\nNumber of permutations: 100\n\n\nThe plot function allows to map the clustering indices and so to identify regions of patches (red, outflow) and gaps (blue, inflow).\n\nplot(sadie_epi2)\n\n\n\n\nA isocline plot can be obtained by setting the isocline argument as TRUE.\n\nplot(sadie_epi2, isoclines = TRUE)"
  },
  {
    "objectID": "spatial-tests.html#sparsely-sampled-data",
    "href": "spatial-tests.html#sparsely-sampled-data",
    "title": "12  Statistical tests",
    "section": "12.2 Sparsely sampled data",
    "text": "12.2 Sparsely sampled data\nDifferent from intensively mapped data, sparsely sampled data do not contain information about the spatial location of the units, and so it is not taken into account in the analysis. The analysis of sparsely sampled data usually involves characterizing the extent of variability in the mean level of disease intensity per sampling unit (Madden, Hughes, and van den Bosch 2017). There are two types of approaches to analyse these data in the context of spatial patterns of plant disease epidemics: 1) testing the goodness of fit to statistical probability distributions and 2) calculating indices of aggregation. These will be discussed further separated depending on the nature of the data, whether count or incidence (proportion), for which specific distributions are assumed to describe the data.\n\n12.2.1 Count data\n\n12.2.1.1 Fit to distributions\nTwo statistical distributions can be adopted as reference for the description of random or aggregated patterns of disease data in the form of counts of infection within sampling units. Take the count of lesions on a leaf, or the count of diseased plants on a quadrat, as an example. If the presence of a lesion/diseased plant does not increase or decrease the chance that other lesions/diseased plants will occur, the Poisson distribution describes the distribution of lesions on the leaf. Otherwise, the negative binomial provides a better description.\nLet’s work with the previous simulation data of 144 quadrats with a variable count of diseased plants per quadrat (in a maximum of 100). Notice that we won’t consider the location of each quadrat as in the previous analyses of intensively mapped data. We only need the vector with the number of infected units per sampling unit.\nThe epiphy package provides a function called fit_two_distr, which allows fitting these two distribution for count data. In this case, either randomness assumption (Poisson distributions) or aggregation assumption (negative binomial) are made, and then, a goodness-of-fit comparison of both distributions is performed using a log-likelihood ratio test. The function requires a dataframe created using the count function where the number of infection units is designated as i. It won’t work with a single vector of numbers. We create the dataframe using:\n\ndata_count <- epi2 |> \n  mutate(i = n) |>  # create i vector\n  epiphy::count()   # create the map object of count class\n\nWe can now run the function that will look fo the the vector i. The function returns a list of four components including the outputs of the fitting process for both distribution and the result of the log-likelihood ratio test, the llr.\n\nfit_data_count <- fit_two_distr(data_count)\nsummary(fit_data_count)\n\nFitting of two distributions by maximum likelihood\nfor 'count' data.\nParameter estimates:\n\n(1) Poisson (random):\n       Estimate  Std.Err Z value    Pr(>z)    \nlambda 27.85417  0.43981  63.333 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Negative binomial (aggregated):\n       Estimate    Std.Err Z value    Pr(>z)    \nk     0.6327452  0.0707846  8.9390 < 2.2e-16 ***\nmu   27.8541667  2.9510198  9.4388 < 2.2e-16 ***\nprob  0.0222118  0.0033463  6.6378 3.184e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nfit_data_count$llr\n\nLikelihood ratio test\n\n               LogLik Df  Chisq Pr(>Chisq)    \nrandom :     -2654.71                         \naggregated :  -616.51  1 4076.4  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe very low value of the P-value of the LLR test suggest that the negative binomial provides a better fit to the data. The plot function allows for visualizing the expected random and aggregated frequencies together with the observed frequencies. The number of breaks can be adjusted as indicated.\n\nplot(fit_data_count, breaks = 5) \n\n\n\n\nSee below another way to plot by extracting the frequency data (and pivoting from wide to long format) from the generated list and using ggplot. Clearly, the negative binomial is a better description for the observed count data.\n\ndf <- fit_data_count$freq |> \n  pivot_longer(2:4, \"pattern\", \"value\")\n\ndf |> \n  ggplot(aes(category, value, fill = pattern))+\n  geom_col(position = \"dodge\", width = 2)+\n  scale_fill_manual(values = c(\"darkred\", \"MediumSeaGreen\", \"steelblue\"))+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n12.2.1.2 Indices of aggregation\n\nidx <- agg_index(data_count, method = \"fisher\")\nidx\n\nFisher's index of dispersion:\n(Version for count data)\n34.25\n\nchisq.test(idx)\n\n\n    Chi-squared test for (N - 1)*index following a chi-squared\n    distribution (df = N - 1)\n\ndata:  idx\nX-squared = 4897.2, df = 143, p-value < 2.2e-16\n\nz.test(idx)\n\n\n    One-sample z-test\n\ndata:  idx\nz = 82.085, p-value < 2.2e-16\nalternative hypothesis: two.sided\n\n# Lloyd index\n\nidx_lloyd <- agg_index(data_count, method = \"lloyd\")\nidx_lloyd\n\nLloyd's index of patchiness:\n2.194\n\nidx_mori <- agg_index(data_count, method = \"morisita\")\nidx_mori\n\nMorisita's coefficient of dispersion:\n(Version for count data)\n2.186\n\n# Using the vegan package\nlibrary (vegan)\n\nLoading required package: permute\n\n\nLoading required package: lattice\n\n\nThis is vegan 2.5-7\n\nz <- data_count$data$i\nmor <- dispindmorisita(z)\nmor\n\n      imor     mclu      muni      imst pchisq\n1 2.185591 1.008728 0.9922162 0.5041152      0\n\n\n\n\n\n12.2.2 Incidence data\n\nfhb <- read_csv(\"data/fhb.csv\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  campo = col_double(),\n  q = col_double(),\n  espiga = col_double(),\n  status = col_double()\n)\n\nfhb2 <- fhb %>%\n  filter(campo == 1) \nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\nglm1 <-glmer(status ~ q + (1 | q), family = \"binomial\", data = fhb2)\nperformance::icc(glm1)\n\n# Intraclass Correlation Coefficient\n\n     Adjusted ICC: 0.641\n  Conditional ICC: 0.600\n\n\n\ntas <- read.csv(\"https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EcologyAndEpidemiologyInR/SpatialAnalysis/Documents/tasmania_test_1.txt\", sep = \"\")\ntas\n\n   quad group_size count\n1     1          6     4\n2     2          6     6\n3     3          6     6\n4     4          6     6\n5     5          6     6\n6     6          6     6\n7     7          6     6\n8     8          6     6\n9     9          6     4\n10   10          6     6\n11   11          6     5\n12   12          6     6\n13   13          6     6\n14   14          6     6\n15   15          6     6\n16   16          6     6\n17   17          6     6\n18   18          6     3\n19   19          6     3\n20   20          6     2\n21   21          6     4\n22   22          6     3\n23   23          6     6\n24   24          6     3\n25   25          6     3\n26   26          6     6\n27   27          6     6\n28   28          6     6\n29   29          6     6\n30   30          6     6\n31   31          6     6\n32   32          6     6\n33   33          6     6\n34   34          6     5\n35   35          6     6\n36   36          6     6\n37   37          6     6\n38   38          6     6\n39   39          6     6\n40   40          6     6\n41   41          6     6\n42   42          6     6\n43   43          6     4\n44   44          6     6\n45   45          6     5\n46   46          6     6\n47   47          6     6\n48   48          6     6\n49   49          6     6\n50   50          6     5\n51   51          6     6\n52   52          6     6\n53   53          6     6\n54   54          6     6\n55   55          6     6\n56   56          6     6\n57   57          6     6\n58   58          6     3\n59   59          6     6\n60   60          6     6\n61   61          6     6\n62   62          6     6\n\n# Create incidence object for epiphy\ndat_tas <- tas |> \n  mutate(n = group_size, i = count) |> \n  epiphy::incidence()\n\n## Fit to two distributions\nfit_tas <- fit_two_distr(dat_tas)\n\nWarning in chisq.test2(freq$observed, freq$random, n_est = n_est[[\"random\"]], :\nChi-squared approximation may be incorrect.\n\n\nWarning in chisq.test2(freq$observed, freq$aggregated, n_est =\nn_est[[\"aggregated\"]], : Chi-squared approximation may be incorrect.\n\nsummary(fit_tas)\n\nFitting of two distributions by maximum likelihood\nfor 'incidence' data.\nParameter estimates:\n\n(1) Binomial (random):\n     Estimate Std.Err Z value    Pr(>z)    \nprob  0.90860 0.01494  60.819 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Beta-binomial (aggregated):\n      Estimate  Std.Err Z value    Pr(>z)    \nalpha 1.923479 0.869621  2.2119  0.026976 *  \nbeta  0.181337 0.075641  2.3973  0.016514 *  \nprob  0.913847 0.023139 39.4943 < 2.2e-16 ***\nrho   0.322080 0.096414  3.3406  0.000836 ***\ntheta 0.475101 0.209789  2.2647  0.023534 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nfit_tas$llr \n\nLikelihood ratio test\n\n              LogLik Df  Chisq Pr(>Chisq)    \nrandom :     -75.061                         \naggregated : -57.430  1 35.263   2.88e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot(fit_tas)\n\n\n\n\n\ntas |> \n  dplyr::count(count)\n\n  count  n\n1     2  1\n2     3  6\n3     4  4\n4     5  4\n5     6 47\n\nbinom.tas = glm(cbind(count, group_size - count)~1,\n              family=binomial,\n              data = tas)\n\nlibrary(aod)\nbbinom.tas <- betabin(cbind(count, group_size - count)~1, ~ 1, data = tas)\nsummary(bbinom.tas)\n\nBeta-binomial model\n-------------------\nbetabin(formula = cbind(count, group_size - count) ~ 1, random = ~1, \n    data = tas)\n\nConvergence was obtained after 45 iterations.\n\nFixed-effect coefficients:\n             Estimate Std. Error   z value Pr(> |z|)\n(Intercept) 2.361e+00  2.939e-01 8.034e+00 8.882e-16\n\nOverdispersion coefficients:\n                 Estimate Std. Error   z value  Pr(> z)\nphi.(Intercept) 3.221e-01  9.642e-02 3.341e+00 4.18e-04\n\nLog-likelihood statistics\n   Log-lik      nbpar    df res.   Deviance        AIC       AICc \n-5.743e+01          2         60   8.25e+01  1.189e+02  1.191e+02 \n\nsummary(binom.tas)\n\n\nCall:\nglm(formula = cbind(count, group_size - count) ~ 1, family = binomial, \n    data = tas)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.447   1.073   1.073   1.073   1.073  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   2.2967     0.1799   12.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 117.76  on 61  degrees of freedom\nResidual deviance: 117.76  on 61  degrees of freedom\nAIC: 152.12\n\nNumber of Fisher Scoring iterations: 5\n\nlibrary(performance)\ncheck_overdispersion(binom.tas)\n\n# Overdispersion test\n\n       dispersion ratio =   2.348\n  Pearson's Chi-Squared = 143.206\n                p-value = < 0.001\n\n\nOverdispersion detected.\n\nlibrary(epiphy)\n\ntas2 <- tas |> \n  mutate(i = count,\n         n = group_size) |>  # create i vector\n  epiphy::incidence() \n\nt <- agg_index(tas2, flavor = \"incidence\")\ncalpha.test(t)\n\n\n    C(alpha) test\n\ndata:  t\nz = 7.9886, p-value = 1.365e-15\n\n\n\ndw <- dogwood_anthracnose |> \n  filter(t == 1990) |> \n  epiphy::incidence()\n\nf <- fit_two_distr(dw)\n\nWarning in chisq.test2(freq$observed, freq$random, n_est = n_est[[\"random\"]], :\nChi-squared approximation may be incorrect.\n\n\nWarning in chisq.test2(freq$observed, freq$aggregated, n_est =\nn_est[[\"aggregated\"]], : Chi-squared approximation may be incorrect.\n\nplot(f)\n\n\n\nsummary(f)\n\nFitting of two distributions by maximum likelihood\nfor 'incidence' data.\nParameter estimates:\n\n(1) Binomial (random):\n      Estimate   Std.Err Z value    Pr(>z)    \nprob 0.1601190 0.0089467  17.897 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Beta-binomial (aggregated):\n      Estimate  Std.Err Z value    Pr(>z)    \nalpha 0.292374 0.050272  5.8159 6.032e-09 ***\nbeta  1.622537 0.307765  5.2720 1.349e-07 ***\nprob  0.152683 0.017410  8.7698 < 2.2e-16 ***\nrho   0.343064 0.040727  8.4235 < 2.2e-16 ***\ntheta 0.522217 0.094371  5.5337 3.136e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nf$llr\n\nLikelihood ratio test\n\n              LogLik Df  Chisq Pr(>Chisq)    \nrandom :     -417.47                         \naggregated : -273.29  1 288.35  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nag <- agg_index(dw)\ncalpha.test(ag)\n\n\n    C(alpha) test\n\ndata:  ag\nz = 27.297, p-value < 2.2e-16\n\n\n\n\n\n\nLi, Baohua, Laurence V. Madden, and Xiangming Xu. 2011. “Spatial Analysis by Distance Indices: An Alternative Local Clustering Index for Studying Spatial Patterns.” Methods in Ecology and Evolution 3 (2): 368–77. https://doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch, eds. 2017. “CHAPTER 9: Spatial Aspects of EpidemicsIII: Patterns of Plant Disease.” In, 235–78. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.009.\n\n\nXu, X.-M., and L. V. Madden. 2004. “Use of SADIE Statistics to Study Spatial Dynamics of Plant Disease Epidemics.” Plant Pathology 53 (1): 38–49. https://doi.org/10.1111/j.1365-3059.2004.00949.x."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barnhart, Huiman X., Michael Haber, and Jingli Song. 2002.\n“Overall Concordance Correlation Coefficient for Evaluating\nAgreement Among Multiple Observers.” Biometrics 58 (4):\n1020–27. https://doi.org/10.1111/j.0006-341x.2002.01020.x.\n\n\nBock, Clive H., Sarah J. Pethybridge, Jayme G. A. Barbedo, Paul D.\nEsker, Anne-Katrin Mahlein, and Emerson M. Del Ponte. 2021. “A\nPhytopathometry Glossary for the Twenty-First Century: Towards\nConsistency and Precision in Intra- and Inter-Disciplinary\nDialogues.” Tropical Plant Pathology 47 (1): 14–24. https://doi.org/10.1007/s40858-021-00454-0.\n\n\nJeger, M. J., and S. L. H. Viljanen-Rollinson. 2001. “The Use of\nthe Area Under the Disease-Progress Curve (AUDPC) to Assess Quantitative\nDisease Resistance in Crop Cultivars.” Theoretical and\nApplied Genetics 102 (1): 32–40. https://doi.org/10.1007/s001220051615.\n\n\nLi, Baohua, Laurence V. Madden, and Xiangming Xu. 2011. “Spatial\nAnalysis by Distance Indices: An Alternative Local Clustering Index for\nStudying Spatial Patterns.” Methods in Ecology and\nEvolution 3 (2): 368–77. https://doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nLin, Lawrence I-Kuei. 1989. “A Concordance Correlation Coefficient\nto Evaluate Reproducibility.” Biometrics 45 (1): 255. https://doi.org/10.2307/2532051.\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch. 2017.\n“The Study of Plant Disease Epidemics,” August. https://doi.org/10.1094/9780890545058.\n\n\nMadden, Laurence V., Gareth Hughes, and Frank van den Bosch, eds. 2017a.\n“CHAPTER 4: Temporal Analysis i: Quantifying and Comparing\nEpidemics.” In, 63–116. The American Phytopathological Society.\nhttps://doi.org/10.1094/9780890545058.004.\n\n\n———, eds. 2017b. “CHAPTER 9: Spatial Aspects of\nEpidemicsIII: Patterns of Plant Disease.” In,\n235–78. The American Phytopathological Society. https://doi.org/10.1094/9780890545058.009.\n\n\nNutter, Forrest W., Paul D. Esker, and Rosalee A. Coelho Netto. 2006.\n“Disease Assessment Concepts and the Advancements Made in\nImproving the Accuracy and Precision of Plant Disease Data.”\nEuropean Journal of Plant Pathology 115 (1): 95–103. https://doi.org/10.1007/s10658-005-1230-z.\n\n\nShrout, Patrick E., and Joseph L. Fleiss. 1979. “Intraclass\nCorrelations: Uses in Assessing Rater Reliability.”\nPsychological Bulletin 86 (2): 420–28. https://doi.org/10.1037/0033-2909.86.2.420.\n\n\nSimko, Ivan, and Hans-Peter Piepho. 2012. “The Area Under the\nDisease Progress Stairs: Calculation, Advantage, and\nApplication.” Phytopathology® 102 (4): 381–89. https://doi.org/10.1094/phyto-07-11-0216.\n\n\nXu, X.-M., and L. V. Madden. 2004. “Use of SADIE Statistics to\nStudy Spatial Dynamics of Plant Disease Epidemics.” Plant\nPathology 53 (1): 38–49. https://doi.org/10.1111/j.1365-3059.2004.00949.x."
  },
  {
    "objectID": "data-actual-severity.html#color-palettes",
    "href": "data-actual-severity.html#color-palettes",
    "title": "4  Measuring severity",
    "section": "4.2 Color palettes",
    "text": "4.2 Color palettes\nThe most critical is the initial step, when the user needs to correctly define the color palettes. In pliman the palettes are actually separate images representing each of three classes named background (b), symptomatic (s) and healthy (h).\nThe reference image palettes can be made simply by manually sampling small areas of the image and producing a composite image. Of course, the results may vary depending on how these areas are chosen, and are subjective in nature due to how researcher prepare the palettes. A work on the validation of the pliman to determine disease severity showed the effect of different palettes prepared independently by three researchers (Olivoto et al. 2022)\nThe observation of the processed masks becomes important to create image palettes that are most representative of the respective class.\nHere, I cut and pasted several sections of images representative of each class from a few leaves into a Google slide. Once the image palette was ready, I exported each one as a separate image PNG file (JPG also works). These were named: sbr_b.png, sbr_h.png and sbr_s.png.\n\n\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image analysis. Methods in Ecology and Evolution. 13:789–798 Available at: http://dx.doi.org/10.1111/2041-210X.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring plant disease severity in R: introducing and evaluating the pliman package. Tropical Plant Pathology. 47:95–104 Available at: http://dx.doi.org/10.1007/s40858-021-00487-5."
  },
  {
    "objectID": "data-actual-severity.html#defining-color-palettes",
    "href": "data-actual-severity.html#defining-color-palettes",
    "title": "4  Measuring severity",
    "section": "4.2 Defining color palettes",
    "text": "4.2 Defining color palettes\nThe most critical is the initial step, when the user needs to correctly define the color palettes. In pliman the palettes are actually separate images representing each of three classes named background (b), symptomatic (s) and healthy (h).\nThe reference image palettes can be made simply by manually sampling small areas of the image and producing a composite image. Of course, the results may vary depending on how these areas are chosen. A work on the validation of the pliman to determine disease severity showed the effect of different palettes prepared independently by three researchers (Olivoto et al. 2022). The observation of the processed masks during the calibration of the palettes is important to create reference palettes that are most representative of the respective class.\nHere, I cut and pasted several sections of images representative of each class from a few leaves into a Google slide. Once the image palette was ready, I exported each one as a separate image PNG file (JPG also works). These were named: sbr_b.png, sbr_h.png and sbr_s.png.\n\nNow that we have the image palettes, we can start by importing the image palettes into the environment for further analysis. Let’s create an image object for each palette named h (healthy), s (symptoms) and b (background).\n\nlibrary(pliman)\nh <- image_import(\"imgs/sbr_h.png\")\ns <- image_import(\"imgs/sbr_s.png\")\nb <- image_import(\"imgs/sbr_b.png\")\n\nWe can visualize the imported images using the image_combine() function.\n\nimage_combine(h, s, b, ncol =3)"
  },
  {
    "objectID": "data-actual-severity.html#measuring-severity",
    "href": "data-actual-severity.html#measuring-severity",
    "title": "4  Measuring severity",
    "section": "4.3 Measuring severity",
    "text": "4.3 Measuring severity\n\n4.3.1 Single image\nTo determine severity in a single image (img46.png), the image file needs to be loaded and assigned to an object using the same image_import() function used to load the palettes for each of the predefined classes. We can then visualize the image, again using image_combine().\n\nimg <- image_import(\"imgs/originals/img46.png\")\nimage_combine(img)\n\n\n\n\nNow the fun begins with the measure_disease() function to determine severity. Four arguments are needed, the one representing the target image and each of the three images of the color palettes. As the author of the package says “pliman will take care of all details!”\n\nset.seed(123)\nmeasure_disease(\n  img = img,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_image = TRUE\n)\n\n\n\n\n$severity\n   healthy symptomatic\n1 92.68302    7.316983\n\n$shape\nNULL\n\n$statistics\nNULL\n\nattr(,\"class\")\n[1] \"plm_disease\"\n\n\n\n\n4.3.2 Multiple images\nThat was fun, but usually we don’t have a single image to process but several. It would take a longer time to process each one using the above procedure, thus becoming tedious.\nTo automate the process, pliman offers a batch processing approach. For such, instead of using img argument, one can use img_pattern and define the prefix of names of the images. In addition, we also need to define the folder where the original files are located.\nIf the users wants to save the processed masks, the save_image argument needs to be set to TRUE and the directory where the images will be saved also should be informed. Check below how to process 10 images of soybean rust symptoms. The outcome is a list object with the measures of the percent healthy and percent symptomatic area for each leaf in the severity object.\n\npliman <- measure_disease(\n  pattern = \"img\",\n  dir_original = \"imgs/originals\" ,\n  dir_processed = \"imgs/processed\",\n  save_image = TRUE,\n  img_healthy = h,\n  img_symptoms = s,\n  img_background = b,\n  show_image = FALSE,\n  col_leaf = \"green\", #leaf color\n  col_lesions = \"red\", #lesion color\n  col_background = \"black\", #background color\n  show_original = FALSE #do not show original \n)\n\nProcessing image img11 |====                                     | 10% 00:00:00 \n\n\nProcessing image img35 |========                                 | 20% 00:00:03 \n\n\nProcessing image img37 |============                             | 30% 00:00:05 \n\n\nProcessing image img38 |================                         | 40% 00:00:06 \n\n\nProcessing image img46 |====================                     | 50% 00:00:07 \n\n\nProcessing image img5 |=========================                 | 60% 00:00:09 \n\n\nProcessing image img63 |=============================            | 70% 00:00:12 \n\n\nProcessing image img67 |=================================        | 80% 00:00:16 \n\n\nProcessing image img70 |=====================================    | 90% 00:00:19 \n\n\nProcessing image img75 |=========================================| 100% 00:00:20 \n\nseverity <- pliman$severity\nseverity\n\n     img  healthy symptomatic\n1  img11 70.80072  29.1992835\n2  img35 46.96430  53.0357002\n3  img37 60.49390  39.5060986\n4  img38 79.14737  20.8526306\n5  img46 93.15143   6.8485680\n6   img5 20.53977  79.4602312\n7  img63 97.15698   2.8430190\n8  img67 99.83723   0.1627709\n9  img70 35.58683  64.4131683\n10 img75 93.04517   6.9548329\n\n\nWith the argument save_image set to TRUE, the images are all saved in the folder with the standard prefix “proc.”\n\nLet’s have a look at one of the processed images."
  },
  {
    "objectID": "data-actual-severity.html#how-good-are-these-measurements",
    "href": "data-actual-severity.html#how-good-are-these-measurements",
    "title": "4  Measuring severity",
    "section": "4.4 How good are these measurements?",
    "text": "4.4 How good are these measurements?\nThese 10 images were previously processed in QUANT software for measuring severity which is also based on image thresholding . Let’s create a tibble for the image code and respective “actual” severity - assuming QUANT’s measures as reference.\n\nlibrary(tidyverse)\nquant <- tribble(\n  ~img, ~actual,\n   \"img5\",     75,\n  \"img11\",     24,\n  \"img35\",     52,\n  \"img37\",     38,\n  \"img38\",     17,\n  \"img46\",      7,\n  \"img63\",    2.5,\n  \"img67\",   0.25,\n  \"img70\",     67,\n  \"img75\",     10\n  )\n\nWe can now combine the two dataframes and produce a scatter plot relating the two measures.\n\ndat <- left_join(severity, quant)\n\nJoining, by = \"img\"\n\ndat %>%\n  ggplot(aes(actual, symptomatic)) +\n  geom_point(size = 5, shape = 16, color = \"gray50\") +\n  ylim(0, 100) +\n  xlim(0, 100) +\n  geom_abline(slope = 1, intercept = 0) +\n  theme_light() +\n  labs(x = \"Quant\",\n       y = \"pliman\")\n\n\n\n\nThe concordance correlation coefficient is a test for agreement between two observers or two methods (see previous chapter). It is an indication of how accurate the pliman measures are compared with a standard. The coefficient is greater than 0.99 (1.0 is perfect concordance), suggesting an excellent agreement!\n\nlibrary(epiR)\nccc <- epi.ccc(dat$actual, dat$symptomatic)\nccc$rho.c\n\n        est     lower     upper\n1 0.9940941 0.9774812 0.9984606"
  },
  {
    "objectID": "data-actual-severity.html#conclusion",
    "href": "data-actual-severity.html#conclusion",
    "title": "4  Measuring severity",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nThe community of R users may enjoy using pliman as an alternative to proprietary software or other point-and-click open source solutions such as imageJ. The simplicity of the batch processing approach can greatly improve the speed of the assessment and the user can set arguments to run R in parallel for enhanced computational speed.\nThe most critical step, as I mentioned, is the definition of the reference image palettes. A few preliminary runs may be needed for a few images to check whether the segmentation is being performed correctly, based on visual judgement. This is no different than any other color-threshold based methods when the choices made by the user affect the final result and contribute to variation among assessors. The cons are the same encountered in the direct competitors, which is the necessity to have images obtained at uniform and controlled conditions, especially a contrasting background.\n\n\n\n\nDel Ponte, E. M., Pethybridge, S. J., Bock, C. H., Michereff, S. J., Machado, F. J., and Spolti, P. 2017. Standard Area Diagrams for Aiding Severity Estimation: Scientometrics, Pathosystems, and Methodological Trends in the Last 25 Years. Phytopathology®. 107:1161–1174 Available at: http://dx.doi.org/10.1094/PHYTO-02-17-0069-FI.\n\n\nOlivoto, T. 2022. Lights, camera, pliman! An R package for plant image analysis. Methods in Ecology and Evolution. 13:789–798 Available at: http://dx.doi.org/10.1111/2041-210X.13803.\n\n\nOlivoto, T., Andrade, S. M. P., and M. Del Ponte, E. 2022. Measuring plant disease severity in R: introducing and evaluating the pliman package. Tropical Plant Pathology. 47:95–104 Available at: http://dx.doi.org/10.1007/s40858-021-00487-5."
  }
]