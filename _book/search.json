[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R4PDE",
    "section": "",
    "text": "R for Plant Disease Epidemiology (R4PDE) is a book project in the development stage. It is based on the teaching notes of my graduate course - FIP 602 - Plant Disease Epidemiology, offered every year for students of the Graduate Program in Plant Pathology of the Universidade Federal de Viçosa.\nThis book is for those interested in studying and modelling plant disease epidemics using R programming. Here, I provide context and showcase several methods for describing, visualizing and analyzing epidemic data collected over time and/or space.\nUsers should have have a minimum knowledge of R programming. I make use of custom functions as well as several general and a few specific R packages for conducting the most common tasks related with the analysis of plant disease epidemiology data, in particular epifitter and epiphy. In most part, I use data and try to reproduce the several analyses shown in the book The study of Plant Disease Epidemics (Madden et al. 2017)\n\n\n\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den. 2017. The study of plant disease epidemics. Available at: http://dx.doi.org/10.1094/9780890545058."
  },
  {
    "objectID": "temporal.html",
    "href": "temporal.html",
    "title": "1  Temporal analysis",
    "section": "",
    "text": "A key understanding of the epidemics relates to the knowledge of rates and patterns. Epidemics can be viewed as dynamic systems that change their state as time goes. The first and simplest way to characterize such changes in time is to produce a graphical plot called disease progress curve (DPC). This curve can be obtained as long as the intensity of the disease (y) in the host population is assessed sequentially in time (t).\nA DPC summarizes the interaction of the three main components of the disease triangle occurring during the epidemic. The curves can vary greatly in shape according to variations in each of the components, in particular due to management practices that alter the course of the epidemics and for which the goal is to stop disease increase. We can create a dataframe in R for a single DPC and make a plot using ggplot. By convention we use t for time and y for disease intensity, expressed in proportion (0 to 1).\nFirstly, let’s load the essential R packages and set up the environment.\n\nlibrary(tidyverse) # essential packages \nlibrary(cowplot) # for themes \ntheme_set(theme_minimal_grid()) # set global theme\n\nThere are several ways to create a dataframe in R. I like to use the tribble function as below. The entered data will be assigned to a dataframe called dpc.\n\ndpc <- \n  tribble(\n   ~t,  ~`y`, \n   0,  0.08, \n   7,  0.13, \n  14,  0.78, \n  21,  0.92, \n  28,  0.99, \n  35, 0.995, \n  42, 0.999, \n  )\n\nNow the plot\n\n\n\n\n\n\nNote\n\n\n\nNote that I use the pipe operator %>% in my programming to express a sequence of multiple operations in a more intuitive way.\n\n\n\ndpc %>%\n  ggplot(aes(t, y)) +\n  geom_point(size = 2)+\n  geom_line(size = 1)+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity\")"
  },
  {
    "objectID": "temporal.html#curve-descriptors-audpc",
    "href": "temporal.html#curve-descriptors-audpc",
    "title": "1  Temporal analysis",
    "section": "1.2 Curve descriptors: AUDPC",
    "text": "1.2 Curve descriptors: AUDPC\nThe depiction and analysis of disease progress curves can provide useful information for gaining understanding of the underlying epidemic process. The curves are extensively used to evaluate how disease control measures affect epidemics. When characterizing DPCs, a researcher may be interested in describing and comparing epidemics that result from different treatments, or simply in their variations as affected by changes in environment, host or pathogen.\nThe precision and complexity of the analysis of progress curve data depends on the objective of the study. In general, the goal is to synthesize similarities and different among epidemics based on common descriptors of the disease progress curves. For example, the simple appraisal of the disease intensity at any time during the course of the epidemic should be sufficient for certain situations. Furthermore, a few descriptors can be extracted including the epidemic duration, the initial and maximum disease, and the area under the disease progress curve (AUDPC).\nThe AUDPC summarizes the “total measure of disease stress” and is largely used to compare epidemics (Jeger and Viljanen-Rollinson 2001). The most common approach to calculate AUDPC is the trapezoidal method, which splits the disease progress curves into a series of rectangles, calculating the area of each of them and then summing the areas. Let’s extend the plot code to show those rectangles using the annotate function.\n\ndpc %>%\n  ggplot(aes(t, y)) +\n  geom_point(size = 2)+\n  geom_line(size = 1)+\n  labs(x = \"Assessment time (days)\",\n       y = \"Disease intensity\",\n       title =\"Area under the disease progress curve\",\n       subtitle = \"Calculated using the trapezoidal method\")+\n  annotate(\"rect\", xmin = dpc$t[1], xmax = dpc$t[2], \n           ymin = 0, ymax = (dpc$y[1]+ dpc$y[2])/2, \n           color = \"black\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[2], xmax = dpc$t[3], \n            ymin = 0, ymax = (dpc$y[2]+ dpc$y[3])/2, \n            color = \"black\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[3], xmax = dpc$t[4], \n            ymin = 0, ymax = (dpc$y[3]+ dpc$y[4])/2,\n            color = \"black\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[4], xmax = dpc$t[5], \n            ymin = 0, ymax = (dpc$y[4]+ dpc$y[5])/2, \n            color = \"black\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[5], xmax = dpc$t[6], \n            ymin = 0, ymax = (dpc$y[5]+ dpc$y[6])/2, \n            color = \"black\", alpha = 0.5)+\n   annotate(\"rect\", xmin = dpc$t[6], xmax = dpc$t[7], \n            ymin = 0, ymax = (dpc$y[6]+ dpc$y[7])/2, \n            color = \"black\", alpha = 0.5)\n\n\n\n\nIn R, we can obtain the AUDPC for the DPC we created earlier using the AUDPC function offered by the epifitter package. Because we are using the percent data, we need to set the argument y_proportion = FALSE. The function returns the absolute AUDPC. If one is interested in relative AUDPC, the argument type should be set to \"relative\". There is also the alternative to AUDPC, the area under the disease progress stairs (AUDPS) (Simko and Piepho 2012).\n\nlibrary(epifitter)\nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 30.4815\n\n# The relative AUDPC \nAUDPC(dpc$t, dpc$y, \n      y_proportion = FALSE, \n      type = \"relative\")\n\n[1] 0.0072575\n\n# To calculate AUDPS, the alternative to AUDPC\nAUDPS(dpc$t, dpc$y, \n      y_proportion = FALSE)\n\n[1] 34.258\n\n\nMathematical models can be fitted to the DPC data to express epidemic progress in terms of rates and absolute/relative quantities. The latter can be accomplished using population dynamics (or growth-curve) models for which the estimated parameters are usually meaningful biologically and appropriately describe epidemics that do not decrease in disease intensity. By fitting an appropriate model to the progress curve data, another set of parameters is available to the researcher when attempting to represent, understand or compare epidemics."
  },
  {
    "objectID": "temporal.html#population-dynamics-models",
    "href": "temporal.html#population-dynamics-models",
    "title": "1  Temporal analysis",
    "section": "1.3 Population dynamics models",
    "text": "1.3 Population dynamics models\nThe family of models that describe the growth of epidemics, hence population dynamics model, are known as deterministic models of continuous time (Madden et al. 2017). These models are usually fitted to DPC data to obtain two or more biologically meaningful parameters. Here, these models and their formulations are shown using R scripts to simulate the theoretical curves for each model.\n\n1.3.1 Non-flexible models\nThese population dynamics models require at least two parameters, hence they are known as non-flexible, as opposed to the flexible ones for which there are at least one additional (third) parameter.\nFollowing the convention proposed by (Madden et al. 2017) in their book “The study of plant disease epidemics”:\n\ntime is represented by \\(t\\)\ndisease intensity by \\(y\\)\nthe rate of change in \\(y\\) between two time units is represented by \\(\\frac{dy}{dt}\\)\n\nNow we can proceed and learn which non-flexible models exist and for which situation they are more appropriate.\n\n1.3.1.1 Exponential\nThe differential equation for the exponential model is given by\n\\(\\frac{dy}{dt} = r_E.y\\),\nwhere \\(r_E\\) is the apparent infection rate (subscript E for this model) (sensu Vanderplank) and \\(y\\) is the disease intensity. Biologically, this formulation suggests that diseased plants, or \\(y\\), and \\(r_E\\) at each time contribute to disease increase. The value of \\(\\frac{dy}{dt}\\) is minimal when \\(y = 0\\) and increases exponentially with the increase in \\(y\\).\nThe integral for the exponential model is given by\n\\(y = y_0 e^{r_Et}\\),\nwhere \\(y0\\) is and \\(r\\) are obtained via estimation. Let’s simulate two curves by varying \\(r\\) while fixing \\(y0\\) and varying the latter while fixing \\(r_E\\). We produce the two plots in ggplot and add the predicted curve using the `stat_function`. But first, we need to define values for the two model parameters. Further modifications to these values will be handled directly in the simulation (e.g. doubling infection rate, reducing initial inoculum by half, etc.).\n\ny0 <- 0.001 \nr <- 0.06 \ntmax <- 60 # maximum duration t of the epidemics\ndat <- data.frame(t = seq(1:tmax), y = seq(0:1)) # define the axes\n\nIn the plot below, note that the infection rate in one curve was doubled (\\(r\\) = 0.12)\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  labs(\n    title = \"Exponential model\",\n    subtitle = \"2 times r (dashed) same y0\",\n    x = \"Time\"\n  )\n\nWarning: Removed 5 row(s) containing missing values (geom_path).\n\n\n\n\n\nNow the inoculum was increased five times while using the same doubled rate.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) y0 * exp(r * 2 * t), linetype = 1) +\n  stat_function(fun = function(t) y0 * 5 * exp(r * 2 * t), linetype = 2) +\n  ylim(0, 1) +\n  labs(title = \"Exponential model\", x = \"Time\",\n       subtitle = \"5 times y0 (dashed) same r\")\n\nWarning: Removed 5 row(s) containing missing values (geom_path).\n\n\nWarning: Removed 27 row(s) containing missing values (geom_path).\n\n\n\n\n\n\n\n1.3.1.2 Monomolecular\nThe differential of the monomolecular model is given by\n\\(\\frac{dy}{dt} = r_M (1-y)\\)\nwhere now the \\(r_M\\) is the rate parameter of the monomolecular model and \\((1-y)\\) is the proportion of non-infected (healthy) individuals or host tissue. Note that \\(\\frac{dy}{dt}\\) is maximum when \\(y = 0\\) and decreases when \\(y\\) approaches 1. Its decline is due to decrease in the proportion of individuals or healthy sites with the increase in \\(y\\). Any inoculum capable of infecting the host will more likely land on infected individuals or sites.\nThe integral of the monomolecular model is given by\n\\(\\frac{dy}{dt} = 1 - (1-y)e^{-r_Mt}\\)\nThis model commonly describes the temporal patterns of the monocyclic epidemics. In those, the inoculum produced during the course of the epidemics do not contribute new infections. Therefore, different from the exponential model, disease intensity \\(y\\) does not affect the epidemics and so the absolute rate is proportional to \\((1-y)\\).\nLet’s simulate two monomolecular curve with different rate parameters where one is one third of the other.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r * t))) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-(r / 3) * t))) +\n  labs(title = \"Monomolecular model\",\n         subtitle = \"Fixed y0 = 0.001\", x = \"Time\"\n       ) +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"r = 0.06\") +\n  annotate(geom = \"text\", x = 50, y = 0.55, label = \"r = 0.02\")\n\n\n\n\nNow inoculum was increased 100 times with the reduced rate.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(fun = function(t) 1 - ((1 - y0) * exp(-r / 2 * t))) +\n  stat_function(fun = function(t) 1 - ((1 - (y0 * 100)) * exp(-r / 2 * t))) +\n  labs(title = \"Monomolecular model\", \n       subtitle = \"Fixed r = 0.06\", x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.01\") +\n  annotate(geom = \"text\", x = 45, y = 0.65, label = \"y0 = 0.001\")\n\n\n\n\n\n\n1.3.1.3 Logistic\nThe logistic model is a more elaborated version of the two previous models as it incorporates the features of them both. Its differential is given by\n\\(\\frac{dy}{dt} = r_L. y . (1 - y)\\),\nwhere \\(r_L\\) is the infection rate of the logistic model, \\(y\\) is the proportion of diseased individuals or host tissue and \\((1-y)\\) is the proportion of non-affected individuals or host area.\nBiologically, \\(y\\) in its differential equation implies that \\(\\frac{dy}{dt}\\) increases with the increase in \\(y\\) (as in the exponential) because more disease means more inoculum. However, \\((1-y)\\) leads to a decrease in \\(\\frac{dy}{dt}\\) when \\(y\\) approaches the maximum \\(y=1\\), because the proportion of healthy individuals or host area decreases (as in the monomolecular). Therefore, \\(\\frac{dy}{dt}\\) is minimal at the onset of the epidemics, reaches a maximum when \\(y/2\\) and declines until \\(y=1\\).\nThe integral is given by\n\\(y = \\frac{1}{1 + (1-y_0).e^{-r.t}}\\),\nwhere \\(r_L\\) is the apparent infection rate of the logistic model and \\(y0\\) is the disease intensity at \\(t=0\\). This model provides a good fit to polycyclic epidemics.\nLet’s check two curves where in one the infection rate is double while keeping the same initial inoculum.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 2 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 4 * t))) +\n  labs(title = \"Logistic model\", subtitle = \"Fixed y0 = 0.001\", x = \"Time\") +\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.18\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.024\")\n\n\n\n\nNow the inoculum is reduced 10 times for a same infection rate.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) 1 / (1 + ((1 - (y0 / 10)) / (y0 / 10)) * exp(-r * 3 * t))\n  ) +\n  stat_function(fun = function(t) 1 / (1 + ((1 - y0) / y0) * exp(-r * 3 * t))) +\n  labs(title = \"Logistic model\", subtitle = \"Fixed r = 0.24\", x = \"Time\") +\n  annotate(geom = \"text\", x = 35, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"y0 = 0.0001\")\n\n\n\n\n\n\n1.3.1.4 Gompertz\nThe Gompertz model is similar to the logistic and also provides a very good fit to several polycyclic diseases. The differential equation is given by\n\\(\\frac{dy}{dt} = r_G.[ln(1) - ln(y)]\\)\nDifferently from the logistic, the variable representing the non-infected individuals or host area is \\(-ln(y)\\). The integral equation is given by\n\\(y = e^{(ln(y0)).{e^{-r_G.t)}}}\\),\nwhere \\(r_G\\) is the apparent infection rate for the Gompertz models and \\(y_0\\) is the disease intensity at \\(t = 0\\).\nLet’s check curves for two rates.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r/2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0) * exp(-r*2 * t))) +\n  labs(title = \"Gompertz model\", subtitle = \"Fixed y0 = 0.001\", x = \"Time\") +\n  annotate(geom = \"text\", x = 41, y = 0.77, label = \"r = 0.12\") +\n  annotate(geom = \"text\", x = 50, y = 0.10, label = \"r = 0.03\")\n\n\n\n\nAnd those when inoculum was reduced thousand times.\n\ndat %>%\n  ggplot(aes(t, y)) +\n  stat_function(\n    linetype = 2,\n    fun = function(t) exp(log(y0) * exp(-r*2 * t))\n  ) +\n  stat_function(fun = function(t) exp(log(y0/1000) * exp(-r*2 * t))) +\n  labs(title = \"Gompertz model\", subtitle = \"Fixed r = 0.12\", x = \"Time\") +\n  annotate(geom = \"text\", x = 15, y = 0.77, label = \"y0 = 0.001\") +\n  annotate(geom = \"text\", x = 25, y = 0.10, label = \"y0 = 0.00001\")"
  },
  {
    "objectID": "temporal.html#fitting-models-to-data",
    "href": "temporal.html#fitting-models-to-data",
    "title": "1  Temporal analysis",
    "section": "1.4 Fitting models to data",
    "text": "1.4 Fitting models to data\nNext, you will learn how to fit models to multiple actual disease progress curves (DPCs) data obtained from the literature. I will demonstrate how to fit and select the models using the epifitter package. A few user friendly functions will help us decide which model to choose to obtain the parameters of interest and further compare the epidemics.\nTo illustrate, I will use two datasets available from Chapter 3 from the book, Study of Plant Disease Epidemics (Madden et al. 2017). In the book, SAS codes are presented to perform a few analysis. We then provide an alternative code for performing similar analysis, although not perfectly reproducing the results from the book.\n\n1.4.1 Non-replicated epidemics\nWe will compare three DPCs of the incidence of tobacco etch, a virus disease, in peppers. Evaluations of incidence were evaluated at a 7-day interval, up to 49 days.The data are available in chapter 4 (page 93). Let’s input the data manually and create a data frame. First column is the assessment time and the other columns correspond to the treatments, called groups in the book, from 1 to 3.\n\n\n1.4.2 Entering data\n\npepper <- \n  tribble(\n   ~t,  ~`1`,  ~`2`,  ~`3`,\n   0,  0.08, 0.001, 0.001,\n   7,  0.13,  0.01, 0.001,\n  14,  0.78,  0.09,  0.01,\n  21,  0.92,  0.25,  0.05,\n  28,  0.99,   0.8,  0.18,\n  35, 0.995,  0.98,  0.34,\n  42, 0.999,  0.99,  0.48,\n  49, 0.999, 0.999,  0.74\n  ) \n\n\n\n1.4.3 Visualize the DPCs\nBefore proceeding with model selection and fitting, let’s visualize the three epidemics. The code below reproduces quite exactly the top plot of Fig. 4.15 ((Madden et al. 2017) page 94). The appraisal of the curves might give us a hint on which models are the best candidates.\nBecause the data was entered in the wide format (each DPCs in a different columns) we need to reshape it to the long format. The pivot_longer function will do the job of reshaping from wide to long format so we can finally use the ggplot function to produce the plot.\n\npepper %>% \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\") %>% \n  ggplot (aes(t, inc, \n              linetype = treat, \n              shape = treat, \n              group = treat))+\n  geom_point(size =2)+\n  geom_line(size = 1)+\n  annotate(geom = \"text\", x = 15, y = 0.84, label = \"1\")+\n  annotate(geom = \"text\", x = 23, y = 0.6, label = \"2\")+\n  annotate(geom = \"text\", x = 32, y = 0.33, label = \"3\")+\n  labs(y = \"Disease incidence (y)\",\n       x = \"Time (days)\")+\n  theme(legend.position = \"none\")\n\n\n\n\nMost of the three curves show a sigmoid shape with the exception of group 3 that resembles an exponential growth, not reaching the maximum value, and thus suggesting an incomplete epidemic. We can easily eliminate the monomolecular and exponential models and decide on the other two non-flexible models: logistic or Gompertz. To do that, let’s proceed to model fitting and evaluate the statistics for supporting a final decision. There are two modeling approaches for model fitting in epifitter: the linear or nonlinear parameter-estimation methods.\n\n\n1.4.4 Fitting: single epidemics\nAmong the several options offered by epifitter we start with the simplest one, which is fit a model to a single epidemics using the linear regression approach. For such, the fit_lin() requires two arguments: time (time) and disease intensity (y) each one as a vector stored or not in a dataframe.\nSince we have three epidemics, fit_lin() will be use three times. The function produces a list object with six elements. Let’s first look at the Stats dataframe of each of the three lists named epi1 to epi3.\n\nlibrary(epifitter)\nepi1 <- fit_lin(time = pepper$t,  \n                y = pepper$`1` )\nepi1$Stats\n\n                 CCC r_squared    RSE\nGompertz      0.9848    0.9700 0.5911\nMonomolecular 0.9838    0.9681 0.5432\nLogistic      0.9782    0.9572 0.8236\nExponential   0.7839    0.6447 0.6705\n\n\n\nepi2 <- fit_lin(time = pepper$t,  \n  y = pepper$`2` )\nepi2$Stats\n\n                 CCC r_squared    RSE\nLogistic      0.9962    0.9924 0.4524\nGompertz      0.9707    0.9431 0.8408\nMonomolecular 0.9248    0.8601 1.0684\nExponential   0.8971    0.8134 1.2016\n\n\n\nepi3 <- fit_lin(time = pepper$t,  \n  y = pepper$`3` )\nepi3$Stats\n\n                 CCC r_squared    RSE\nLogistic      0.9829    0.9665 0.6045\nGompertz      0.9825    0.9656 0.2263\nExponential   0.9636    0.9297 0.7706\nMonomolecular 0.8592    0.7531 0.2534\n\n\nThe statistics of the model fit confirms our initial guess that the predictions by the logistic or the Gompertz are closer to the observations than predictions by the other models. There is no much difference between them based on these statistics. However, to pick one of the models, it is important to inspect the curves with the observed and predicted values to check which model is best for all curves.\n\n\n1.4.5 Fitting: multiple epidemics\nBefore looking at the prediction, let’s use another handy function that allows us to simultaneously fit the models to multiple DPC data. Different from fit_lin(), fit_multi() requires the data to be structured in the long format where there is a column specifying each of the epidemics.\nLet’s then create a new data set called pepper2 using the data transposing functions of the tidyr package.\n\npepper2 <- pepper %>% \n  pivot_longer(2:4, names_to =\"treat\", values_to = \"inc\")\n\nNow we fit the models to all DPCs. Note that the name of the variable indicating the DPC code needs to be informed in strata_cols argument.\n\nepi_all <- fit_multi(\n  time_col = \"t\",\n  intensity_col = \"inc\",\n  data = pepper2,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nNow let’s select the statistics of model fitting. Again, Epifitter ranks the models based on the CCC (the higher the better) but it is important to check the RSE as well - the lower the better. In fact, the RSE is more important when the goal is prediction.\n\nepi_all$Parameters %>% \n  select(treat, model, best_model, RSE, CCC)\n\n   treat         model best_model       RSE       CCC\n1      1      Gompertz          1 0.5911056 0.9847857\n2      1 Monomolecular          2 0.5431977 0.9838044\n3      1      Logistic          3 0.8235798 0.9781534\n4      1   Exponential          4 0.6705085 0.7839381\n5      2      Logistic          1 0.4523616 0.9961683\n6      2      Gompertz          2 0.8407922 0.9707204\n7      2 Monomolecular          3 1.0683633 0.9247793\n8      2   Exponential          4 1.2015809 0.8971003\n9      3      Logistic          1 0.6045243 0.9829434\n10     3      Gompertz          2 0.2262550 0.9824935\n11     3   Exponential          3 0.7705736 0.9635747\n12     3 Monomolecular          4 0.2533763 0.8591837\n\n\nTo be more certain about our decision, let’s advance to the final step which is to produce the plots with the observed and predicted values for each assessment time by calling the Data dataframe of the `epi_all list.\n\nepi_all$Data %>%\n filter(model %in% c(\"Gompertz\", \"Logistic\")) %>% \n  ggplot(aes(time, predicted, shape = treat)) +\n  geom_point(aes(time, y)) +\n  geom_line() +\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(0, 1)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nOverall, the logistic model seems a better fit for all the curves. Let’s produce a plot with the prediction error versus time.\n\nepi_all$Data %>%\n filter(model %in% c(\"Gompertz\", \"Logistic\")) %>% \n  ggplot(aes(time, predicted -y, shape = treat)) +\n  geom_point() +\n  geom_line() +\n  geom_hline(yintercept = 0, linetype =2)+\n  facet_wrap(~ model) +\n coord_cartesian(ylim = c(-0.4, 0.4)) + # set the max to 0.6\n  labs(\n    y = \"Prediction error\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nThe plots above confirms the logistic model as good fit overall because the errors for all epidemics combined are more scattered around the non-error line.\n\n  epi_all$Parameters %>%\n    filter(model == \"Logistic\") %>%\n    select(treat, y0, y0_ci_lwr, y0_ci_upr, r, r_ci_lwr, r_ci_upr \n)\n\n  treat           y0    y0_ci_lwr   y0_ci_upr         r  r_ci_lwr  r_ci_upr\n1     1 0.0935037690 0.0273207272 0.274728744 0.2104047 0.1659824 0.2548270\n2     2 0.0013727579 0.0006723537 0.002800742 0.2784814 0.2540818 0.3028809\n3     3 0.0008132926 0.0003131745 0.002110379 0.1752146 0.1426077 0.2078215\n\n\nWe can produce a plot for visual inference on the differences in the parameters.\n\np1 <- epi_all$Parameters %>%\n  filter(model == \"Logistic\") %>%\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np2 <- epi_all$Parameters %>%\n  filter(model == \"Logistic\") %>%\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:cowplot':\n\n    align_plots\n\np1 | p2\n\n\n\n\n\n\n1.4.6 Designed experiments\nIn this next section, we will work with disease data collected over time in the same plot unit (also called repeated measures) from a designed experiment for evaluating and comparing treatment effects.\nAgain, we will use a dataset of progress curves shown in page 98 (Madden et al. 2017). The curves represent the incidence of soybean plants symptomatic for bud blight caused by tobacco streak virus. Four treatments (different planting dates) were evaluated in randomized complete block design with four replicates. There are four assessment in time for each curve. The data was stored as a csv file and will be loaded using read_csv() function and stored as dataframe called budblight.\n\n1.4.6.1 Loading data\n\nbudblight <- read_csv(\"data/bud-blight-soybean.csv\")\n\n\n── Column specification ────────────────────────────────────────────────────────\ncols(\n  treat = col_character(),\n  time = col_double(),\n  block = col_double(),\n  y = col_double()\n)\n\n\nLet’s have a look at the first six rows of the dataset and check the data type for each column. There is an additional column representing the replicates, called block.\n\nhead(budblight)\n\n# A tibble: 6 × 4\n  treat  time block     y\n  <chr> <dbl> <dbl> <dbl>\n1 PD1      30     1  0.1 \n2 PD1      30     2  0.3 \n3 PD1      30     3  0.1 \n4 PD1      30     4  0.1 \n5 PD1      40     1  0.3 \n6 PD1      40     2  0.38\n\n\n####Visualizing the DPCs\nLet’s have a look at the curves and produce a combo plot figure similar to Fig. 4.17 of the book, but without the line of the predicted values.\n\np3 <- budblight %>%\n  ggplot(aes(\n    time, y,\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 1.5) +\n  ylim(0, 0.6) +\n  theme(legend.position = \"none\")+\n  facet_wrap(~treat, ncol =1)+\n  labs(y = \"Disease incidence\",\n       x = \"Time (days after emergence)\")\n\n\np4 <- budblight %>%\n  ggplot(aes(\n    time, log(1 / (1 - y)),\n    group = block,\n    shape = factor(block)\n  )) +\n  geom_point(size = 2) +\n  facet_wrap(~treat, ncol = 1) +\n  theme(legend.position = \"none\")+\n  labs(y = \"Transformed incidence\", x = \"Time (days after emergence)\")\n\np3 | p4\n\n\n\n\n\n\n1.4.6.2 Model fitting\nRemember that the first step in model selection is the visual appraisal of the curve data linearized with the model transformation. In the case the curves represent complete epidemics (close to 100%) appraisal of the absolute rate (difference in y between two times) over time is also helpful.\nFor the treatments above, it looks like the curves are typical of a monocyclic disease (the case of soybean bud blight), for which the monomolecular is usually a good fit, but other models are also possible as well. For this exercise, we will use both the linear and the nonlinear estimation method.\n\n1.4.6.2.1 Linear regression\nFor convenience, we use the fit_multi() to handle multiple epidemics. The function returns a list object where a series of statistics are provided to aid in model selection and parameter estimation. We need to provide the names of columns (arguments): assessment time (time_col), disease incidence (intensity_col), and treatment (strata_cols).\n\nlin1 <- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = FALSE\n)\n\nLet’s look at how well the four models fitted the data. Epifitter suggests the best fitted model (1 to 4, where 1 is best) for each treatment. Let’s have a look at the statistics of model fitting.\n\n  lin1$Parameters %>% \n    select(treat, best_model, model, CCC, RSE)\n\n   treat best_model         model       CCC        RSE\n1    PD1          1 Monomolecular 0.9348429 0.09805661\n2    PD1          2      Gompertz 0.9040182 0.22226189\n3    PD1          3      Logistic 0.8711178 0.44751963\n4    PD1          4   Exponential 0.8278055 0.36124036\n5    PD2          1 Monomolecular 0.9547434 0.07003116\n6    PD2          2      Gompertz 0.9307192 0.17938711\n7    PD2          3      Logistic 0.9062012 0.38773023\n8    PD2          4   Exponential 0.8796705 0.32676216\n9    PD3          1 Monomolecular 0.9393356 0.06832499\n10   PD3          2      Gompertz 0.9288436 0.17156394\n11   PD3          3      Logistic 0.9085414 0.39051075\n12   PD3          4   Exponential 0.8896173 0.33884790\n13   PD4          1      Gompertz 0.9234736 0.17474422\n14   PD4          2 Monomolecular 0.8945962 0.06486949\n15   PD4          3      Logistic 0.8911344 0.52412586\n16   PD4          4   Exponential 0.8739618 0.49769642\n\n\nAnd now we extract values for each parameter estimated from the fit of the monomolecular model.\n\n  lin1$Parameters %>%\n  filter(model == \"Monomolecular\") %>%\n  select(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.5727700 0.02197351\n2   PD2 -0.5220593 0.01902952\n3   PD3 -0.4491365 0.01590586\n4   PD4 -0.3619898 0.01118047\n\n\nNow we visualize the fit of the monomolecular model (using filter function - see below) to the data together with the observed data and then reproduce the right plots in Fig. 4.17 from the book.\n\nlin1$Data %>%\n  filter(model == \"Monomolecular\") %>%\n  ggplot(aes(time, predicted)) +\n  geom_point(aes(time, y)) +\n  geom_line(size = 0.5) +\n  facet_wrap(~treat) +\n  coord_cartesian(ylim = c(0, 0.6)) + # set the max to 0.6\n  labs(\n    y = \"Disease incidence\",\n    x = \"Time (days after emergence)\"\n  )\n\n\n\n\nNow we can plot the means and respective 95% confidence interval of the apparent infection rate (\\(r\\)) and initial inoculum (\\(y_0\\)) for visual inference.\n\np5 <- lin1$Parameters %>%\n  filter(model == \"Monomolecular\") %>%\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np6 <- lin1$Parameters %>%\n  filter(model == \"Monomolecular\") %>%\n  ggplot(aes(treat, 1 - exp(-y0))) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\np5 | p2\n\n\n\n\n\n\n1.4.6.2.2 Non-linear regression\nTo estimate the parameters using the non-linear approach, we repeat the same arguments in the fit_multi function, but include an additional argument nlin set to TRUE.\n\nnlin1 <- fit_multi(\n  time_col = \"time\",\n  intensity_col = \"y\",\n  data = budblight,\n  strata_cols = \"treat\",\n  nlin = TRUE\n)\n\nWarning in log(y0/1): NaNs produced\n\nWarning in log(y0/1): NaNs produced\n\nWarning in log(y0/1): NaNs produced\n\n\nLet’s check statistics of model fit.\n\nnlin1$Parameters %>%\n  select(treat, model, CCC, RSE, best_model)\n\n   treat         model       CCC        RSE best_model\n1    PD1 Monomolecular 0.9382991 0.06133704          1\n2    PD1      Gompertz 0.9172407 0.06986307          2\n3    PD1      Logistic 0.8957351 0.07700720          3\n4    PD1   Exponential 0.8544194 0.08799512          4\n5    PD2 Monomolecular 0.9667886 0.04209339          1\n6    PD2      Gompertz 0.9348370 0.05726761          2\n7    PD2      Logistic 0.9077857 0.06657793          3\n8    PD2   Exponential 0.8702365 0.07667322          4\n9    PD3 Monomolecular 0.9570853 0.04269129          1\n10   PD3      Gompertz 0.9261609 0.05443852          2\n11   PD3      Logistic 0.8997106 0.06203037          3\n12   PD3   Exponential 0.8703443 0.06891021          4\n13   PD4 Monomolecular 0.9178226 0.04595409          1\n14   PD4      Gompertz 0.9085579 0.04791331          2\n15   PD4      Logistic 0.8940731 0.05083336          3\n16   PD4   Exponential 0.8842437 0.05267415          4\n\n\nAnd now we obtain the two parameters of interest. Note that the values are not the sames as those estimated using linear regression, but they are similar and highly correlated.\n\n  nlin1$Parameters %>%\n    filter(model == \"Monomolecular\") %>%\n    select(treat, y0, r)\n\n  treat         y0          r\n1   PD1 -0.7072562 0.02381573\n2   PD2 -0.6335713 0.02064629\n3   PD3 -0.5048763 0.01674209\n4   PD4 -0.3501234 0.01094368\n\n\n\np7 <- nlin1$Parameters %>%\n  filter(model == \"Monomolecular\") %>%\n  ggplot(aes(treat, r)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = r_ci_lwr, ymax = r_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"r\"\n  )\n\np8 <- nlin1$Parameters %>%\n  filter(model == \"Monomolecular\") %>%\n  ggplot(aes(treat, y0)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = y0_ci_lwr, ymax = y0_ci_upr),\n    width = 0,\n    size = 1\n  ) +\n  labs(\n    x = \"Time\",\n    y = \"y0\"\n  )\n\np7 | p8\n\n\n\n\n\n\n\n\n\nJeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area under the disease-progress curve (AUDPC) to assess quantitative disease resistance in crop cultivars. Theoretical and Applied Genetics. 102:32–40 Available at: http://dx.doi.org/10.1007/s001220051615.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F., eds. 2017. CHAPTER 4: Temporal analysis i: Quantifying and comparing epidemics. In The American Phytopathological Society, p. 63–116. Available at: http://dx.doi.org/10.1094/9780890545058.004.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress Stairs: Calculation, Advantage, and Application. Phytopathology®. 102:381–389 Available at: http://dx.doi.org/10.1094/phyto-07-11-0216."
  },
  {
    "objectID": "spatial-gradients.html",
    "href": "spatial-gradients.html",
    "title": "2  Spatial gradients",
    "section": "",
    "text": "The assessment of disease in space, in terms of changes in the intensity as it spreads over distance, is called disease gradient. In reality, it is the dispersal (migration) of the pathogen by various means (e.g. wind, vectors, rain, movement of infected material or human mediation) that promotes the spread of plant diseases within a field or across continents and generates the disease gradients. There are two kinds of gradients, the inoculum gradient where host availability is not necessarily required and the disease gradient where the three elements of the disease triangle are required.\nIn disease gradients, assuming that there is only a single source of inoculum, the intensity of the disease decreases more steeply within short distances of the source, and less steeply at greater distances until they reach zero or a low background level of occasional diseased plants. The shapes of the gradients are defined by mechanisms associated with the dispersal of the inoculum which depends on the biology of the pathogen but strongly to environmental factors that affect pathogen dispersal.\nWhen studying disease gradients, researchers need to make sure that there is a well defined single source of inoculum. In gradients, this is called a focus, from where the inoculum originates. The resulting gradients can be classified in two types: primary or secondary. The primary gradient originates only from the initial focus, while the secondary gradient originates from the movement of inoculum produced at previously infected (due to primary gradients) plants to other plants at increasing distances from the source. It is expected that a mix of both kinds of gradients exists as the disease increases over time.\nSimilar to disease progress curves, models can be fitted empirically to observed disease gradient curves and provide insights into the mechanisms of inoculum dispersal and deposition, the source of inoculum, and the physical processes underlying dispersal."
  },
  {
    "objectID": "spatial-gradients.html#linearization-of-the-models",
    "href": "spatial-gradients.html#linearization-of-the-models",
    "title": "2  Spatial gradients",
    "section": "2.3 Linearization of the models",
    "text": "2.3 Linearization of the models\n\n2.3.1 Transformations of y\nThe gradient models, again similar to the temporal disease models, are non linear in their parameters. The model is intrinsically linear if transformations are applied (according to the model) in both sides of the equations. The linear model in its generic state is given by\n\\(y* = a* + bx\\) ,\nwhere the asterisk in \\(a\\) indicated that one of the transformations was applied in \\(y\\) that produced the linear model. Note that \\(a*\\) is the transformed version of the initial disease intensity, which needs to be returned to the original scale according to the respective back-transformation. Follows the linearized form of the two most common gradient models.\n\\(ln(y) = ln(a_{E}) - b_{E}. x\\)\n\\(ln(y) = ln(a_{P}) - b_{E}. ln(x+C)\\)\n\n\n2.3.2 Plot for the linearized form of models\nLet’s visualize the linearization of the exponential model with two different slopes (gradient 1 and 2). Note that the transformation used was \\(ln(y)\\).\nFollows the linearization of the modified power law model.\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 %>%\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) log(a1) - (b1 * x), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * x), linetype = 2) +\n  labs(\n    title = \"Exponential\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )\n\n\n\n\nFollows the linearization of the modified power law model. Note that the transformation used was \\(ln(y)\\) and \\(ln(x+C)\\) .\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- log(80) # maximum distance for gradient 1\nmax2 <- log(80) # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\ndat2 %>%\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) log(a1) - (b1 * log(x + C)), linetype = 1) +\n  stat_function(fun = function(x) log(a2) - (b2 * log(x + C)), linetype = 2) +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"log of distance (m)\",\n    y = \"log of disease incidence\"\n  )"
  },
  {
    "objectID": "spatial-gradients.html#model-fitting",
    "href": "spatial-gradients.html#model-fitting",
    "title": "2  Spatial gradients",
    "section": "2.4 Model fitting",
    "text": "2.4 Model fitting\n\n2.4.1 Dataset\nThe hypothetical data below shows a gradient for the number of lesions counted at varying distances in meters from the source. Let’s create two vectors, one for the distances \\(x\\) and the other for the lesion count \\(Y\\), and then a dataframe by combining the two vectors.\n\n# create the two vectors\nx <- c(0.8, 1.6, 2.4, 3.2, 4, 7.2, 12, 15.2, 21.6, 28.8)\nY <- c(184.9, 113.3, 113.3, 64.1, 25, 8, 4.3, 2.5, 1, 0.8)\ngrad1 <- data.frame(x, Y) # create the dataframe\ngrad1 # show the gradient\n\n      x     Y\n1   0.8 184.9\n2   1.6 113.3\n3   2.4 113.3\n4   3.2  64.1\n5   4.0  25.0\n6   7.2   8.0\n7  12.0   4.3\n8  15.2   2.5\n9  21.6   1.0\n10 28.8   0.8\n\n\n\n\n2.4.2 Visualize the gradient\n\ngrad1 %>% \n  ggplot(aes(x, Y))+\n  geom_point()+\n  geom_line()+\n  labs(y = \"Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\n\n\n2.4.3 Linear regression\nA linear regression model is fitted to the transformed variables according to the model. The higher the coefficient of determination, the better is the fit of the model to the data.\nExponential model\n\nreg_exp <- lm(log(Y) ~ x, data = grad1)\nsummary(reg_exp)\n\n\nCall:\nlm(formula = log(Y) ~ x, data = grad1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.04868 -0.58973 -0.00144  0.59572  0.99554 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.57705    0.35222  12.995 1.17e-06 ***\nx           -0.20124    0.02656  -7.576 6.45e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7612 on 8 degrees of freedom\nMultiple R-squared:  0.8777,    Adjusted R-squared:  0.8624 \nF-statistic: 57.39 on 1 and 8 DF,  p-value: 6.45e-05\n\n\nPower law model with \\(C = 0\\).\n\nreg_p <- lm(log(Y) ~ log(x), data = grad1)\nsummary(reg_p)\n\n\nCall:\nlm(formula = log(Y) ~ log(x), data = grad1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.72281 -0.11989 -0.03146  0.08755  0.65267 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   5.5638     0.2456   22.66 1.53e-08 ***\nlog(x)       -1.6978     0.1191  -14.26 5.71e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4235 on 8 degrees of freedom\nMultiple R-squared:  0.9621,    Adjusted R-squared:  0.9574 \nF-statistic: 203.3 on 1 and 8 DF,  p-value: 5.71e-07\n\n\nPower law model with \\(C = 0.4\\).\n\nreg_pm <- lm(log(Y) ~ log(x + 0.4), data = grad1)\nsummary(reg_pm)\n\n\nCall:\nlm(formula = log(Y) ~ log(x + 0.4), data = grad1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53733 -0.17258 -0.03646  0.08450  0.56928 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    6.1007     0.2283   26.73 4.13e-09 ***\nlog(x + 0.4)  -1.8841     0.1084  -17.38 1.22e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3495 on 8 degrees of freedom\nMultiple R-squared:  0.9742,    Adjusted R-squared:  0.971 \nF-statistic: 302.2 on 1 and 8 DF,  p-value: 1.223e-07\n\n\nGraphs for the fitted models\nExponential\n\ngrad1 %>% \n  ggplot(aes(x, log(Y)))+\n  geom_point()+\n  geom_line()+\n  geom_abline(slope = coef(reg_exp)[[2]], intercept = coef(reg_exp)[[1]])+\n labs(y = \"Log of Lesion count\",\n       x = \"Distance (m)\")\n\n\n\n\nPower law model\n\ngrad1 %>% \n  ggplot(aes(log(x), log(Y)))+\n  geom_point()+\n  geom_line()+\n  geom_abline(slope = coef(reg_p)[[2]], intercept = coef(reg_p)[[1]])+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance\")\n\n\n\n\nModified power law model\n\ngrad1 %>% \n  ggplot(aes(log(x+0.4), log(Y)))+\n  geom_point()+\n  geom_line()+\n  geom_abline(slope = coef(reg_pm)[[2]], intercept = coef(reg_pm)[[1]])+\n labs(y = \"Log of Lesion count\",\n       x = \"Log of distance (m)\")\n\n\n\n\nConclusion: The modified power law model provided the best fit."
  },
  {
    "objectID": "spatial-patterns.html",
    "href": "spatial-patterns.html",
    "title": "3  Spatial patterns",
    "section": "",
    "text": "A spatial disease pattern can be defined as the arrangement of diseased entities relative to each other and to the architecture of the host crop (Madden et al. 2017) . Such arrangement is the realization of the underlying dispersal of the pathogen, from one or several sources within and/or outside the area of interest, under the influence of physical, biological and environmental factors.\nThe study of spatial patterns is conducted at a specific time or multiple times during the epidemic. When assessed multiple times, both spatial and temporal processes can be characterized. Because epidemics change over time, it is expected that spatial patterns are not constant but change over time as well. Usually, plant pathologists are interested in determining spatial patterns at one or various spatial scales, depending on the objective of the study. The scale of interest may be a leaf or root, plant, field, municipality, state, country or even intercontinental area. The diseased units observed may vary from lesions on a single leaf to diseased fields in a large production region.\nThe patterns can be classified into two main types that occur naturally: random or aggregated. The random pattern originates because the chances for the units (leaf, plant, crop) to be infected are equal and low, and are largely independent from each other. In aggregated spatial patterns, such chances are unequal and there is dependency among the units, for example, a healthy unit close to a diseased unit is at higher risk than more distant units.\nA range of techniques, most based on statistical tests, can be used to detect deviations from randomness in space and the choice of the methods depends on the scale of observation. Usually, more than one test is applied for the same or different scales of interest depending on how the data are collected. The several statistical tests can be classified based on the spatial scale and type of data (binary, count, etc) collected, but mainly if the spatial location of the unit is known (mapped) or not known (sampled). Following Madden et al. (2007), two major groups can be formed. The sparsely sampled (incidence or count data) data or intensively mapped (binary or grouped data) data."
  },
  {
    "objectID": "spatial-patterns.html#intensively-mapped",
    "href": "spatial-patterns.html#intensively-mapped",
    "title": "3  Spatial patterns",
    "section": "3.2 Intensively mapped",
    "text": "3.2 Intensively mapped\n\n3.2.1 Binary data\nIn this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted is a binay variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.\n\n3.2.1.1 Runs test\nA run is defined as a succession of one or more diseased or healthy plants, which are followed and preceded by a plant of the other disease status or no plant at all. There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.\nLet’s create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 20 plants and assign it to y. For plotting purposes, we make a dataframe for more complete information.\n\ny1 <- c(1,1,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,1)\nx1 <- c(1:20) # position of each plant\nz1 <- 1\nrow1 <- data.frame(x1, y1, z1) # create a dataframe\n\nWe can then visualize the series using ggplot and count the number of runs as 7, aided by the color used to identify a run.\n\nlibrary(tidyverse)\nrow1 %>% \n  ggplot(aes(x1, z1, label = x1, color = factor(y1)))+\n  geom_point(shape =15, size =6)+\n  theme_void()+\n  scale_x_continuous(breaks = max(z1))+\n  scale_color_manual(values = c(\"grey80\", \"grey20\"))+\n  geom_text(vjust = 0, nudge_y = 0.5)+\ncoord_fixed()+\n  ylim(-0.5,2.5)+\n  theme(legend.position = \"right\")+\n  labs(color = \"Status\", title = \"Sequence of diseased (1) or non-diseased (0) units (plants)\", \n       subtitle = \"The numbers represent the position of the unit\")\n\n\n\n\nWe can write a code in R and create a function named oruns.test for the ordinary runs test.\n\noruns.test <- function(x) {\n# identify the sequence\nS <- x \n# Compute the number or runs\nU = max(cumsum(c(1, diff(S)!=0)))\n# Compute the number of diseased plants\nm = sum(S)\n# Count the total number of plants\nN = length(S)\n# Calculate the number of expected runs\nEU = 1 + (2 * m*(N - m)/N)\n# Calculate the standard deviation in the sample\nsU = sqrt(2 * m * (N - m) * (2 * m *(N-m)-N)/ (N^2 *(N-1)))\n# Calculate the z-value\nZ = (U - EU)/sU\n# Obtain the p-value for the Z\npvalue <- (2*pnorm(abs(Z), lower.tail=FALSE))\n# test if Z is lower than 1.64\nresult <- ifelse(Z < 1.64, \nc(\"clustering\"), \nc(\"randomness\"))\n# Print the results\nprint(paste(\"There are\",U,\"runs. The number of expected runs is\", round(EU,1), \"P-value:\",round(pvalue,6), \". Alternative hypothesis: non-randomness\"))\n}\n\nWe can now run the test for the example series above.\n\noruns.test(row1$y1)\n\n[1] \"There are 7 runs. The number of expected runs is 10.6 P-value: 0.084166 . Alternative hypothesis: non-randomness\"\n\n\nThere are built-in functions in R packages that allow for running the ordinary runs test. Let’s load the packages and runt the test. Note that the results of the runs.test is the same as the one produced by our custom function.\n\nlibrary(randtests)\nruns.test(row1$y1, threshold = 0.5)\n\n\n    Runs Test\n\ndata:  row1$y1\nstatistic = -1.727, runs = 7, n1 = 8, n2 = 12, n = 20, p-value =\n0.08417\nalternative hypothesis: nonrandomness\n\nlibrary(DescTools)\nr <- RunsTest(row1$y1)\n\n\n\n3.2.1.2 Doublets\nDoublet analysis is used to compare the observed number or adjacent diseased plants, a doublet (DD or 11), to the number expected if the disease were randomly distributed in the yard. If the observed number is greater than the expected number, contagion within the field is suspected.\nLet’s manually produce a code to execute the doublets test. To facilitate, we can create a function and name it doublets.test. The only argument needed is the vector of binary data.\n\ndoublets.test <- function(x) {\n# Identify the sequence\nS <- x\n# Compute the number of doublets Db\nmatrix <- cbind(S[-length(S)], S[-1])\npairs <- table(data.frame(matrix))\nDb <- pairs[2,2]\n# Count the number of diseased plants\nN <- length(S) \n# Count the number of total plants\nm = sum(S) \n# Expected number of doublets\nEDb = m *((m -1)/N)\n# Standard deviation \nSDb = sqrt ( EDb * (1 - (2 / N)))\n# Calculate the Z-value \nZDb = (Db - EDb)/ SDb \n# two-sided P-value calculation\npvalue <- (2*pnorm(abs(ZDb), lower.tail = FALSE))\n# Result of the test\nresult <- ifelse(abs(ZDb) >= 1.64, \nc(\"aggregation or clustering\"), \nc(\"randomness\")) \n# Print the results\nprint(paste(\"There are\",Db,\"doublets. The number of expected doublets is\",EDb,\".\",\"P-value:\", round(pvalue,4), \". Alternative hypothesis: non-randomness\"))\n}\n\n\n# Run the function calling the vector\ndoublets.test(row1$y1)\n\n[1] \"There are 4 doublets. The number of expected doublets is 2.8 . P-value: 0.4497 . Alternative hypothesis: non-randomness\"\n\n\n\n\n3.2.1.3 Join count\nIn this analysis, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The orientation(s) of interest (along rows, across rows, diagonally, or a a combination o these) should be specified. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation.\nIn R, we can use the join.count function of the spdep package to perform a joint count test. First we need to create the series of binary data from top to bottom and left to right. The data are shown in Fig. 9.13 in page 260 of the book chapter on spatial analysis (Madden et al. 2017). In the example, there are 5 rows and 5 columns. This will be informed later to run the test.\n\n# Enter the data\nS2 <- c(1,0,1,1,0,\n       1,1,0,0,0,\n       1,0,1,0,0,\n       1,0,0,1,0,\n       0,1,0,1,1)\n\nVisualize the two-dimensional array:\n\n# Convert to raster \nmapS2 <- raster::raster(matrix(S2, 5 ,5))\n\n# Convert to data frame\nmapS3 <- raster::as.data.frame(mapS2,xy=TRUE)\n\n# Map using ggplot\nmapS3 %>% \n  ggplot(aes(x, y, label = layer, fill = factor(layer)))+\n  geom_tile(color = \"black\", size =1)+\n  theme_void()+\n  geom_text(size = 10)+\n  labs(fill = \"Status\")+\n  scale_fill_manual(values = c(\"white\", \"grey80\"))\n\n\n\n\nLoad the library\n\nlibrary(spdep)\n\nFirst, we need to generate a list of neighbors (nb) for a grid of cells. This is performed with the cell2nb function by informing the number of rows and columns. The argument “rook” means shared edge, but it could be the “queen”, for shared edge or vertex. We can use the default.\n\nnb <- cell2nb(nrow = 5, \n              ncol = 5, \n              type=\"rook\")\n\nThe joincount.test function runs the BB join count test for spatial autocorrelation. From the function description, the method uses a spatial weights matrix in weights list form for testing whether same-status joins occur more frequently than would be expected if the zones were labelled in a spatially random way. We need to inform the sequence as factor and the nb object we created previously.\n\njoincount.test(factor(S2), \n                nb2listw(nb))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = -0.58266, p-value = 0.7199\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.9583333             3.2500000             0.2505797 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = -0.66841, p-value = 0.7481\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.4166667             2.7500000             0.2486957 \n\n\nThe function returns a list with a class for each of the status (in this case 0 and 1) with several components. We should look at the P-value. The alternative hypothesis (greater) is that the same status joins occur more frequently than expected if they were labelled in a spatial random way. In this case, we do not reject the null hypothesis of randomness.\nWe can run the ordinary runs and doublets tests, which only considers the adjacent neighbor, for the same series and compare the results.\n\noruns.test(S2)\n\n[1] \"There are 17 runs. The number of expected runs is 13.5 P-value: 0.149673 . Alternative hypothesis: non-randomness\"\n\ndoublets.test(S2)\n\n[1] \"There are 3 doublets. The number of expected doublets is 5.28 . P-value: 0.3009 . Alternative hypothesis: non-randomness\"\n\n\nLet’s repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence to reject the null hypothesis, indicating aggregation of plants.\n\nS3 <- c(1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       0,0,0,0,0)\n\njoincount.test(factor(S3), \n                nb2listw(nb))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = 4.2451, p-value = 1.093e-05\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.3750000             3.2500000             0.2505797 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = 4.5953, p-value = 2.16e-06\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.0416667             2.7500000             0.2486957 \n\noruns.test(S3)\n\n[1] \"There are 8 runs. The number of expected runs is 13.5 P-value: 0.024904 . Alternative hypothesis: non-randomness\"\n\n\nWe can apply these tests for a real example epidemic data provided by the epiphy R package. Let’s work with part of the intensively mapped data on the incidence of tomato spotted wilt virus (TSWV) disease in field trials reported by Cochran (1936) and Bald (1937). First, we need to load the library and then assign one dataframe (the dataset has two dataframes) of the dataset tomato_tswv to a new dataframe called tswv_1929.\n\nlibrary(epiphy) \nlibrary(cowplot) # theming the ggplot\ntswv_1929 <- tomato_tswv$field_1929\nhead(tswv_1929) \n\n  x y t i n\n1 1 1 1 0 1\n2 1 2 1 1 1\n3 1 3 1 0 1\n4 1 4 1 1 1\n5 1 5 1 0 1\n6 1 6 1 0 1\n\n\nThe inspection of the first rows of the dataframe shows five variables where x and y are spatial grid coordinates, t is assessment time, i is the status of the plant (0 = healthy, 1 = diseased) and n is the sampling unit size (here all one). Let’s visualize these data for each sampling time.\n\ntswv_1929 %>% \n  ggplot(aes(x, y, fill = factor(i)))+\n  geom_tile()+\n  coord_fixed()+\n  theme_minimal_grid()+\n  scale_fill_grey(start = 0.8, end = 0.2)+\n  facet_wrap(~ t)+\n  labs(fill = \"Status\")\n\n\n\n\nCheck the number of rows (y) and columns (x) for further preparing the neighbor object for the join count statistics.\n\ntswv_1929 %>% \n  select(x, y) %>% \n  summary()\n\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 6.75   1st Qu.:15.75  \n Median :12.50   Median :30.50  \n Mean   :12.50   Mean   :30.50  \n 3rd Qu.:18.25   3rd Qu.:45.25  \n Max.   :24.00   Max.   :60.00  \n\n\nThere are 60 rows and 24 columns.\n\n# Neighbor grid\nnb1 <- cell2nb(nrow = 60, \n              ncol = 24, \n              type=\"rook\")\n\n# Pull the binary sequence of time 1\nS1 <- tswv_1929 %>% \n  filter(t == \"1\") %>% \n  pull(i)\n\njoincount.test(factor(S1), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = -0.28351, p-value = 0.6116\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           482.000000            482.578874              4.169132 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = -0.059497, p-value = 0.5237\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            23.458333             23.578874              4.104614 \n\n\nWe can apply the join count test for time 2 and time 3. Results show that the pattern changes from random to aggregate over time.\n\n# Pull the binary sequence of time 1\nS2 <- tswv_1929 %>% \n  filter(t == \"2\") %>% \n  pull(i)\n\njoincount.test(factor(S2), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 0.35872, p-value = 0.3599\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           317.000000            315.900625              9.392312 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 0.34604, p-value = 0.3647\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            82.958333             81.900625              9.342754 \n\n# Pull the binary sequence of time 1\nS3 <- tswv_1929 %>% \n  filter(t == \"3\") %>% \n  pull(i)\n\njoincount.test(factor(S3), \n                nb2listw(nb1))\n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 1.8541, p-value = 0.03186\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            136.12500             129.92773              11.17243 \n\n\n    Join count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 1.7275, p-value = 0.04204\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            243.70833             237.92773              11.19743 \n\n\n\n\n\n3.2.2 Grouped data\nIf the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are autocorrelation (known as Moran’s I), semivariance and SADIE (an alternative approach to autocorrelation.)\n\n3.2.2.1 Autocorrelation\nSpatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity (Madden et al. 2017).\nWe will illustrate the method by reproducing the example provided in page 264 of the chapter on spatial analysis (Madden et al. 2017), which was extracted from table 11.3 of Campbell and Madden (1990). The data represent a single transect with the number of Macrophomia phaseolina propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.\n\nmp <- data.frame(\n  i = c(1:16),\n  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)\n)\nmp\n\n    i  y\n1   1 41\n2   2 60\n3   3 81\n4   4 22\n5   5  8\n6   6 20\n7   7 28\n8   8  2\n9   9  0\n10 10  2\n11 11  2\n12 12  8\n13 13  0\n14 14 43\n15 15 61\n16 16 50\n\n\nWe can produce a plot to visualize the number of propagules across the transect.\n\nmp %>% \n  ggplot(aes(i, y))+\n  geom_col(fill = \"darkorange\")+\n  theme_minimal_grid()+\n  labs(x = \"Relative position within a transect\", \n       y = \"Number of propagules\",\n       title = \"Macrophomina phaseolina in the soil\",\n       caption = \"Source: Campbell and Madden (1990)\")\n\n\n\n\nTo calculate the autocorrelation coefficient in R, we can use the ac function of the tseries package.\n\nlibrary(tseries)\nac_mp <- acf(mp$y, lag = 5, pl = FALSE)\nac_mp\n\n\nAutocorrelations of series 'mp$y', by lag\n\n     0      1      2      3      4      5 \n 1.000  0.586  0.126 -0.033 -0.017 -0.181 \n\n\nLet’s store the results in a dataframe to facilitate visualization using ggplot.\n\nac_mp_dat <- data.frame(index = ac_mp$lag, ac_mp$acf)\nac_mp_dat\n\n  index   ac_mp.acf\n1     0  1.00000000\n2     1  0.58579374\n3     2  0.12636306\n4     3 -0.03307249\n5     4 -0.01701392\n6     5 -0.18092810\n\n\nAnd now the plot known as autocorrelogram.\n\nac_mp_dat %>% \n  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf,3)))+\n  geom_col()+\n  geom_text(vjust = 0, nudge_y = 0.05)+\n  scale_x_continuous(n.breaks = 6)+\n  theme_minimal_grid()+\n  geom_hline(yintercept = 0)+\n  labs(x = \"Distance lag\", y = \"Autocorrelation coefficient\")\n\n\n\n\nThe values we obtained here are not the same but quite close to the values reported in Madden et al. (2007). For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.\nThe method above is usually referred to Moran’s I (Moran, 1950). Let’s use another example dataset from the book to calculate the Moran’s I in R. The data is shown in page 269 of the book. The data represent the number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats. It was based on an epidemic generated using the stochastic simulator of Xu and Madden (2004). The data is stored in a csv file.\n\nepi <- read_csv(\"data/xu-madden-simulated.csv\")\n# Transform from wide to long format \n# Pull the n variable to store as a vector\nepi1 <- epi %>% \n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") %>% \n  pull(n)\n\nUsing moran function of the spdep R package.\n\nset.seed(100)\nlibrary(spdep)\n\nThe cell2nb function creates the neighbor list with 12 rows and 12 columns, which is how the 144 quadrats are arranged.\n\nnb <- cell2nb(12, 12, type=\"queen\", torus = FALSE)\n\nThe nb2listw function supplements a neighbors list with spatial weights for the chosen coding scheme. We use the default W, which is the row standardized (sums over all links to n). We then create the col.W neighbor list.\n\ncol.W <- nb2listw(nb, style=\"W\")\n\nThe Moran’s I statistic is given by the moran function\n\nmoran(x = epi1, # numeric vector\n      listw = col.W, # the nb list\n      n = 12, # number of zones\n      S0 = Szero(col.W)) # global sum of weights\n\n$I\n[1] 0.05818595\n\n$K\n[1] 2.878088\n\n\nThe Moran’s test for spatial autocorrelation uses spatial weights matrix in weights list form.\n\nmoran.test(x = epi1, \n           listw = col.W)\n\n\n    Moran I test under randomisation\n\ndata:  epi1  \nweights: col.W    \n\nMoran I statistic standard deviate = 15.919, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.698231416      -0.006993007       0.001962596 \n\n\n\ncorrel_I <- sp.correlogram(nb, epi1, \n                           order = 10,\n                           method = \"I\",  \n                           zero.policy = TRUE)\n\nWe can generate a correlogram using the output of the sp.correlogram function. Note that the figure below is very similar to the one shown in Figure 91.5 in page 269 of the book chapter (Madden et al. 2017). Let’s store the results in a dataframe.\n\ndf_correl <- data.frame(correl_I$res) %>% \n  mutate(lag = c(1:10))\n\n# Show the spatial autocorrelation for 10 distance lags\nround(df_correl$X1,3)\n\n [1]  0.698  0.340  0.086 -0.002 -0.009 -0.024 -0.090 -0.180 -0.217 -0.124\n\n\nThen, we can generate the plot using ggplot.\n\ndf_correl %>% \n  ggplot(aes(lag, X1))+\n  geom_col(fill = \"darkorange\")+\n  theme_minimal_grid()+\n  scale_x_continuous(n.breaks = 10)+\n  labs(x = \"Distance lag\", y = \"Spatial autocorrelation\")\n\n\n\n\n\n\n3.2.2.2 Semivariance\nSemi-variance is a key quantity in geostatistics. This differs from spatial autocorrelation because distances are usually measured in discrete spatial lags. The semi-variance can be defined as half the variance of the differences between all possible points spaced a constant distance apart.\nThe semi-variance at a distance d = 0 will be zero, because there are no differences between points that are compared to themselves. However, as points are compared to increasingly distant points, the semi-variance increases. At some distance, called the Range, the semi-variance will become approximately equal to the variance of the whole surface itself. This is the greatest distance over which the value at a point on the surface is related to the value at another point. In fact, when the distance between two sampling units is small, the sampling units are close together and, usually, variability is low. As the distance increases, so (usually) does the variability.\nResults of semi-variance analysis are normally presented as a graphical plot of semi-variance against distance, which is referred to as a semi-variogram. The main characteristics of the semi-variogram of interest are the nugget, the range and the sill, and their estimations are usually based on an appropriate (non-linear) model fitted to the data points representing the semi-variogram.\nFor the semi-variance, we will use the variog function of the geoR package. We need the data in the long format (x, y and z). Let’s reshape the data to the long format and store it in epi2 dataframe.\n\nepi2 <-epi %>% \n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") %>% \n  mutate(y = as.numeric(y))\n\nhead(epi2)\n\n# A tibble: 6 × 3\n      x     y     n\n  <dbl> <dbl> <dbl>\n1     1     1     2\n2     1     2     2\n3     1     3     3\n4     1     4    33\n5     1     5     4\n6     1     6     0\n\n\n\nlibrary(geoR)\n# the coordinates are x and y and the data is the n\nv1 <- variog(coords = epi2[,1:2], data = epi2[,3])\n\nvariog: computing omnidirectional variogram\n\n\n\n# Model fitting\nv2 <- variofit(v1, ini.cov.pars = c(1200, 12), \n               cov.model = \"exponential\", \n               fix.nugget = F)\n\nvariofit: covariance model used is exponential \nvariofit: weights used: npairs \nvariofit: minimisation function used: optim \n\n# Plotting \nplot(v1, xlim = c(0,15))\nlines(v2, lty = 1, lwd = 2)\n\n\n\n\n\n\n3.2.2.3 SADIE\nSADIE (spatial analysis by distance indices) is an alternative to autocorrelation and semi-variance methods described previously, which has found use in plant pathology (Madden et al. 2017; Xu and Madden 2004; Li et al. 2011). Similar to those methods, the spatial coordinates for the disease intensity (count of diseased individuals) or pathogen propagules values should be provided.\nSADIE quantifies spatial pattern by calculating the minimum total distance to regaularity. That is, the distance that individuals must be moved from the starting point defined by the observed counts to the end point at which there is the same number of individuals in each sampling unit. Therefore, if the data are highly aggregated, the distance to regularity will be large, but if the data are close to regular to start with, the distance to regularity will be smaller.\nThe null hypothesis to test is that the observed pattern is random. SADIE calculates an index of aggregation (Ia). When this is equal to 1, the pattern is random. If this is greater than 1, the pattern is aggregated. Hypothesis testing is based on the randomization procedure. The null hypothesis of randomness, with an alternative hypothesis of aggregation.\nAn extension was made to quantify the contribution of each sampling unit count to the observed pattern. Regions with large counts are defined as patches and regions with small counts are defined as gaps. For each sampling unit, a clustering index is calculated and can be mapped.\nIn R, we can use the sadie function of the epiphy package. The function computes the different indices and probabilities based on the distance to regularity for the observed spatial pattern and a specified number of random permutations of this pattern. To run the analysis, the dataframe should have only three columns: the first two must be the x and y coordinates and the third one the observations. Let’s continue working with the simulated epidemic dataset named epi2. We can map the original data as follows:\n\nepi2 %>% \n  ggplot(aes(x, y, label = n, fill = n))+\n  geom_tile()+\n  geom_text(size = 5)+\n  theme_void()+\n  coord_fixed()+\n  scale_fill_viridis_c()\n\n\n\n\n\nlibrary(epiphy)\nsadie_epi2 <- sadie(epi2)\n\nComputation of Perry's indices:\n\nsadie_epi2\n\nSpatial Analysis by Distance IndicEs (sadie)\n\nCall:\nsadie.data.frame(data = epi2)\n\nIa: 2.4622 (Pa = < 2.22e-16)\n\n\nThe simple output shows the Ia value and associated P-value. As suggested by the low P-value, the pattern is highly aggregated. The summary function provides a more complete information such as the overall inflow and outflow measures. A dataframe with the clustering index for each sampling unit is also provided.\n\nsummary(sadie_epi2)\n\n\nCall:\nsadie.data.frame(data = epi2)\n\nFirst 6 rows of clustering indices:\n  x y  i cost_flows      idx_P idx_LMX prob\n1 1 1  2 -11.382725 -7.2242617      NA   NA\n2 1 2  2  -9.461212 -6.2258877      NA   NA\n3 1 3  3  -7.299482 -5.3390880      NA   NA\n4 1 4 33   1.000000  0.8708407      NA   NA\n5 1 5  4  -5.830952 -3.6534511      NA   NA\n6 1 6  0  -5.301329 -2.9627172      NA   NA\n\nSummary indices:\n                      overall    inflow  outflow\nPerry's index        2.495346 -2.811023 2.393399\nLi-Madden-Xu's index       NA        NA       NA\n\nMain outputs:\nIa: 2.4622 (Pa = < 2.22e-16)\n\n'Total cost': 201.6062\nNumber of permutations: 100\n\n\nThe plot function allows to map the clustering indices and so to identify regions of patches (red, outflow) and gaps (blue, inflow).\n\nplot(sadie_epi2)\n\n\n\n\nA isocline plot is also possible to obtain by setting the isocline argument as TRUE.\n\nplot(sadie_epi2, isoclines = TRUE)\n\n\n\n\n\n\n\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance indices: an alternative local clustering index for studying spatial patterns. Methods in Ecology and Evolution. 3:368–377 Available at: http://dx.doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F., eds. 2017. CHAPTER 9: Spatial aspects of epidemicsIII: Patterns of plant disease. In The American Phytopathological Society, p. 235–278. Available at: http://dx.doi.org/10.1094/9780890545058.009.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics to study spatial dynamics of plant disease epidemics. Plant Pathology. 53:38–49 Available at: http://dx.doi.org/10.1111/j.1365-3059.2004.00949.x."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Jeger, M. J., and Viljanen-Rollinson, S. L. H. 2001. The use of the area\nunder the disease-progress curve (AUDPC) to assess quantitative disease\nresistance in crop cultivars. Theoretical and Applied Genetics.\n102:32–40 Available at: http://dx.doi.org/10.1007/s001220051615.\n\n\nLi, B., Madden, L. V., and Xu, X. 2011. Spatial analysis by distance\nindices: an alternative local clustering index for studying spatial\npatterns. Methods in Ecology and Evolution. 3:368–377 Available at: http://dx.doi.org/10.1111/j.2041-210x.2011.00165.x.\n\n\nMadden, L. V., Hughes, G., and Bosch, F. van den. 2017a. The study of\nplant disease epidemics. Available at: http://dx.doi.org/10.1094/9780890545058.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F., eds. 2017b. CHAPTER 4:\nTemporal analysis i: Quantifying and comparing epidemics. In The\nAmerican Phytopathological Society, p. 63–116. Available at: http://dx.doi.org/10.1094/9780890545058.004.\n\n\nMadden, L. V., Hughes, G., and van den Bosch, F., eds. 2017c. CHAPTER 9:\nSpatial aspects of epidemicsIII: Patterns of plant disease.\nIn The American Phytopathological Society, p. 235–278. Available at: http://dx.doi.org/10.1094/9780890545058.009.\n\n\nSimko, I., and Piepho, H.-P. 2012. The Area Under the Disease Progress\nStairs: Calculation, Advantage, and Application. Phytopathology®.\n102:381–389 Available at: http://dx.doi.org/10.1094/phyto-07-11-0216.\n\n\nXu, X.-M., and Madden, L. V. 2004. Use of SADIE statistics to study\nspatial dynamics of plant disease epidemics. Plant Pathology. 53:38–49\nAvailable at: http://dx.doi.org/10.1111/j.1365-3059.2004.00949.x."
  },
  {
    "objectID": "spatial-gradients.html#models",
    "href": "spatial-gradients.html#models",
    "title": "2  Spatial gradients",
    "section": "2.2 Models",
    "text": "2.2 Models\nWhen modeling disease gradients, the distance is represented by \\(x\\), a continuous variable which can be expressed by various units (cm, m, km, etc). The gradient models, similar to the population dynamics models (disease progress) are of the deterministic type. The difference is that, for disease progress curves, disease intensity tends to increase with increasing time, while in disease gradients the disease intensity tends to decrease with increasing distance from the source of inoculum. Two models are most commonly fitted to data on disease gradients. More details about these models can be obtained it this tutorial.\n\n2.2.1 Exponential model\nThe exponential model is also known as Kiyosawa & Shiyomi model. The differential of the exponential model is given by\n\\(\\frac{dy}{dx}\\) = \\(-b_{E}.y\\) ,\nwhere \\(b_{E}\\) is the exponential form of the rate of decline and \\(y\\) is the disease intensity. This model suggests that \\(y\\) (any disease intensity) is greater close to the source of inoculum, or at the distance zero. The integral form of the model is given by\n\\(y = a . e^{-b.x}\\) ,\nwhere \\(a\\) is the disease intensity at the distance zero and \\(b\\) is the rate of decline, in this case negative because disease intensity decreases with the increase of the distance from inoculum source. Let’s make a plot for two disease gradients of varying parameters for this model.\nFirst we need to load essential packages for programming, customizing the outputs and defining a global ggplot theme.\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(cowplot) # for themes \ntheme_set(theme_minimal_grid()) # set global theme\n\nSet the parameters for the exponential model with two rates and same inoculum at the source:\n\na1 <- 0.2 # y at distance zero for gradient 1\na2 <- 0.2 # y at distance zero for gradient 2\nb1 <- 0.1 # decline rate for gradient 1\nb2 <- 0.05 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\nThe following code allows to visualize the model predictions.\n\ndat %>%\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) a1 * exp(-b1 * x), linetype = 1) +\n  stat_function(fun = function(x) a2 * exp(-b2 * x), linetype = 2) +\n  ylim(0, a1) +\n  annotate(\"text\", x = 20, y = 0.04, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.10, label = \"b = 0.05\") +\n  labs(\n    title = \"Exponential model\",\n    subtitle = \"\",\n    x = \"Distance (m)\",\n    y = \"Disease incidence (proportion)\"\n  )\n\n\n\n\n\n\n2.2.2 Power law model\nAlso known as the modified Gregory’s model (Gregory was a pioneer in the use this model to describe plant disease gradients). In the power law model, \\(Y\\) is proportional to the power of the distance, and is given by:\n\\(Y = a_{P}.x - b_{P}\\)\nwhere \\(a_{P}\\) and \\(b_{P}\\) are the two parameters of the power law model. They differ from the exponential because as closer to \\(x\\) is to zero, \\(Y\\) is indefinitely large (not meaningful biologically). However, the model can still be useful because it produces realistic values at any distance \\(x\\) away from the source. The values of the \\(a_{P}\\) parameter should be interpreted in accord to the scale of \\(x\\), whether in centimeters or meters. If the distance between the source and the first measure away from the source is 0.5m, it is so more appropriate to record the distance in cm than in m or km.\nOnce \\(y\\) at the distance zero from the source is undefined when using the power law model, this is usually modified by the addition of a positive constant \\(C\\) in \\(x\\):\n\\(Y = a_{P}.(x + C) - b_{P}\\)\nFor this reason, the model is named as the modified power law. Here, the constant \\(C\\) is of the same unit of \\(x\\). At the distance zero, the positive constant is a term that express the size of the inoculum source. In other words, the \\(a\\) parameter is a theoretical value of \\(Y\\) at the distance \\(1-C\\) from the center of the inoculum source.\nLet’s plot two gradients with two rate parameters for the modified power law model:\n\nC <- 0.5\na1 <- 0.2 # y at zero distance for gradient 1\na2 <- 0.2 # y at zero distance for gradient 2\nb1 <- 0.5 # decline rate for gradient 1\nb2 <- 0.7 # decline rate for gradient 2\nmax1 <- 80 # maximum distance for gradient 1\nmax2 <- 80 # maximum distance for gradient 2\ndat2 <- data.frame(x = seq(1:max1), y = seq(0:a1))\n\n\ndat2 %>%\n  ggplot(aes(x, y)) +\n  stat_function(fun = function(x) a1 * ((x + C)^-b1), linetype = 1) +\n  stat_function(fun = function(x) a2 * ((x + C)^-b2), linetype = 2) +\n  ylim(0, a1 - 0.02) +\n  annotate(\"text\", x = 20, y = 0.03, label = \"b = 0.1\") +\n  annotate(\"text\", x = 20, y = 0.06, label = \"b = 0.05\") +\n  labs(\n    title = \"Modified Power Law\",\n    subtitle = \"\",\n    x = \"Distance (m)\",\n    y = \"Disease incidence\"\n  )\n\n\n\n\nThe differential equation of the power law model is given by:\n\\(\\frac{dy}{dx}\\) = \\(\\frac{-b_{P}.Y}{x - C}\\)\nSimilar to the exponential model, \\(\\frac{dy}{dx}\\) is proportional to \\(Y\\), meaning that the gradient is steeper (more negative) at the highest disease intensity value, usually closer to the source."
  }
]