# Spatial patterns

## Introduction

A spatial disease pattern can be defined as the arrangement of diseased entities relative to each other and to the architecture of the host crop [@chapter2017] . Such arrangement is the realization of the underlying dispersal of the pathogen, from one or several sources within and/or outside the area of interest, under the influence of physical, biological and environmental factors.

The study of spatial patterns is conducted at a specific time or multiple times during the epidemic. When assessed multiple times, both spatial and temporal processes can be characterized. Because epidemics change over time, it is expected that spatial patterns are not constant but change over time as well. Usually, plant pathologists are interested in determining spatial patterns at one or various spatial scales, depending on the objective of the study. The scale of interest may be a leaf or root, plant, field, municipality, state, country or even intercontinental area. The diseased units observed may vary from lesions on a single leaf to diseased fields in a large production region.

The patterns can be classified into two main types that occur naturally: **random** or **aggregated**. The random pattern originates because the chances for the units (leaf, plant, crop) to be infected are equal and low, and are largely independent from each other. In aggregated spatial patterns, such chances are unequal and there is dependency among the units, for example, a healthy unit close to a diseased unit is at higher risk than more distant units.

A range of techniques, most based on statistical tests, can be used to detect deviations from randomness in space and the choice of the methods depends on the scale of observation. Usually, more than one test is applied for the same or different scales of interest depending on how the data are collected. Three general categories of statistical tests can be determined based on the spatial scale and type of data collected: intensively mapped (plant to plant); quadrat or plot count/incidence data; or distance among the diseased units.

## Intensively mapped

### Binary data

In this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted is a binay variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.

#### Runs test

A run is defined as a succession of one or more diseased or healthy plants, which are followed and preceded by a plant of the other disease status or no plant at all. There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.

Let's create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 20 plants and assign it to `y`. For plotting purposes, we make a dataframe for more complete information.

```{r}
y1 <- c(1,1,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,1)
x1 <- c(1:20) # position of each plant
z1 <- 1
row1 <- data.frame(x1, y1, z1) # create a dataframe
```

We can then visualize the series using ggplot and count the number of runs as 7, aided by the color used to identify a run.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
row1 %>% 
  ggplot(aes(x1, z1, label = x1, color = factor(y1)))+
  geom_point(shape =15, size =6)+
  theme_void()+
  scale_x_continuous(breaks = max(z1))+
  scale_color_manual(values = c("grey80", "grey20"))+
  geom_text(vjust = 0, nudge_y = 0.5)+
coord_fixed()+
  ylim(-0.5,2.5)+
  theme(legend.position = "right")+
  labs(color = "Status", title = "Sequence of diseased (1) or non-diseased (0) units (plants)", 
       subtitle = "The numbers represent the position of the unit")
```

We can write a code in R and create a function named `oruns.test` for the ordinary runs test.

```{r}
oruns.test <- function(x) {
# identify the sequence
S <- x 
# Compute the number or runs
U = max(cumsum(c(1, diff(S)!=0)))
# Compute the number of diseased plants
m = sum(S)
# Count the total number of plants
N = length(S)
# Calculate the number of expected runs
EU = 1 + (2 * m*(N - m)/N)
# Calculate the standard deviation in the sample
sU = sqrt(2 * m * (N - m) * (2 * m *(N-m)-N)/ (N^2 *(N-1)))
# Calculate the z-value
Z = (U - EU)/sU
# Obtain the p-value for the Z
pvalue <- (2*pnorm(abs(Z), lower.tail=FALSE))
# test if Z is lower than 1.64
result <- ifelse(Z < 1.64, 
c("clustering"), 
c("randomness"))
# Print the results
print(paste("There are",U,"runs. The number of expected runs is", round(EU,1), "P-value:",round(pvalue,6), ". Alternative hypothesis: non-randomness"))
}
```

We can now run the test for the example series above.

```{r}
oruns.test(row1$y1)
```

There are built-in functions in R packages that allow for running the ordinary runs test. Let's load the packages and runt the test. Note that the results of the `runs.test` is the same as the one produced by our custom function.

```{r}
library(randtests)
runs.test(row1$y1, threshold = 0.5)

library(DescTools)
r <- RunsTest(row1$y1)

```

#### Doublets

Doublet analysis is used to compare the observed number or adjacent diseased plants, a doublet (DD or 11), to the number expected if the disease were randomly distributed in the yard. If the observed number is greater than the expected number, contagion within the field is suspected.

Let's manually produce a code to execute the doublets test. To facilitate, we can create a function and name it `doublets.test`. The only argument needed is the vector of binary data.

```{r}
doublets.test <- function(x) {
# Identify the sequence
S <- x
# Compute the number of doublets Db
matrix <- cbind(S[-length(S)], S[-1])
pairs <- table(data.frame(matrix))
Db <- pairs[2,2]
# Count the number of diseased plants
N <- length(S) 
# Count the number of total plants
m = sum(S) 
# Expected number of doublets
EDb = m *((m -1)/N)
# Standard deviation 
SDb = sqrt ( EDb * (1 - (2 / N)))
# Calculate the Z-value 
ZDb = (Db - EDb)/ SDb 
# two-sided P-value calculation
pvalue <- (2*pnorm(abs(ZDb), lower.tail = FALSE))
# Result of the test
result <- ifelse(abs(ZDb) >= 1.64, 
c("aggregation or clustering"), 
c("randomness")) 
# Print the results
print(paste("There are",Db,"doublets. The number of expected doublets is",EDb,".","P-value:", round(pvalue,4), ". Alternative hypothesis: non-randomness"))
}
```

```{r}
# Run the function calling the vector
doublets.test(row1$y1)
```

#### Join count

In this analysis, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The orientation(s) of interest (along rows, across rows, diagonally, or a a combination o these) should be specified. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation.

In R, we can use the `join.count` function of the `spdep` package to perform a joint count test. First we need to create the series of binary data from top to bottom and left to right. The data are shown in Fig. 9.13 in page 260 of the book chapter on spatial analysis [@chapter2017]. In the example, there are 5 rows and 5 columns. This will be informed later to run the test.

```{r}
# Enter the data
S2 <- c(1,0,1,1,0,
       1,1,0,0,0,
       1,0,1,0,0,
       1,0,0,1,0,
       0,1,0,1,1)
```

Visualize the two-dimensional array:

```{r}
# Convert to raster 
mapS2 <- raster::raster(matrix(S2, 5 ,5))

# Convert to data frame
mapS3 <- raster::as.data.frame(mapS2,xy=TRUE)

# Map using ggplot
mapS3 %>% 
  ggplot(aes(x, y, label = layer, fill = factor(layer)))+
  geom_tile(color = "black", size =1)+
  theme_void()+
  geom_text(size = 10)+
  labs(fill = "Status")+
  scale_fill_manual(values = c("white", "grey80"))
```

Load the library

```{r message=FALSE, warning=FALSE}
library(spdep)
```

First, we need to generate a list of neighbors (nb) for a grid of cells. This is performed with the `cell2nb` function by informing the number of rows and columns. The argument "rook" means shared edge, but it could be the "queen", for shared edge or vertex. We can use the default.

```{r}
nb <- cell2nb(nrow = 5, 
              ncol = 5, 
              type="rook")
```

The `joincount.test` function runs the BB join count test for spatial autocorrelation. From the function description, the method uses a spatial weights matrix in weights list form for testing whether same-status joins occur more frequently than would be expected if the zones were labelled in a spatially random way. We need to inform the sequence as factor and the nb object we created previously.

```{r}
joincount.test(factor(S2), 
                nb2listw(nb))
```

The function returns a list with a class for each of the status (in this case 0 and 1) with several components. We should look at the **P-value**. The alternative hypothesis (greater) is that the same status joins occur more frequently than expected if they were labelled in a spatial random way. In this case, we do not reject the null hypothesis of randomness.

We can run the ordinary runs and doublets tests, which only considers the adjacent neighbor, for the same series and compare the results.

```{r}
oruns.test(S2)
doublets.test(S2)
```

Let's repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence to reject the null hypothesis, indicating aggregation of plants.

```{r}
S3 <- c(1,1,1,0,0,
       1,1,1,0,0,
       1,1,1,0,0,
       1,1,1,0,0,
       0,0,0,0,0)

joincount.test(factor(S3), 
                nb2listw(nb))
oruns.test(S3)

```

We can apply these tests for a real example epidemic data provided by the [epiphy](https://chgigot.github.io/epiphy/) R package. Let's work with part of the intensively mapped data on the incidence of tomato spotted wilt virus (TSWV) disease in field trials reported by Cochran (1936) and Bald (1937). First, we need to load the library and then assign one dataframe (the dataset has two dataframes) of the dataset `tomato_tswv` to a new dataframe called `tswv_1929`.

```{r message=FALSE, warning=FALSE}
library(epiphy) 
library(cowplot) # theming the ggplot
tswv_1929 <- tomato_tswv$field_1929
head(tswv_1929) 
```

The inspection of the first rows of the dataframe shows five variables where x and y are spatial grid coordinates, t is assessment time, i is the status of the plant (0 = healthy, 1 = diseased) and n is the sampling unit size (here all one). Let's visualize these data for each sampling time.

```{r}
tswv_1929 %>% 
  ggplot(aes(x, y, fill = factor(i)))+
  geom_tile()+
  coord_fixed()+
  theme_minimal_grid()+
  scale_fill_grey(start = 0.8, end = 0.2)+
  facet_wrap(~ t)+
  labs(fill = "Status")
```

Check the number of rows (y) and columns (x) for further preparing the neighbor object for the join count statistics.

```{r}
tswv_1929 %>% 
  select(x, y) %>% 
  summary()
```

There are 60 rows and 24 columns.

```{r}
# Neighbor grid
nb1 <- cell2nb(nrow = 60, 
              ncol = 24, 
              type="rook")

# Pull the binary sequence of time 1
S1 <- tswv_1929 %>% 
  filter(t == "1") %>% 
  pull(i)

joincount.test(factor(S1), 
                nb2listw(nb1))
```

We can apply the join count test for time 2 and time 3. Results show that the pattern changes from random to aggregate over time.

```{r}
# Pull the binary sequence of time 1
S2 <- tswv_1929 %>% 
  filter(t == "2") %>% 
  pull(i)

joincount.test(factor(S2), 
                nb2listw(nb1))

# Pull the binary sequence of time 1
S3 <- tswv_1929 %>% 
  filter(t == "3") %>% 
  pull(i)

joincount.test(factor(S3), 
                nb2listw(nb1))

```

### Grouped data

If the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are autocorrelation (known as Moran's I), semivariance and SADIE (an alternative approach to autocorrelation.)

#### Autocorrelation

Spatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity [@chapter2017].

We will illustrate the method by reproducing the example provided in page 264 of the chapter on spatial analysis [@chapter2017], which was extracted from table 11.3 of Campbell and Madden (1990). The data represent a single transect with the number of *Macrophomia phaseolina* propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.

```{r}
mp <- data.frame(
  i = c(1:16),
  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)
)
mp
```

We can produce a plot to visualize the number of propagules across the transect.

```{r}
mp %>% 
  ggplot(aes(i, y))+
  geom_col(fill = "darkorange")+
  theme_minimal_grid()+
  labs(x = "Relative position within a transect", 
       y = "Number of propagules",
       title = "Macrophomina phaseolina in the soil",
       caption = "Source: Campbell and Madden (1990)")
```

To calculate the autocorrelation coefficient in R, we can use the `ac` function of the *tseries* package.

```{r message=FALSE, warning=FALSE}
library(tseries)
ac_mp <- acf(mp$y, lag = 5, pl = FALSE)
ac_mp
```

Let's store the results in a dataframe to facilitate visualization using ggplot.

```{r}
ac_mp_dat <- data.frame(index = ac_mp$lag, ac_mp$acf)
ac_mp_dat
```

And now the plot known as autocorrelogram.

```{r}
ac_mp_dat %>% 
  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf,3)))+
  geom_col()+
  geom_text(vjust = 0, nudge_y = 0.05)+
  scale_x_continuous(n.breaks = 6)+
  theme_minimal_grid()+
  geom_hline(yintercept = 0)+
  labs(x = "Distance lag", y = "Autocorrelation coefficient")
  
```

The values we obtained here are not the same but quite close to the values reported in Madden et al. (2007). For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.

The method above is usually referred to Moran's I (Moran, 1950). Let's use another example dataset from the book to calculate Moran's I in R. The data is shown in page 269 of the book. The data are number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats, based on an epidemic generated using the stochastic simulator of Xu and Madden (2004). The data is stored in a csv file.

```{r}
epi <- read_csv("data/xu-madden-simulated.csv")
# transform from wide to long format and pull the n variable
epi1 <- epi %>% 
  pivot_longer(2:13,
               names_to = "quadrat",
               values_to = "n") %>% 
  pull(n)

```

Using `moran` function of the *spdep* R package. 

```{r}
set.seed(100)
library(spdep)
nb <- cell2nb(12, 12, type="queen", torus = FALSE) #
col.W <- nb2listw(nb, style="W")
count<- epi1

# Cálculo do Moran's I
moran(count, col.W, 12, Szero(col.W))

# Aplicando o teste de Moran's
moran.test(count, nb2listw(nb, style="W"))

# Correlogram
correl_I <- sp.correlogram(nb, count, order=10,
                           method="I",  zero.policy=TRUE)
plot(correl_I, main="Correlogram")


```

#### Semivariance


#### SADIE (distance indices)




