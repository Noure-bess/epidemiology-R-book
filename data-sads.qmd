---
title: "Standard area diagrams"
editor_options: 
  chunk_output_type: inline
---

::: {.callout-note appearance="simple"}
This is a work in progress that is currently undergoing heavy technical editing and copy-editing
:::

## Definitions

According to a glossary on phytopathometry [@bock2021], standard area diagram (SAD) can be defined as "*a generic term for a pictorial or graphic representation (drawing or true-color photo) of selected disease severities on plants or plant parts (leaves, fruit, flowers, etc.) generally used as an aid for more accurate visual estimation (on the percentage scale) or classification (using an ordinal scale) of severity on a specimen".*

The SADs, also known as diagrammatic scales, are a set of diagrams which have long use in plant pathology. The tool dates back to the late 1800s, when the Cobb scale was developed with five diagrams illustrating a range of severity values of rust pustules on wheat leaves.

In the last 20 years, plant pathologists have taken advantage of advances in image processing and analysis tools and from knowledge gained from the psychophysical and measurement sciences to develop SADs that are realistic (e.g. true color photographs), with appropriate validation and illustrated severities to maximize estimation accuracy. SADs have been designed in various color types (black or white, two-color or true-color) and and incremental scales (approximated linear or logarithmic) [@delponte2017].

The SADs have proven useful to increase accuracy of the visual estimates as the estimation of percentage areas is deemed more challenging than classification of severity into [ordinal classes](data-ordinal.html) - there is a large number of options to choose from on the percentage scale, compared to the finite and small number of classes in ordinal scales. A recent quantitative review confirmed that the use of SADs most often results in improved accuracy and precision of visual estimates, but identified factors related to SAD design and structure, disease symptoms, and actual severity that affected the results. In particular, the SADs have proven greater utility for raters that inherently less accurate and diseases characterized by small and numerous lesions [@delponte2022]. Follows examples of SADs in black and white, two-color and true-color:

------------------------------------------------------------------------

![Actual photos of symptoms of loquat scab on fruit (left) and a SADs with eight diagrams (right). Each number represents severity as the percent area affected [@gonzález-domínguez2014]](imgs/sad-loquat.png){#fig-sad-loquat fig-align="center" width="625"}

------------------------------------------------------------------------

![SADs for Glomerella leaf spot on apple leaf. Each number represents severity as the percent area affected [@moreira2018]](imgs/sad-apple.png){#fig-sad-apple fig-align="center" width="622"}

------------------------------------------------------------------------

![SADs for soybean rust. Each number represents severity as the percent area affected [@franceschi2020]](imgs/sad-sbr.png){#fig-sad-sbr fig-align="center" width="625"}

More SADs can be found in the [SADBank](https://emdelponte.github.io/sadbank/), a curated collection of articles on SAD development and validation. Click on the image below to get access to the database.

[![SADBank, a curated collection of articles](imgs/sadbank.png){#fig-sadbank fig-align="center" width="87%"}](https://emdelponte.github.io/sadbank/)

## SAD development and validation

A systematic review of the literature on SADs highlighted the most important aspects related with the development and validation of the tool [@delponte2017]. A list of best practices was proposed in the review to guide future research in the area. Follows the most important aspects to be noted:

::: callout-tip
## Best practices on SADs development

-   Sample a minimum number (e.g., n = 100) of specimens from natural epidemics representing the range of disease severity and typical symptoms observed.

-   Use reliable image analysis software to discriminate disease symptoms from healthy areas to calculate percent area affected.

-   When designing the illustrations for the SAD set, ensure that the individual diagrams are prepared realistically, whether line drawn, actual photos, or computer generated.

-   The number of diagrams should be no less than 6 and no more than 10, distributed approximately linearly, and spaced no more than 15% apart. Additional diagrams (±2) should be included between 0 and 10% severity.

-   For the validation trial, select at least 50 specimens representing the full range of actual severity and symptom patterns.

-   When selecting raters (a minimum of 15) for validation, make sure they do not have previous experience in using the SAD under evaluation.

-   Provide standard instructions on how to recognize the symptoms of the disease and how to assess severity, first without and then with the SAD.

-   Ideally repeat the assessment in time, with a 1- or 2-week interval, both without and with the aid, using the same set of raters in order to evaluate the effect of training and experience on gains in accuracy.

-   Both pre- and posttest experiment conditions should be the same to avoid any impact of distraction on accuracy of estimates during the tests.
:::

## Designing SADs in R

The diagrams used in the set have been designed using several methods and technology, ranging from hand drawing to actual photos [@delponte2017]. There is an increasing tendency to use actual photos that are analysed digitally, using standard image analysis software, to determine the percent area affected. In this approach, a large set of images is analyzed and some images are chosen to represent the severities of the SAD according to the scale structure.

In R, the pliman package has a function called `sad()` which allows the automatic generation of a SADs with a pre-defined number of diagrams. Firstly, as shown in the [previous chapter](data-actual-severity.html), the set of images to be selected needs to be analysed using the `measure_disease()` function. Then, a SADs is automatically generated. In the function, the leaves with the smallest and highest severity will be selected for the SAD. The intermediate diagrams are sampled sequentially to achieve the pre-defined number of images after the severity has been ordered from low to high. More details of the function [here](https://tiagoolivoto.github.io/pliman/reference/sad.html).

Let's use the [same set](data-actual-severity.html#multiple-images) of 10 soybean leaves, as seen in the previous chapter, depicting the rust symptoms and create the `sbr` object.

```{r}
#| message: false
#| warning: false
library(pliman)
h <- image_import("imgs/sbr_h.png")
s <- image_import("imgs/sbr_s.png")
b <- image_import("imgs/sbr_b.png")

sbr <- measure_disease(
  pattern = "img",
  dir_original = "imgs/originals" ,
  dir_processed = "imgs/processed",
  save_image = TRUE,
  img_healthy = h,
  img_symptoms = s,
  img_background = b,
  show_image = FALSE,
  show_original = FALSE, # set to TRUE for showing the original.
  col_background = "white", 
  verbose = FALSE
)
```

We are ready to run the `sad()` function to create a SADs with five diagrams side by side. The resulting SADs is in two-color as standard. Set the argument `show_original` to `TRUE` for showing the orignal image in the SADs.

```{r}
sad(sbr, 5, ncol = 5)
```

## Analysis of SADs validation data

To evaluate the effect of SAD on accuracy components, analyze the data, preferably using concordance analysis methods ([see chapter](data-accuracy.html)), to fully explore which component is affected and to gain insight into the ramification of errors. Linear regression should not be used as the sole method but it could be complementary for comparison with previous literature.

Inferential methods should be used for testing hypotheses related to gain in accuracy. If parametric tests are used (paired t-test for example), make sure to check that the assumptions are not violated. Alternatively, nonparametric tests (Wilcoxon signed rank) or nonparametric bootstrapping should be used when the conditions for parametric tests are not met. More recently, a (parametric) mixed modelling framework has been used to analyse SADs validation data where raters are taken as a random effects in the model [@gonzález-domínguez2014; @franceschi2020; @pereira2020].

### Boostrapping differences of means

A bootstrap-based equivalence test procedure was first proposed as complementary to parametric (paired t-test) or non-parametric (Wilcoxon) to analyze severity estimation data in a study on the development and validation of a SADs for pecan scab [@yadav2012]. The equivalence test was used to calculate 95% confidence intervals (CIs) for each statistic by bootstrapping using the percentile method (with an equivalence test, the null hypothesis is the converse of H0, i.e. the null hypothesis is non-equivalence). In that study, the test was used to compare means of the CCC statistics across raters under two conditions: 1) without versus with the SAD; and 2) experienced versus inexperienced raters.

Let's work with the CCC data for a sample of 20 raters who estimated severity of soybean rust SAD first without and then with the aid. The CCC was calculated as shown [here](data-accuracy.html#concordance-correlation-coefficient).

```{r}
sbr <- tibble::tribble(
  ~rater, ~aided, ~unaided,
      1L,   0.97,     0.85,
      2L,   0.97,     0.85,
      3L,   0.95,     0.82,
      4L,   0.93,     0.69,
      5L,   0.97,     0.84,
      6L,   0.96,     0.86,
      7L,   0.98,     0.78,
      8L,   0.93,     0.72,
      9L,   0.94,     0.67,
     10L,   0.95,     0.53,
     11L,   0.94,     0.78,
     12L,   0.98,     0.89,
     13L,   0.96,      0.8,
     14L,   0.98,     0.87,
     15L,   0.98,      0.9,
     16L,   0.98,     0.87,
     17L,   0.98,     0.84,
     18L,   0.97,     0.86,
     19L,   0.98,     0.89,
     20L,   0.98,     0.78
  )
```

Let's visualize the data using boxplots. Each point in the plot represents a rater.

```{r}
#| warning: false

library(tidyverse)
theme_set(theme_bw(base_size = 16))
sbr |> 
  pivot_longer(2:3, names_to = "condition", values_to ="estimate") |> 
  ggplot(aes(condition, estimate))+
  geom_boxplot(outlier.colour = NA)+
  geom_jitter(width = 0.05)+
  ylim(0.2,1)
```

We now need to create a variable for the difference of the means of the estimates (aided minus unaided) prior to bootstrapping this difference using two specialized packages. If the 95% CI does not include zero, this means that there was a significant improvement in the statistics.

```{r}
#| warning: false


# diff of means
sbr$diff <- sbr$aided - sbr$unaided


## Using the simpleboot and boot packages
library(simpleboot)
b.mean <- one.boot(sbr$diff, mean, 999)
boot::boot.ci(b.mean)
mean(b.mean$data)
hist(b.mean)

# Using the bootstrap package
library(bootstrap)
b <- bootstrap(sbr$diff, 999, mean)
quantile(b$thetastar, c(.025,.975))
mean(b$thetastar)
sd(b$thetastar)
se <- function(x) sqrt(var(x)/length(x))
se(b$thetastar)

```

### Parametric and non-parametric paired sample tests

In the case that two estimates are obtained by the same rater at different times, the data are not independent. For this situation, a **paired sample t-test** can be used for testing whether the mean difference between two sets of observations is zero. In this test, each subject (the leaf in our case) is measured or estimated twice, resulting in *pairs* of observations. If the assumptions of the test are violated (e.g. lack of normality), an equivalent non-parametric test can be used, such as the Wilcoxon, also known as **Wilcoxon signed-rank test**. It is used when your data are not normally distributed.

Let's apply these two tests for our data and compare the results. First, we need to check whether the data are normally distributed. Then, we can also check whether the variances are equal.

```{r}
# normality test
shapiro.test(sbr$aided)
shapiro.test(sbr$unaided)

# equal variance test
var.test(sbr$aided, sbr$unaided)

# paired t-test
t.test(sbr$aided, sbr$unaided, paired = TRUE)

# Wilcoxon test
wilcox.test(sbr$aided, sbr$unaided, paired = TRUE)
```

As shown above, the two assumptions were violated, so we could rely more confidently on the non-parametric test.

### Mixed modeling

Mixed models are a form of linear modeling used for hierarchical data when the response variable has a normal distribution and the predictor variables are a mix of fixed and random effects. These models are also good when data points might not be fully independent of each other, which is the case in our example.

Here we treat raters as random effects. We expect them to be samples from a larger population about which we are trying to draw conclusions. We can sample more individuals who would be different from the ones we have and we can infer something about the whole population from the sampled in our analyses. The random effects capture variation that exists but isn't relevant to the question, like variation between subjects for which we have repeated measures.

Let's start reshaping our data to the long format and assign it to a new dataframe.

```{r}

sbr2 <- sbr |> 
  pivot_longer(2:3, names_to = "condition", values_to = "estimate")


```

Now we fit the mixed model using the `lmer` function of the lme4 package. We will fit the model to the logit of the estimate because they should be bounded between zero and one. Preliminary analysis using non-transformed or log-transformed data resulted in lack of normality of residuals and heterocedasticity (not shown).

```{r}
#| warning: false
#| message: false
library(lme4) 
library(car) # for logit function
mix <- lmer(logit(estimate) ~ condition + (1 | rater), data = sbr2)

# Check model performance
library(performance)
check_model(mix)
check_normality(mix)
check_heteroscedasticity(mix)

# Check effect of condition
car::Anova(mix)

# Estimate the means for each group
library(emmeans)
em <- emmeans(mix, ~ condition, transform = "response")
em

# Contrast the means
pairs(em)

# plot the means with 95% CIs
plot(em) +
  coord_flip()+
  xlim(0.7,1)

```
