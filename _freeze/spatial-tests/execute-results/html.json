{
  "hash": "ab72b86b180e8a9c531fd708ee0cbebd",
  "result": {
    "markdown": "---\ntitle: \"Tests for patterns\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n::: {.callout-note appearance=\"simple\"}\nThis is a work in progress that is currently undergoing heavy technical editing and copy-editing\n:::\n\nA range of techniques, most based on statistical tests, can be used to detect deviations from randomness in space and the choice of the methods depends on the scale of observation. Usually, more than one test is applied for the same or different scales of interest depending on how the data are collected. The several statistical tests can be classified based on the spatial scale and type of data (binary, count, etc.) collected, but mainly if the spatial location of the unit is known (mapped) or not known (sampled). Following @madden2017, two major groups can be formed. The intensively mapped (binary or grouped data) data or the sparsely sampled (incidence or count data) data.\n\n## Intensively mapped\n\n### Binary data\n\nIn this situation the individual plants are mapped, meaning that their relative positions to one another are known. It is the case when a census is used to map presence/absence data. The status of each unit (usually a plant) is noted as a binary variable. The plant is either diseased (D or 1) or non-diseased or healthy (H or 0). Several statistical tests can be used to detect a deviation from randomness. The most commonly used tests are runs, doublets and join count.\n\n#### Runs test\n\nA run is defined as a succession of one or more diseased or healthy plants, which are followed and preceded by a plant of the other disease status or no plant at all. There would be few runs if there is an aggregation of diseased or healthy plants and a large number of runs for a random mixing of diseased and healthy plants.\n\nLet's create a vector of binary (0 = non-diseased; 1 = diseased) data representing a crop row with 20 plants and assign it to `y`. For plotting purposes, we make a dataframe for more complete information.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-1_eb5c40d4f8dda2785c548c2a842b4cbb'}\n\n```{.r .cell-code}\nlibrary(tidyverse) \ntheme_set(theme_bw(base_size = 16))\n```\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-2_67aabc95eea09add96dd952544cbf971'}\n\n```{.r .cell-code}\ny1 <- c(1,1,1,0,0,0,0,0,1,0,0,0,0,1,1,0,0,0,1,1)\nx1 <- c(1:20) # position of each plant\nz1 <- 1\nrow1 <- data.frame(x1, y1, z1) # create a dataframe\n```\n:::\n\n\nWe can then visualize the series using ggplot and count the number of runs as 7, aided by the color used to identify a run.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-runs_0eca293db0be6efd524736e9ae647151'}\n\n```{.r .cell-code}\nrow1 |>\n  ggplot(aes(x1, z1, label = x1, color = factor(y1))) +\n  geom_point(shape = 15, size = 7) +\n  theme_void() +\n  scale_x_continuous(breaks = max(z1)) +\n  scale_color_manual(values = c(\"gray70\", \"darkred\")) +\n  geom_text(vjust = 0, nudge_y = 0.5) +\n  coord_fixed() +\n  ylim(-0.5, 2.5) +\n  theme(legend.position = \"top\") +\n  labs(color = \"Status\")\n```\n\n::: {.cell-output-display}\n![Sequence of diseased (1) or non-diseased (0) units (plants). The numbers represent the poistion of the unit](spatial-tests_files/figure-html/fig-runs-1.png){#fig-runs width=672}\n:::\n:::\n\n\nWe can write a code in R and create a function named `oruns.test()` for the ordinary runs test.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-4_e9cae2a14e25e75db7e00c6ba88652f9'}\n\n```{.r .cell-code}\noruns.test <- function(x) {\n  S <- x \n  U = max(cumsum(c(1, diff(S) != 0))) \n  m = sum(S) \n  N = length(S) \n  EU = 1 + (2 * m * (N - m) / N) \n  sU = sqrt(2 * m * (N - m) * (2 * m * (N - m) - N) / (N ^ 2 * (N - 1))) \n  Z = (U - EU) / sU \n  pvalue <- (2 * pnorm(abs(Z), lower.tail = FALSE))\n  result <- ifelse(Z < 1.64,\n                   c(\"clustering\"),\n                   c(\"randomness\"))\n  print(\n    paste(\n      \"There are\",\n      U,\n      \"runs. The number of expected runs is\",\n      round(EU, 1),\n      \". P-value:\",\n      round(pvalue, 6),\n      \". Alternative hypothesis: non-randomness\"\n    )\n  )\n}\n```\n:::\n\n\nWe can now run the test for the example series above.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-5_d01dbc14932787e0bf8b3bacbd3cc3de'}\n\n```{.r .cell-code}\noruns.test(row1$y1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"There are 7 runs. The number of expected runs is 10.6 . P-value: 0.084166 . Alternative hypothesis: non-randomness\"\n```\n:::\n:::\n\n\nThere are built-in functions in R packages that allow for running the ordinary runs test. Let's load the packages and run the test. Note that the results of the `RunsTest()` of the *DescTools* package is the same as the one produced by our custom function.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-6_2aeb9933521a708222459285e14e95d5'}\n\n```{.r .cell-code}\nlibrary(DescTools)\nr <- RunsTest(row1$y1)\nr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tRuns Test for Randomness\n\ndata:  row1$y1\nruns = 7, m = 12, n = 8, p-value = 0.09595\nalternative hypothesis: true number of runs is not equal the expected number\n```\n:::\n:::\n\n\n#### Doublets\n\nDoublet analysis is used to compare the observed number or adjacent diseased plants, a doublet (DD or 11), to the number expected if the disease were randomly distributed in the yard. If the observed number is greater than the expected number, contagion within the field is suspected.\n\nLet's manually produce a code to execute the doublets test. To facilitate, we can create a function and name it `doublets.test()`. The only argument needed is the vector of binary data.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-7_0d4d435d14db1b1497fcb8b1ff466f05'}\n\n```{.r .cell-code}\ndoublets.test <- function(x) {\n  S <- x\n  matrix <- cbind(S[-length(S)], S[-1])\n  pairs <- table(data.frame(matrix))\n  Db <- pairs[2, 2]\n  N <- length(S)\n  m = sum(S)\n  EDb = m * ((m - 1) / N)\n  SDb = sqrt(EDb * (1 - (2 / N)))\n  ZDb = (Db - EDb) / SDb\n  pvalue <- (2 * pnorm(abs(ZDb), lower.tail = FALSE))\n  result <- ifelse(abs(ZDb) >= 1.64,\n                   c(\"aggregation or clustering\"),\n                   c(\"randomness\"))\n  print(\n    paste(\n      \"There are\",\n      Db,\n      \"doublets. The number of expected doublets is\",\n      EDb,\n      \".\",\n      \"P-value:\",\n      round(pvalue, 4),\n      \". Alternative hypothesis: non-randomness\"\n    )\n  )\n}\n```\n:::\n\n\nNow we can run the doublets test for the same sequence.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-8_a241c13cb60102709becc5f53517a2dd'}\n\n```{.r .cell-code}\ndoublets.test(row1$y1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"There are 4 doublets. The number of expected doublets is 2.8 . P-value: 0.4497 . Alternative hypothesis: non-randomness\"\n```\n:::\n:::\n\n\n#### Join count\n\nIn this analysis, two adjacent plants may be classified by the type of join that links them: D-D, H-H or H-D. The orientation(s) of interest (along rows, across rows, diagonally, or a a combination o these) should be specified in the test. The number of joins of the specified type in the orientation(s) of interest is then counted. The question is whether the observed join-count is large (or small) relative to that expected for a random pattern. The join-count statistics provides a basic measure of spatial autocorrelation.\n\nIn R, we can use the `join.count()` function of the *spdep* package to perform a joint count test. First, we need to create the series of binary data from top to bottom and left to right. The data are shown in Fig. 9.13 in page 260 of the book chapter on spatial analysis [@chapter2017]. In the example, there are 5 rows and 5 columns. This will be informed later to run the test.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-9_10fca2339caa1be3fedbc96a5cfd54fb'}\n\n```{.r .cell-code}\nS2 <- c(1,0,1,1,0,\n       1,1,0,0,0,\n       1,0,1,0,0,\n       1,0,0,1,0,\n       0,1,0,1,1)\n```\n:::\n\n\nVisualize the two-dimensional array:\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-joincount1_01be2dc878c3054ac8995646c406a149'}\n\n```{.r .cell-code}\n# Convert to raster \nmapS2 <- terra::rast(matrix(S2, 5 , 5))\n# Convert to data frame\nmapS3 <- terra::as.data.frame(mapS2, xy = TRUE)\nmapS3 |>\n  ggplot(aes(x, y, label = lyr.1, fill = factor(lyr.1))) +\n  geom_tile(color = \"white\", size = 0.5) +\n  theme_void() +\n  labs(fill = \"Status\") +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\"))+\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![Visualization of a matrix of presence or absence data representing a disease spatial pattern](spatial-tests_files/figure-html/fig-joincount1-1.png){#fig-joincount1 width=672}\n:::\n:::\n\n\nAfter loading the library, we need to generate a list of neighbors (nb) for a grid of cells. This is performed with the `cell2nb()` function by informing the number of rows and columns. The argument `rook` means shared edge, but it could be the `queen`, for shared edge or vertex. We can use the default.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-11_d3f4509d138c9a3d3f808cde82e439c1'}\n\n```{.r .cell-code}\nlibrary(spdep)\nnb <- cell2nb(nrow = 5,\n              ncol = 5,\n              type = \"rook\")\n```\n:::\n\n\nThe `joincount.test()` function runs the BB join count test for spatial autocorrelation. The method uses a spatial weights matrix in weights list form for testing whether same-status joins occur more frequently than would be expected if the zones were labelled in a spatially random way. We need to inform the sequence as factor and the `nb` object we created previously.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-12_e06aca01f550868ffe81f196899157d1'}\n\n```{.r .cell-code}\njoincount.test(factor(S2), \n                nb2listw(nb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = -0.58266, p-value = 0.7199\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.9583333             3.2500000             0.2505797 \n\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = -0.66841, p-value = 0.7481\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            2.4166667             2.7500000             0.2486957 \n```\n:::\n:::\n\n\nThe function returns a list with a class for each of the status (in this case 0 and 1) with several components. We should look at the *P-value*. The alternative hypothesis (greater) is that the same status joins occur more frequently than expected if they were labelled in a spatial random way. In this case, we do not reject the null hypothesis of randomness.\n\nWe can run the ordinary runs and doublets tests, which only considers the adjacent neighbor, for the same series and compare the results.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-13_55b226eb863a6132ab1371c78d678c97'}\n\n```{.r .cell-code}\noruns.test(S2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"There are 17 runs. The number of expected runs is 13.5 . P-value: 0.149673 . Alternative hypothesis: non-randomness\"\n```\n:::\n\n```{.r .cell-code}\ndoublets.test(S2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"There are 3 doublets. The number of expected doublets is 5.28 . P-value: 0.3009 . Alternative hypothesis: non-randomness\"\n```\n:::\n:::\n\n\nLet's repeat the procedure using the second array of data shown in the book chapter, for which the result is different. In this case, there is evidence to reject the null hypothesis, indicating aggregation of plants.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-14_caab9f66475edfd9a13e2bf1b1f7c14d'}\n\n```{.r .cell-code}\nS3 <- c(1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       1,1,1,0,0,\n       0,0,0,0,0)\n\njoincount.test(factor(S3), \n                nb2listw(nb))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 0 = 4.2451, p-value = 1.093e-05\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.3750000             3.2500000             0.2505797 \n\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb) \n\nStd. deviate for 1 = 4.5953, p-value = 2.16e-06\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            5.0416667             2.7500000             0.2486957 \n```\n:::\n\n```{.r .cell-code}\noruns.test(S3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"There are 8 runs. The number of expected runs is 13.5 . P-value: 0.024904 . Alternative hypothesis: non-randomness\"\n```\n:::\n:::\n\n\nWe can apply these tests for a real example epidemic data provided by the [epiphy](https://chgigot.github.io/epiphy/) R package [@gigot2018]. Let's work with part of the intensively mapped data on the incidence of tomato spotted wilt virus (TSWV) disease in field trials reported by Cochran (1936) and Bald (1937). First, we need to load the library and then assign one dataframe (the dataset has two dataframes) of the dataset `tomato_tswv` to a new dataframe called `tswv_1929`.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-15_09b7b0821523fca7b189f2f8c8ecee31'}\n\n```{.r .cell-code}\nlibrary(epiphy)\ntswv_1929 <- tomato_tswv$field_1929\ntswv_1929 |>  head(10) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   x  y t i n\n1  1  1 1 0 1\n2  1  2 1 1 1\n3  1  3 1 0 1\n4  1  4 1 1 1\n5  1  5 1 0 1\n6  1  6 1 0 1\n7  1  7 1 0 1\n8  1  8 1 0 1\n9  1  9 1 1 1\n10 1 10 1 0 1\n```\n:::\n:::\n\n\nThe inspection of the first 10 rows of the dataframe shows five variables where x and y are spatial grid coordinates, t is assessment time, i is the status of the plant (0 = healthy, 1 = diseased) and n is the sampling unit size (here all one). Let's visualize these data for each sampling time.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-tsw_f1f9153435d8f4709cc44e67b3a56931'}\n\n```{.r .cell-code}\ntswv_1929 |>\n  ggplot(aes(x, y, fill = factor(i))) +\n  geom_tile() +\n  coord_fixed() +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\")) +\n  facet_wrap( ~ t) +\n  labs(fill = \"Status\")+\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![Incidence maps for for tomato spotted wilt virus (TSWV) disease in field trials reported by Cochran (1936) and Bald (1937)](spatial-tests_files/figure-html/fig-tsw-1.png){#fig-tsw width=672}\n:::\n:::\n\n\nCheck the number of rows (y) and columns (x) for further preparing the neighbor object for the join count statistics.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-17_cd7aa9b8e90268ad79a2a5e245275585'}\n\n```{.r .cell-code}\ntswv_1929 |> \n  dplyr::select(x, y) |> \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       x               y        \n Min.   : 1.00   Min.   : 1.00  \n 1st Qu.: 6.75   1st Qu.:15.75  \n Median :12.50   Median :30.50  \n Mean   :12.50   Mean   :30.50  \n 3rd Qu.:18.25   3rd Qu.:45.25  \n Max.   :24.00   Max.   :60.00  \n```\n:::\n:::\n\n\nThere are 60 rows and 24 columns.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-18_eb9ed25080aa87fe81207cfb604ab155'}\n\n```{.r .cell-code}\n# Neighbor grid\nnb1 <- cell2nb(nrow = 60,\n               ncol = 24,\n               type = \"rook\")\n\n# Pull the binary sequence of time 1\nS1 <- tswv_1929 |>\n  filter(t == \"1\") |>\n  pull(i)\n\njoincount.test(factor(S1),\n               nb2listw(nb1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = -0.28351, p-value = 0.6116\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           482.000000            482.578874              4.169132 \n\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S1) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = -0.059497, p-value = 0.5237\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            23.458333             23.578874              4.104614 \n```\n:::\n:::\n\n\nWe can apply the join count test for time 2 and time 3. Results show that the pattern changes from random to aggregate over time.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-19_9c80cea60d2cd9478a393795ea47a38e'}\n\n```{.r .cell-code}\n# Pull the binary sequence of time 1\nS2 <- tswv_1929 |>\n  filter(t == \"2\") |>\n  pull(i)\n\njoincount.test(factor(S2),\n               nb2listw(nb1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 0.35872, p-value = 0.3599\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n           317.000000            315.900625              9.392312 \n\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S2) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 0.34604, p-value = 0.3647\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            82.958333             81.900625              9.342754 \n```\n:::\n\n```{.r .cell-code}\n# Pull the binary sequence of time 1\nS3 <- tswv_1929 |>\n  filter(t == \"3\") |>\n  pull(i)\n\njoincount.test(factor(S3), \n                nb2listw(nb1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 0 = 1.8541, p-value = 0.03186\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            136.12500             129.92773              11.17243 \n\n\n\tJoin count test under nonfree sampling\n\ndata:  factor(S3) \nweights: nb2listw(nb1) \n\nStd. deviate for 1 = 1.7275, p-value = 0.04204\nalternative hypothesis: greater\nsample estimates:\nSame colour statistic           Expectation              Variance \n            243.70833             237.92773              11.19743 \n```\n:::\n:::\n\n\n### Grouped data\n\nIf the data are intensively mapped, meaning that the spatial locations of the sampling units are known, we are not limited to analyse presence/absence (incidence) only data at the unit level. The sampling units may be quadrats where the total number of plants and the number of disease plants (or number of pathogen propagules) are known. Alternatively, it could be a continuous measure of severity. The question here, similar to the previous section, is whether a plant being diseased makes it more (or less) likely that neighboring plants will be diseased. If that is the case, diseased plants are exhibiting spatial autocorrelation. The most common methods are autocorrelation (known as Moran's I), semivariance and SADIE (an alternative approach to autocorrelation.)\n\n#### Autocorrelation\n\nSpatial autocorrelation analysis provides a quantitative assessment of whether a large value of disease intensity in a sampling unit makes it more (positive autocorrelation) or less (negative auto- correlation) likely that neighboring sampling units tend to have a large value of disease intensity [@chapter2017].\n\nWe will illustrate the method by reproducing the example provided in page 264 of the chapter on spatial analysis [@chapter2017], which was extracted from table 11.3 of @Campbell1990. The data represent a single transect with the number of *Macrophomia phaseolina* propagules per 10 g air-dry soil recorded in 16 contiguous quadrats across a field.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-20_0a44a58daf2e35dd8c6e3ab150ca52b3'}\n\n```{.r .cell-code}\nmp <- data.frame(\n  i = c(1:16),\n  y = c(41, 60, 81, 22, 8, 20, 28, 2, 0, 2, 2, 8, 0, 43, 61, 50)\n)\nmp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    i  y\n1   1 41\n2   2 60\n3   3 81\n4   4 22\n5   5  8\n6   6 20\n7   7 28\n8   8  2\n9   9  0\n10 10  2\n11 11  2\n12 12  8\n13 13  0\n14 14 43\n15 15 61\n16 16 50\n```\n:::\n:::\n\n\nWe can produce a plot to visualize the number of propagules across the transect.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-macrophomina_a74db61428becca54ae190b27c4d3c65'}\n\n```{.r .cell-code}\nmp |>\n  ggplot(aes(i, y)) +\n  geom_col(fill = \"darkred\") +\n  labs(\n    x = \"Relative position within a transect\",\n    y = \"Number of propagules\",\n    caption = \"Source: Campbell and Madden (1990)\"\n  )\n```\n\n::: {.cell-output-display}\n![Number of propagules of Macrophomina phaseolina in the soil at various positions within a transect](spatial-tests_files/figure-html/fig-macrophomina-1.png){#fig-macrophomina width=672}\n:::\n:::\n\n\nTo calculate the autocorrelation coefficient in R, we can use the `ac()` function of the *tseries* package.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-22_4e6b75b7bd5af6a9c349335fce0d9166'}\n\n```{.r .cell-code}\nlibrary(tseries)\nac_mp <- acf(mp$y, lag = 5, pl = FALSE)\nac_mp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nAutocorrelations of series 'mp$y', by lag\n\n     0      1      2      3      4      5 \n 1.000  0.586  0.126 -0.033 -0.017 -0.181 \n```\n:::\n:::\n\n\nLet's store the results in a data frame to facilitate visualization.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-23_bff48949cdf89ba61c1f073f88c49c8a'}\n\n```{.r .cell-code}\nac_mp_dat <- data.frame(index = ac_mp$lag, ac_mp$acf)\nac_mp_dat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  index   ac_mp.acf\n1     0  1.00000000\n2     1  0.58579374\n3     2  0.12636306\n4     3 -0.03307249\n5     4 -0.01701392\n6     5 -0.18092810\n```\n:::\n:::\n\n\nAnd now the plot known as autocorrelogram.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-autocorrel_88fc8ac7593fdab33f1d7a18f8ef13b0'}\n\n```{.r .cell-code}\nac_mp_dat |>\n  ggplot(aes(index, ac_mp.acf, label = round(ac_mp.acf, 3))) +\n  geom_col(fill = \"darkred\") +\n  geom_text(vjust = 0, nudge_y = 0.05) +\n  scale_x_continuous(n.breaks = 6) +\n  geom_hline(yintercept = 0) +\n  labs(x = \"Distance lag\", y = \"Autocorrelation coefficient\")\n```\n\n::: {.cell-output-display}\n![Autocorrelogram for the spatial distribution of Macrophomina phaseolina in soil](spatial-tests_files/figure-html/fig-autocorrel-1.png){#fig-autocorrel width=672}\n:::\n:::\n\n\nThe values we obtained here are not the same but quite close to the values reported in @madden2017. For the transect data, the calculated coefficients in the book example for lags 1, 2 and 3 are 0.625, 0.144, and - 0.041. The conclusion is the same, the smaller the distance between sampling units, the stronger is the correlation between the count values.\n\nThe method above is usually referred to Moran's I [@Moran1950]. Let's use another example dataset from the book to calculate the Moran's I in R. The data is shown in page 269 of the book. The data represent the number of diseased plants per quadrat (out of a total of 100 plants in each) in 144 quadrats. It was based on an epidemic generated using the stochastic simulator of @xu2004. The data is stored in a CSV file.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-25_e747c89610a1be0d4ddc3598e7667353'}\n\n```{.r .cell-code}\nepi <- read_csv(\"https://raw.githubusercontent.com/emdelponte/epidemiology-R/main/data/xu-madden-simulated.csv\")\nepi1 <- epi |>\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |>\n  pull(n)\n```\n:::\n\n\nUsing `moran()` function of the *spdep* R package.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-26_c75d7a9d5845ed968850dd67513bcb28'}\n\n```{.r .cell-code}\nset.seed(100)\nlibrary(spdep)\n```\n:::\n\n\nThe `cell2nb()` function creates the neighbor list with 12 rows and 12 columns, which is how the 144 quadrats are arranged.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-27_24cc0444191764081e1df47533ecbff9'}\n\n```{.r .cell-code}\nnb <- cell2nb(12, 12, type = \"queen\", torus = FALSE)\n```\n:::\n\n\nThe `nb2listw()` function supplements a neighbors list with spatial weights for the chosen coding scheme. We use the default W, which is the row standardized (sums over all links to n). We then create the `col.W` neighbor list.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-28_3d20b8c6d53b5cdea0a21ff4501ffcf5'}\n\n```{.r .cell-code}\ncol.W <- nb2listw(nb, style = \"W\")\n```\n:::\n\n\nThe Moran's I statistic is given by the `moran()` function\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-29_ab588a66c111e76e3d6be7da8d17628e'}\n\n```{.r .cell-code}\nmoran(x = epi1, # numeric vector\n      listw = col.W, # the nb list\n      n = 12, # number of zones\n      S0 = Szero(col.W)) # global sum of weights\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$I\n[1] 0.05818595\n\n$K\n[1] 2.878088\n```\n:::\n:::\n\n\nThe Moran's test for spatial autocorrelation uses spatial weights matrix in weights list form.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-30_96a33636c971dbb066649d8f398ecbc7'}\n\n```{.r .cell-code}\nmoran.test(x = epi1, \n           listw = col.W)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tMoran I test under randomisation\n\ndata:  epi1  \nweights: col.W    \n\nMoran I statistic standard deviate = 15.919, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.698231416      -0.006993007       0.001962596 \n```\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-31_bc693b58e735275eb242dec2c590484b'}\n\n```{.r .cell-code}\ncorrel_I <- sp.correlogram(nb, epi1, \n                           order = 10,\n                           method = \"I\",  \n                           zero.policy = TRUE)\n```\n:::\n\n\nWe can generate a correlogram using the output of the `sp.correlogram()` function. Note that the figure below is very similar to the one shown in Figure 91.5 in page 269 of the book chapter [@chapter2017]. Let's store the results in a dataframe.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-32_964649b3f3d605601f2c02b320d3bc0d'}\n\n```{.r .cell-code}\ndf_correl <- data.frame(correl_I$res) |> \n  mutate(lag = c(1:10))\n\n# Show the spatial autocorrelation for 10 distance lags\nround(df_correl$X1,3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  0.698  0.340  0.086 -0.002 -0.009 -0.024 -0.090 -0.180 -0.217 -0.124\n```\n:::\n:::\n\n\nThen, we can generate the plot using *ggplot*.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-autocorrel2_5182cf7aca25d2cac2cac76c4ca99865'}\n\n```{.r .cell-code}\ndf_correl |>\n  ggplot(aes(lag, X1)) +\n  geom_col(fill = \"darkred\") +\n  scale_x_continuous(n.breaks = 10) +\n  labs(x = \"Distance lag\", y = \"Spatial autocorrelation\")\n```\n\n::: {.cell-output-display}\n![Autocorrelogram for the spatial distribution of simulated epidemics](spatial-tests_files/figure-html/fig-autocorrel2-1.png){#fig-autocorrel2 width=672}\n:::\n:::\n\n\n#### Semivariance\n\nSemi-variance is a key quantity in geostatistics. This differs from spatial autocorrelation because distances are usually measured in discrete spatial lags. The semi-variance can be defined as half the variance of the differences between all possible points spaced a constant distance apart.\n\nThe semi-variance at a distance d = 0 will be zero, because there are no differences between points that are compared to themselves. However, as points are compared to increasingly distant points, the semi-variance increases. At some distance, called the *Range*, the semi-variance will become approximately equal to the variance of the whole surface itself. This is the greatest distance over which the value at a point on the surface is related to the value at another point. In fact, when the distance between two sampling units is small, the sampling units are close together and, usually, variability is low. As the distance increases, so (usually) does the variability.\n\nResults of semi-variance analysis are normally presented as a graphical plot of semi-variance against distance, which is referred to as a semi-variogram. The main characteristics of the semi-variogram of interest are the nugget, the range and the sill, and their estimations are usually based on an appropriate (non-linear) model fitted to the data points representing the semi-variogram.\n\nFor the semi-variance, we will use the `variog()` function of the *geoR* package. We need the data in the long format (x, y and z). Let's reshape the data to the long format and store it in `epi2` dataframe.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-34_30d2378463c97f2d685dd97a914fd5c4'}\n\n```{.r .cell-code}\nepi2 <- epi |>\n  pivot_longer(2:13,\n               names_to = \"y\",\n               values_to = \"n\") |>\n  mutate(y = as.numeric(y))\n\nhead(epi2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 3\n      x     y     n\n  <dbl> <dbl> <dbl>\n1     1     1     2\n2     1     2     2\n3     1     3     3\n4     1     4    33\n5     1     5     4\n6     1     6     0\n```\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-35_aba3a9a1e947ead06332750ac3da5b2a'}\n\n```{.r .cell-code}\nlibrary(geoR)\n# the coordinates are x and y and the data is the n\nv1 <- variog(coords = epi2[,1:2], data = epi2[,3])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvariog: computing omnidirectional variogram\n```\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/fig-semivariance_a2e448d34ba51aa62fc02ffa1871face'}\n\n```{.r .cell-code}\nv2 <- variofit(v1, ini.cov.pars = c(1200, 12), \n               cov.model = \"exponential\", \n               fix.nugget = F)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvariofit: covariance model used is exponential \nvariofit: weights used: npairs \nvariofit: minimisation function used: optim \n```\n:::\n\n```{.r .cell-code}\n# Plotting \nplot(v1, xlim = c(0,15))\nlines(v2, lty = 1, lwd = 2)\n```\n\n::: {.cell-output-display}\n![Semivariance plot for the spatial distribution simulated epidemic](spatial-tests_files/figure-html/fig-semivariance-1.png){#fig-semivariance width=672}\n:::\n:::\n\n\n#### SADIE\n\nSADIE (spatial analysis by distance indices) is an alternative to autocorrelation and semi-variance methods described previously, which has found use in plant pathology [@chapter2017; @xu2004; @li2011]. Similar to those methods, the spatial coordinates for the disease intensity (count of diseased individuals) or pathogen propagules values should be provided.\n\nSADIE quantifies spatial pattern by calculating the minimum total distance to regularity. That is, the distance that individuals must be moved from the starting point defined by the observed counts to the end point at which there is the same number of individuals in each sampling unit. Therefore, if the data are highly aggregated, the distance to regularity will be large, but if the data are close to regular to start with, the distance to regularity will be smaller.\n\nThe null hypothesis to test is that the observed pattern is random. SADIE calculates an index of aggregation (*Ia*). When this is equal to 1, the pattern is random. If this is greater than 1, the pattern is aggregated. Hypothesis testing is based on the randomization procedure. The null hypothesis of randomness, with an alternative hypothesis of aggregation.\n\nAn extension was made to quantify the contribution of each sampling unit count to the observed pattern. Regions with large counts are defined as patches and regions with small counts are defined as gaps. For each sampling unit, a clustering index is calculated and can be mapped.\n\nIn R, we can use the `sadie()` function of the *epiphy* package [@gigot2018]. The function computes the different indices and probabilities based on the distance to regularity for the observed spatial pattern and a specified number of random permutations of this pattern. To run the analysis, the dataframe should have only three columns: the first two must be the x and y coordinates and the third one the observations. Let's continue working with the simulated epidemic dataset named `epi2`. We can map the original data as follows:\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-mapgrouped_6177b0a67a8fa85fd7217d2edf28f5d8'}\n\n```{.r .cell-code}\nepi2 |>\n  ggplot(aes(x, y, label = n, fill = n)) +\n  geom_tile() +\n  geom_text(size = 5, color = \"white\") +\n  theme_void() +\n  coord_fixed() +\n  scale_fill_gradient(low = \"gray70\", high = \"darkred\")\n```\n\n::: {.cell-output-display}\n![Spatial map for the number of diseased plants per quadrat (n = 144) in simulated epidemic](spatial-tests_files/figure-html/fig-mapgrouped-1.png){#fig-mapgrouped width=672}\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-38_e4f855f844aa596527d8508bd24674a7'}\n\n```{.r .cell-code}\nlibrary(epiphy)\nsadie_epi2 <- sadie(epi2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nComputation of Perry's indices:\n```\n:::\n\n```{.r .cell-code}\nsadie_epi2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSpatial Analysis by Distance IndicEs (sadie)\n\nCall:\nsadie.data.frame(data = epi2)\n\nIa: 2.4622 (Pa = < 2.22e-16)\n```\n:::\n:::\n\n\nThe simple output shows the *Ia* value and associated *P*-value. As suggested by the low value of the *P*-value, the pattern is highly aggregated. The `summary()` function provides a more complete information such as the overall inflow and outflow measures. A dataframe with the clustering index for each sampling unit is also provided using the `summary()` function.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-39_007b6a7ced659cdca300f523ddb098f8'}\n\n```{.r .cell-code}\nsummary(sadie_epi2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nsadie.data.frame(data = epi2)\n\nFirst 6 rows of clustering indices:\n  x y  i cost_flows      idx_P idx_LMX prob\n1 1 1  2 -11.382725 -7.2242617      NA   NA\n2 1 2  2  -9.461212 -6.2258877      NA   NA\n3 1 3  3  -7.299482 -5.3390880      NA   NA\n4 1 4 33   1.000000  0.8708407      NA   NA\n5 1 5  4  -5.830952 -3.6534511      NA   NA\n6 1 6  0  -5.301329 -2.9627172      NA   NA\n\nSummary indices:\n                      overall    inflow  outflow\nPerry's index        2.495346 -2.811023 2.393399\nLi-Madden-Xu's index       NA        NA       NA\n\nMain outputs:\nIa: 2.4622 (Pa = < 2.22e-16)\n\n'Total cost': 201.6062\nNumber of permutations: 100\n```\n:::\n:::\n\n\nThe `plot()` function allows to map the clustering indices and so to identify regions of patches (red, outflow) and gaps (blue, inflow).\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-sadie1_1bba131d3cd3ec4bc304c3a06b71aa03'}\n\n```{.r .cell-code}\nplot(sadie_epi2)\n```\n\n::: {.cell-output-display}\n![Map of the SADIE clustering indices where red identifiy patches (outflow) and blue identify gaps (inflow)](spatial-tests_files/figure-html/fig-sadie1-1.png){#fig-sadie1 width=672}\n:::\n:::\n\n\nA isocline plot can be obtained by setting the `isocline` argument as `TRUE`.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-sadie2_2eb466704424d43df8a24bc9ff2bdad4'}\n\n```{.r .cell-code}\nplot(sadie_epi2, isoclines = TRUE)\n```\n\n::: {.cell-output-display}\n![Map of the SADIE clustering indices](spatial-tests_files/figure-html/fig-sadie2-1.png){#fig-sadie2 width=672}\n:::\n:::\n\n\n## Sparsely sampled data\n\nDifferent from intensively mapped data, sparsely sampled data do not contain information about the spatial location of the units, and so it is not taken into account in the analysis. The analysis of sparsely sampled data usually involves characterizing the extent of variability in the mean level of disease intensity per sampling unit [@chapter2017]. There are two types of approaches to analyse these data in the context of spatial patterns of plant disease epidemics: 1) testing the goodness of fit to statistical probability distributions and 2) calculating indices of aggregation. These will be discussed further separated depending on the nature of the data, whether count or incidence (proportion), for which specific distributions are assumed to describe the data.\n\n### Count data\n\n#### Fit to distributions\n\nTwo statistical distributions can be adopted as reference for the description of random or aggregated patterns of disease data in the form of counts of infection within sampling units. Take the count of lesions on a leaf, or the count of diseased plants on a quadrat, as an example. If the presence of a lesion/diseased plant does not increase or decrease the chance that other lesions/diseased plants will occur, the *Poisson* distribution describes the distribution of lesions on the leaf. Otherwise, the *negative binomial* provides a better description.\n\nLet's work with the previous simulation data of 144 quadrats with a variable count of diseased plants per quadrat (in a maximum of 100). Notice that we won't consider the location of each quadrat as in the previous analyses of intensively mapped data. We only need the vector with the number of infected units per sampling unit.\n\nThe *epiphy* package provides a function called `fit_two_distr()`, which allows fitting these two distribution for count data. In this case, either randomness assumption (Poisson distributions) or aggregation assumption (negative binomial) are made, and then, a goodness-of-fit comparison of both distributions is performed using a log-likelihood ratio test. The function requires a dataframe created using the `count()` function where the number of infection units is designated as `i`. It won't work with a single vector of numbers. We create the dataframe using:\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-42_1a310c3e3f01e278d849b3bc8256ad98'}\n\n```{.r .cell-code}\ndata_count <- epi2 |> \n  mutate(i = n) |>  # create i vector\n  epiphy::count()   # create the map object of count class\n```\n:::\n\n\nWe can now run the function that will look fo the the vector `i`. The function returns a list of four components including the outputs of the fitting process for both distribution and the result of the log-likelihood ratio test, the `llr`.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-43_a266dccdb2575ee7f9612f9fc34b9d91'}\n\n```{.r .cell-code}\nfit_data_count <- fit_two_distr(data_count)\nsummary(fit_data_count)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting of two distributions by maximum likelihood\nfor 'count' data.\nParameter estimates:\n\n(1) Poisson (random):\n       Estimate  Std.Err Z value    Pr(>z)    \nlambda 27.85417  0.43981  63.333 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Negative binomial (aggregated):\n       Estimate    Std.Err Z value    Pr(>z)    \nk     0.6327452  0.0707846  8.9390 < 2.2e-16 ***\nmu   27.8541667  2.9510198  9.4388 < 2.2e-16 ***\nprob  0.0222118  0.0033463  6.6378 3.184e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-44_878a44b339b9892e662925e79de0bb48'}\n\n```{.r .cell-code}\nfit_data_count$llr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio test\n\n               LogLik Df  Chisq Pr(>Chisq)    \nrandom :     -2654.71                         \naggregated :  -616.51  1 4076.4  < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nThe very low value of the *P*-value of the LLR test suggest that the negative binomial provides a better fit to the data. The `plot()` function allows for visualizing the expected random and aggregated frequencies together with the observed frequencies. The number of breaks can be adjusted as indicated.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-freq_586a2cf0b967cc740c20a38c835a3938'}\n\n```{.r .cell-code}\nplot(fit_data_count, breaks = 5) \n```\n\n::: {.cell-output-display}\n![Frequencies of the observed and expected aggregated and random distributions](spatial-tests_files/figure-html/fig-freq-1.png){#fig-freq width=672}\n:::\n:::\n\n\nSee below another way to plot by extracting the frequency data (and pivoting from wide to long format) from the generated list and using *ggplot*. Clearly, the negative binomial is a better description for the observed count data.\n\n\n::: {.cell hash='spatial-tests_cache/html/fig-freq1_6fc233a72ce6a09e7b0d9df787fc268d'}\n\n```{.r .cell-code}\ndf <- fit_data_count$freq |>\n  pivot_longer(2:4, \"pattern\", \"value\")\n\ndf |>\n  ggplot(aes(category, value, fill = pattern)) +\n  geom_col(position = \"dodge\", width = 2) +\n  scale_fill_manual(values = c(\"gray70\", \"darkred\", \"steelblue\")) +\n  theme(legend.position = \"top\")\n```\n\n::: {.cell-output-display}\n![Frequencies of the observed and expected aggregated and random distributions](spatial-tests_files/figure-html/fig-freq1-1.png){#fig-freq1 width=672}\n:::\n:::\n\n\n#### Indices of aggregation\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-47_f0413c804157fb4f5b0c750754cc00d1'}\n\n```{.r .cell-code}\nidx <- agg_index(data_count, method = \"fisher\")\nidx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFisher's index of dispersion:\n(Version for count data)\n34.25\n```\n:::\n\n```{.r .cell-code}\nchisq.test(idx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tChi-squared test for (N - 1)*index following a chi-squared\n\tdistribution (df = N - 1)\n\ndata:  idx\nX-squared = 4897.2, df = 143, p-value < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nz.test(idx)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne-sample z-test\n\ndata:  idx\nz = 82.085, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n:::\n\n```{.r .cell-code}\n# Lloyd index\n\nidx_lloyd <- agg_index(data_count, method = \"lloyd\")\nidx_lloyd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLloyd's index of patchiness:\n2.194\n```\n:::\n\n```{.r .cell-code}\nidx_mori <- agg_index(data_count, method = \"morisita\")\nidx_mori\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMorisita's coefficient of dispersion:\n(Version for count data)\n2.186\n```\n:::\n\n```{.r .cell-code}\n# Using the vegan package\nlibrary(vegan)\nz <- data_count$data$i\nmor <- dispindmorisita(z)\nmor\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      imor     mclu      muni      imst pchisq\n1 2.185591 1.008728 0.9922162 0.5041152      0\n```\n:::\n:::\n\n\n### Incidence data\n\n#### Intraclass correlation coefficient \n\nWhen analyzing cluster (or quadrat-based) sampling data, aggregation is the tendency for elements in a sampling unit to have the same status more frequently than expected on the basis of spatial randomness, which can formally be assessed in terms of the intraclass correlation coefficient - ICC [@chapter2017]. In cluster or quadrat sampling, an ICC of zero indicates that the disease status of any plant in a quadrat is unnaffected by the status of other plants in the same quadrat. The closer to one, the higher the tendency for plants in the same quadrat to have the same status.\n\nWe will work with a data set of Fusarium head blight in wheat. On each field, 20 quadrats were randomly defined, each composed of 10 spikes. The data is in the long format with each wheat spike denoted as zero or 1. Let's load the data and inspect the first 10 rows\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-48_040d63f85056653131e502a953e77ffb'}\n\n```{.r .cell-code}\nfhb <- read_csv(\"data/fhb.csv\")\nhead(fhb, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 × 4\n   campo     q espiga status\n   <dbl> <dbl>  <dbl>  <dbl>\n 1     1     1      1      1\n 2     1     1      2      1\n 3     1     1      3      0\n 4     1     1      4      0\n 5     1     1      5      0\n 6     1     1      6      0\n 7     1     1      7      0\n 8     1     1      8      0\n 9     1     1      9      0\n10     1     1     10      0\n```\n:::\n\n```{.r .cell-code}\nfhb2 <- fhb |> \n  group_by(campo, q) |> \n  summarize(total_heads = max(espiga),\n            diseased_heads = sum(status))\n```\n:::\n\n\nNow let's filter data for the field number 1 and fit a generalized linear model with the binomial family and obtain the icc using the `icc` function of the *performance* package.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-49_912091dd741d0c9813e52271731221c6'}\n\n```{.r .cell-code}\nfhb1 <- fhb %>%\n  filter(campo == 1) \nlibrary(lme4)\nglm1 <-glmer(status ~ q + (1 | q), family = \"binomial\", data = fhb1)\nperformance::icc(glm1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Intraclass Correlation Coefficient\n\n    Adjusted ICC: 0.641\n  Unadjusted ICC: 0.600\n```\n:::\n:::\n\n\nWe can fit the model for each of the 20 fields using `group_map` function. Then, we can extract several statistics, including the ICC, with the `modelsummary` function of the *modelsummary* package.\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-50_b2575a011d1fb708bdc12e4932e62607'}\n\n```{.r .cell-code}\nglm2 <- fhb %>%\n  group_by(campo) %>%\n  group_map(~glmer(status ~ q + (1 | q), family = \"binomial\", data= .x))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\nboundary (singular) fit: see help('isSingular')\n```\n:::\n\n```{.r .cell-code}\nlibrary(modelsummary)\nmodelsummary(glm2)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"width: auto !important; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\">   </th>\n   <th style=\"text-align:center;\"> Model 1 </th>\n   <th style=\"text-align:center;\"> Model 2 </th>\n   <th style=\"text-align:center;\"> Model 3 </th>\n   <th style=\"text-align:center;\"> Model 4 </th>\n   <th style=\"text-align:center;\"> Model 5 </th>\n   <th style=\"text-align:center;\"> Model 6 </th>\n   <th style=\"text-align:center;\"> Model 7 </th>\n   <th style=\"text-align:center;\"> Model 8 </th>\n   <th style=\"text-align:center;\"> Model 9 </th>\n   <th style=\"text-align:center;\"> Model 10 </th>\n   <th style=\"text-align:center;\"> Model 11 </th>\n   <th style=\"text-align:center;\"> Model 12 </th>\n   <th style=\"text-align:center;\"> Model 13 </th>\n   <th style=\"text-align:center;\"> Model 14 </th>\n   <th style=\"text-align:center;\"> Model 15 </th>\n   <th style=\"text-align:center;\"> Model 16 </th>\n   <th style=\"text-align:center;\"> Model 17 </th>\n   <th style=\"text-align:center;\"> Model 18 </th>\n   <th style=\"text-align:center;\"> Model 19 </th>\n   <th style=\"text-align:center;\"> Model 20 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:center;\"> −1.322 </td>\n   <td style=\"text-align:center;\"> 0.620 </td>\n   <td style=\"text-align:center;\"> −0.522 </td>\n   <td style=\"text-align:center;\"> −0.110 </td>\n   <td style=\"text-align:center;\"> −0.330 </td>\n   <td style=\"text-align:center;\"> −0.691 </td>\n   <td style=\"text-align:center;\"> −0.090 </td>\n   <td style=\"text-align:center;\"> −0.329 </td>\n   <td style=\"text-align:center;\"> 0.396 </td>\n   <td style=\"text-align:center;\"> −0.095 </td>\n   <td style=\"text-align:center;\"> −1.283 </td>\n   <td style=\"text-align:center;\"> −0.511 </td>\n   <td style=\"text-align:center;\"> 1.791 </td>\n   <td style=\"text-align:center;\"> −0.371 </td>\n   <td style=\"text-align:center;\"> 1.117 </td>\n   <td style=\"text-align:center;\"> 0.148 </td>\n   <td style=\"text-align:center;\"> −1.222 </td>\n   <td style=\"text-align:center;\"> 0.576 </td>\n   <td style=\"text-align:center;\"> 0.479 </td>\n   <td style=\"text-align:center;\"> 0.375 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (1.210) </td>\n   <td style=\"text-align:center;\"> (0.458) </td>\n   <td style=\"text-align:center;\"> (0.346) </td>\n   <td style=\"text-align:center;\"> (0.296) </td>\n   <td style=\"text-align:center;\"> (0.303) </td>\n   <td style=\"text-align:center;\"> (0.465) </td>\n   <td style=\"text-align:center;\"> (0.307) </td>\n   <td style=\"text-align:center;\"> (0.340) </td>\n   <td style=\"text-align:center;\"> (0.298) </td>\n   <td style=\"text-align:center;\"> (0.296) </td>\n   <td style=\"text-align:center;\"> (0.359) </td>\n   <td style=\"text-align:center;\"> (0.317) </td>\n   <td style=\"text-align:center;\"> (0.409) </td>\n   <td style=\"text-align:center;\"> (0.297) </td>\n   <td style=\"text-align:center;\"> (0.540) </td>\n   <td style=\"text-align:center;\"> (0.294) </td>\n   <td style=\"text-align:center;\"> (0.376) </td>\n   <td style=\"text-align:center;\"> (0.317) </td>\n   <td style=\"text-align:center;\"> (0.291) </td>\n   <td style=\"text-align:center;\"> (0.297) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> q </td>\n   <td style=\"text-align:center;\"> 0.138 </td>\n   <td style=\"text-align:center;\"> −0.043 </td>\n   <td style=\"text-align:center;\"> −0.033 </td>\n   <td style=\"text-align:center;\"> −0.020 </td>\n   <td style=\"text-align:center;\"> −0.028 </td>\n   <td style=\"text-align:center;\"> 0.007 </td>\n   <td style=\"text-align:center;\"> −0.068 </td>\n   <td style=\"text-align:center;\"> 0.033 </td>\n   <td style=\"text-align:center;\"> −0.015 </td>\n   <td style=\"text-align:center;\"> −0.018 </td>\n   <td style=\"text-align:center;\"> 0.014 </td>\n   <td style=\"text-align:center;\"> 0.025 </td>\n   <td style=\"text-align:center;\"> −0.082 </td>\n   <td style=\"text-align:center;\"> 0.055 </td>\n   <td style=\"text-align:center;\"> 0.138 </td>\n   <td style=\"text-align:center;\"> −0.005 </td>\n   <td style=\"text-align:center;\"> 0.081 </td>\n   <td style=\"text-align:center;\"> −0.030 </td>\n   <td style=\"text-align:center;\"> −0.028 </td>\n   <td style=\"text-align:center;\"> −0.024 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\">  </td>\n   <td style=\"text-align:center;\"> (0.102) </td>\n   <td style=\"text-align:center;\"> (0.038) </td>\n   <td style=\"text-align:center;\"> (0.030) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n   <td style=\"text-align:center;\"> (0.026) </td>\n   <td style=\"text-align:center;\"> (0.039) </td>\n   <td style=\"text-align:center;\"> (0.027) </td>\n   <td style=\"text-align:center;\"> (0.028) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n   <td style=\"text-align:center;\"> (0.029) </td>\n   <td style=\"text-align:center;\"> (0.026) </td>\n   <td style=\"text-align:center;\"> (0.032) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n   <td style=\"text-align:center;\"> (0.059) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n   <td style=\"text-align:center;\"> (0.031) </td>\n   <td style=\"text-align:center;\"> (0.026) </td>\n   <td style=\"text-align:center;\"> (0.023) </td>\n   <td style=\"text-align:center;\"> (0.025) </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;box-shadow: 0px 1px\"> SD (Intercept q) </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 2.426 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.700 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.296 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.714 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.340 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.0000001 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.154 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.212 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.315 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.627 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.0000002 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.392 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.196 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n   <td style=\"text-align:center;box-shadow: 0px 1px\"> 0.000 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Num.Obs. </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 199 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n   <td style=\"text-align:center;\"> 210 </td>\n   <td style=\"text-align:center;\"> 200 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Marg. </td>\n   <td style=\"text-align:center;\"> 0.065 </td>\n   <td style=\"text-align:center;\"> 0.016 </td>\n   <td style=\"text-align:center;\"> 0.011 </td>\n   <td style=\"text-align:center;\"> 0.004 </td>\n   <td style=\"text-align:center;\"> 0.008 </td>\n   <td style=\"text-align:center;\"> 0.0004 </td>\n   <td style=\"text-align:center;\"> 0.045 </td>\n   <td style=\"text-align:center;\"> 0.011 </td>\n   <td style=\"text-align:center;\"> 0.002 </td>\n   <td style=\"text-align:center;\"> 0.003 </td>\n   <td style=\"text-align:center;\"> 0.002 </td>\n   <td style=\"text-align:center;\"> 0.006 </td>\n   <td style=\"text-align:center;\"> 0.062 </td>\n   <td style=\"text-align:center;\"> 0.030 </td>\n   <td style=\"text-align:center;\"> 0.148 </td>\n   <td style=\"text-align:center;\"> 0.0002 </td>\n   <td style=\"text-align:center;\"> 0.059 </td>\n   <td style=\"text-align:center;\"> 0.009 </td>\n   <td style=\"text-align:center;\"> 0.009 </td>\n   <td style=\"text-align:center;\"> 0.006 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> R2 Cond. </td>\n   <td style=\"text-align:center;\"> 0.665 </td>\n   <td style=\"text-align:center;\"> 0.144 </td>\n   <td style=\"text-align:center;\"> 0.037 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.134 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.044 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.009 </td>\n   <td style=\"text-align:center;\"> 0.020 </td>\n   <td style=\"text-align:center;\"> 0.089 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.239 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.101 </td>\n   <td style=\"text-align:center;\"> 0.020 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> AIC </td>\n   <td style=\"text-align:center;\"> 192.8 </td>\n   <td style=\"text-align:center;\"> 273.7 </td>\n   <td style=\"text-align:center;\"> 248.6 </td>\n   <td style=\"text-align:center;\"> 277.4 </td>\n   <td style=\"text-align:center;\"> 263.8 </td>\n   <td style=\"text-align:center;\"> 262.5 </td>\n   <td style=\"text-align:center;\"> 248.8 </td>\n   <td style=\"text-align:center;\"> 280.9 </td>\n   <td style=\"text-align:center;\"> 280.0 </td>\n   <td style=\"text-align:center;\"> 278.8 </td>\n   <td style=\"text-align:center;\"> 228.4 </td>\n   <td style=\"text-align:center;\"> 279.2 </td>\n   <td style=\"text-align:center;\"> 240.1 </td>\n   <td style=\"text-align:center;\"> 276.4 </td>\n   <td style=\"text-align:center;\"> 126.7 </td>\n   <td style=\"text-align:center;\"> 282.7 </td>\n   <td style=\"text-align:center;\"> 267.1 </td>\n   <td style=\"text-align:center;\"> 278.4 </td>\n   <td style=\"text-align:center;\"> 294.1 </td>\n   <td style=\"text-align:center;\"> 281.6 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> BIC </td>\n   <td style=\"text-align:center;\"> 202.7 </td>\n   <td style=\"text-align:center;\"> 283.6 </td>\n   <td style=\"text-align:center;\"> 258.5 </td>\n   <td style=\"text-align:center;\"> 287.3 </td>\n   <td style=\"text-align:center;\"> 273.7 </td>\n   <td style=\"text-align:center;\"> 272.4 </td>\n   <td style=\"text-align:center;\"> 258.7 </td>\n   <td style=\"text-align:center;\"> 290.8 </td>\n   <td style=\"text-align:center;\"> 289.9 </td>\n   <td style=\"text-align:center;\"> 288.7 </td>\n   <td style=\"text-align:center;\"> 238.3 </td>\n   <td style=\"text-align:center;\"> 289.1 </td>\n   <td style=\"text-align:center;\"> 250.0 </td>\n   <td style=\"text-align:center;\"> 286.3 </td>\n   <td style=\"text-align:center;\"> 136.6 </td>\n   <td style=\"text-align:center;\"> 292.6 </td>\n   <td style=\"text-align:center;\"> 277.0 </td>\n   <td style=\"text-align:center;\"> 288.3 </td>\n   <td style=\"text-align:center;\"> 304.1 </td>\n   <td style=\"text-align:center;\"> 291.5 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> ICC </td>\n   <td style=\"text-align:center;\"> 0.6 </td>\n   <td style=\"text-align:center;\"> 0.1 </td>\n   <td style=\"text-align:center;\"> 0.03 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.1 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.03 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.007 </td>\n   <td style=\"text-align:center;\"> 0.01 </td>\n   <td style=\"text-align:center;\"> 0.03 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.1 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\"> 0.04 </td>\n   <td style=\"text-align:center;\"> 0.01 </td>\n   <td style=\"text-align:center;\">  </td>\n   <td style=\"text-align:center;\">  </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> RMSE </td>\n   <td style=\"text-align:center;\"> 0.32 </td>\n   <td style=\"text-align:center;\"> 0.46 </td>\n   <td style=\"text-align:center;\"> 0.45 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.48 </td>\n   <td style=\"text-align:center;\"> 0.44 </td>\n   <td style=\"text-align:center;\"> 0.46 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.43 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.44 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.29 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.47 </td>\n   <td style=\"text-align:center;\"> 0.49 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n   <td style=\"text-align:center;\"> 0.50 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nWe note that for some fields the ICC could not be estimated because of the zero variance.\n\n#### Fit to distributions\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-51_41835d441a7494f37fee83d1577ce69a'}\n\n```{.r .cell-code}\ntas <-\n  read.csv(\n    \"https://www.apsnet.org/edcenter/disimpactmngmnt/topc/EcologyAndEpidemiologyInR/SpatialAnalysis/Documents/tasmania_test_1.txt\",\n    sep = \"\"\n  )\nhead(tas,10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   quad group_size count\n1     1          6     4\n2     2          6     6\n3     3          6     6\n4     4          6     6\n5     5          6     6\n6     6          6     6\n7     7          6     6\n8     8          6     6\n9     9          6     4\n10   10          6     6\n```\n:::\n\n```{.r .cell-code}\n# Create incidence object for epiphy\ndat_tas <- tas |>\n  mutate(n = group_size, i = count) |>\n  epiphy::incidence()\n\n## Fit to two distributions\nfit_tas <- fit_two_distr(dat_tas)\nsummary(fit_tas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFitting of two distributions by maximum likelihood\nfor 'incidence' data.\nParameter estimates:\n\n(1) Binomial (random):\n     Estimate Std.Err Z value    Pr(>z)    \nprob  0.90860 0.01494  60.819 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(2) Beta-binomial (aggregated):\n      Estimate  Std.Err Z value    Pr(>z)    \nalpha 1.923479 0.869621  2.2119  0.026976 *  \nbeta  0.181337 0.075641  2.3973  0.016514 *  \nprob  0.913847 0.023139 39.4943 < 2.2e-16 ***\nrho   0.322080 0.096414  3.3406  0.000836 ***\ntheta 0.475101 0.209789  2.2647  0.023534 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nfit_tas$llr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nLikelihood ratio test\n\n              LogLik Df  Chisq Pr(>Chisq)    \nrandom :     -75.061                         \naggregated : -57.430  1 35.263   2.88e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n\n```{.r .cell-code}\nplot(fit_tas)\n```\n\n::: {.cell-output-display}\n![](spatial-tests_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n:::\n\n\n#### Dispersion index\n\n glm model\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-52_4233858f7d2c64014f049cbb44217c6e'}\n\n```{.r .cell-code}\nbinom.tas = glm(cbind(count, group_size - count) ~ 1,\n                family = binomial,\n                data = tas)\nsummary(binom.tas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = cbind(count, group_size - count) ~ 1, family = binomial, \n    data = tas)\n\nDeviance Residuals: \n   Min      1Q  Median      3Q     Max  \n-3.447   1.073   1.073   1.073   1.073  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   2.2967     0.1799   12.77   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 117.76  on 61  degrees of freedom\nResidual deviance: 117.76  on 61  degrees of freedom\nAIC: 152.12\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n\n```{.r .cell-code}\nlibrary(performance)\ncheck_overdispersion(binom.tas)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Overdispersion test\n\n       dispersion ratio =   2.348\n  Pearson's Chi-Squared = 143.206\n                p-value = < 0.001\n```\n:::\n:::\n\n\nepiphy(c-alpha test)\n\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-53_d259afbba9ffe6d8a0dd2ef3a8534f57'}\n\n```{.r .cell-code}\nlibrary(epiphy)\ntas2 <- tas |>\n  mutate(i = count,\n         n = group_size) |>  # create i vector\n  epiphy::incidence()\n\nt <- agg_index(tas2, flavor = \"incidence\")\nt\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFisher's index of dispersion:\n(Version for incidence data)\n2.348\n```\n:::\n:::\n\n::: {.cell hash='spatial-tests_cache/html/unnamed-chunk-54_4f0acbb234be79ad985d9fb3ef289347'}\n\n```{.r .cell-code}\ncalpha.test(t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tC(alpha) test\n\ndata:  t\nz = 7.9886, p-value = 1.365e-15\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}